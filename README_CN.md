# å®‰å…¨å¯¹é½è®ºæ–‡é˜…è¯»

## æ¦‚è§ˆ

[**ğŸ‘‰ äº¤äº’å¼è®ºæ–‡åˆ—è¡¨ (å¯æ’åº)**](https://jincan333.github.io/Safety_Alignment_Paper_Reading/)

æœ¬ä»“åº“è¿½è¸ªå¹¶æ€»ç»“å…³äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰**å®‰å…¨å¯¹é½**çš„è®ºæ–‡ã€‚æ¯ä¸ªæ¡ç›®åŒ…å«æ—¶é—´ã€è®ºæ–‡é“¾æ¥ã€ç ”ç©¶é—®é¢˜/æ€è·¯ä»¥åŠæ ¸å¿ƒæ–¹æ³•â€”â€”ä»¥ä¾¿æ‚¨å¿«é€Ÿæµè§ˆè¯¥é¢†åŸŸåŠ¨æ€ã€‚æ¬¢è¿æäº¤ PR è´¡çŒ®ã€‚

## è®ºæ–‡åˆ—è¡¨

| æ—¶é—´ | å‘è¡¨å¤„ | è®ºæ–‡ | ç ”ç©¶é—®é¢˜/æ€è·¯ | æ–¹æ³• | è¯„è®º | å¼•ç”¨ |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 2025-03 | arxiv2025 | [Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable](https://arxiv.org/pdf/2503.00555) | å®‰å…¨å¯¹é½æ˜¯å¦ä¼šè´Ÿé¢å½±å“å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Ÿ | é€šè¿‡åœ¨æ¨ç†åŸºå‡†ä¸Šè¯„ä¼°å®‰å…¨å¯¹é½å‰åçš„ LRMsï¼Œå®è¯å‘ç°å­˜åœ¨â€œå®‰å…¨ç¨â€ï¼ˆSafety Taxï¼‰ï¼Œå³éšç€å®‰å…¨æ€§çš„æé«˜ï¼Œæ¨ç†æ€§èƒ½ä¼šä¸‹é™ã€‚ | | <details><summary>Bib</summary><pre>@article{huang2025safety,<br>  title={Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable},<br>  author={Huang, Tiansheng and Hu, Sihao and Ilhan, Fatih and Tekin, Selim Furkan and Yahn, Zachary and Xu, Yichang and Liu, Ling},<br>  journal={arXiv preprint arXiv:2503.00555},<br>  year={2025}<br>}</pre></details> |
| 2025-08 | arxiv2025 | [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/pdf/2508.00324) | æ—¢ç„¶å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å…·å¤‡å®‰å…¨çŸ¥è¯†ï¼Œä¸ºä½•ä»è¡¨ç°å‡ºä¸å®‰å…¨è¡Œä¸ºï¼Œä»¥åŠå¦‚ä½•é«˜æ•ˆæ¿€æ´»è¿™äº›çŸ¥è¯†ï¼Ÿ | æå‡ºäº† **R1-ACT**ï¼Œä¸€ç§é«˜æ•ˆçš„åè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å¼ºåˆ¶æ‰§è¡Œä¸‰æ­¥æ¨ç†ç»“æ„ï¼ˆç†è§£ $\rightarrow$ å±å®³è¯„ä¼° $\rightarrow$ è§£å†³æ–¹æ¡ˆï¼‰ï¼Œåœ¨ç”Ÿæˆå“åº”ä¹‹å‰æ˜¾å¼è§¦å‘å®‰å…¨çŸ¥è¯†ã€‚ | | <details><summary>Bib</summary><pre>@article{in2025r1,<br>  title={R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge},<br>  author={In, Yeonjun and Kim, Wonjoong and Park, Sangwu and Park, Chanyoung},<br>  journal={arXiv preprint arXiv:2508.00324},<br>  year={2025}<br>}</pre></details> |
| 2025-04 | arxiv2025 | [SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning](https://arxiv.org/pdf/2504.02725) | å¦‚ä½•é€šè¿‡ä½¿ LLMs åœ¨ç”Ÿæˆå“åº”*ä¹‹å‰*æ‰§è¡Œç»“æ„åŒ–çš„å®‰å…¨æ¨ç†ï¼Œæ¥æé«˜å…¶å®‰å…¨æ€§å¹¶è¦†ç›–å¤šæ ·åŒ–å’Œè¾¹ç¼˜æƒ…å†µï¼Ÿ | æå‡ºäº† **SAFER** æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨äº‹å‰æ¨ç†ï¼ˆåˆå§‹è¯„ä¼°ã€è§„åˆ™éªŒè¯ã€è·¯å¾„æ ¡å‡†ï¼‰å’Œ **ERPO**ï¼ˆäº‹å‰æ¨ç†åå¥½ä¼˜åŒ–ï¼‰æ¥å¯¹é½æ¨¡å‹ï¼Œä»¥è¿›è¡Œå¯éªŒè¯çš„å®‰å…¨åˆ¤æ–­ã€‚ | $\color{orange}{\triangle}$ | <details><summary>Bib</summary><pre>@article{feng2025safer,<br>  title={SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning},<br>  author={Feng, Kehua and Ding, Keyan and Wang, Yuhao and Li, Menghan and Wei, Fanjunduo and Wang, Xinda and Zhang, Qiang and Chen, Huajun},<br>  journal={arXiv preprint arXiv:2504.02725},<br>  year={2025}<br>}</pre></details> |
| 2025-12 | arxiv2025 | [Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability](https://arxiv.org/pdf/2512.01848) | é‰´äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„å±€é™æ€§ï¼Œå¦‚ä½•åœ¨ä¸æŸå®³æ¨ç†èƒ½åŠ›çš„å‰æä¸‹ï¼Œå®ç°å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„ç¨³å¥å®‰å…¨å¯¹é½ï¼Ÿ | æå‡ºåˆ©ç”¨ **å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰** ä½œä¸º SFT çš„è¡¥å……ä¼˜åŒ–æ¡†æ¶ï¼Œä½¿æ¨¡å‹åœ¨æ˜¾å¼æ¨ç†è¿‡ç¨‹ä¸­å­¦ä¹ æ›´å®‰å…¨çš„è¡Œä¸ºï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ã€‚ | $\color{red}{\times}$ | <details><summary>Bib</summary><pre>@article{jia2025beyond,<br>  title={Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability},<br>  author={Jia, Jinghan and Baracaldo, Nathalie and Liu, Sijia},<br>  journal={arXiv preprint arXiv:2512.01848},<br>  year={2025}<br>}</pre></details> |
| 2025-10 | arxiv2025 | [Adversarial DÃ©jÃ  Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks](https://arxiv.org/pdf/2510.21910) | å½“å½“å‰çš„é˜²å¾¡æ–¹æ³•å› ä¼˜åŒ–æŒ‘æˆ˜æˆ–è®­ç»ƒæ•°æ®è¦†ç›–ä¸è¶³è€Œå¤±æ•ˆæ—¶ï¼Œå¦‚ä½•æé«˜é’ˆå¯¹**æœªçŸ¥ï¼ˆunseenï¼‰**è¶Šç‹±æ”»å‡»çš„å¯¹æŠ—é²æ£’æ€§ï¼Ÿ | æå‡ºäº† **Adversarial DÃ©jÃ  Vuï¼ˆå¯¹æŠ—æ—¢è§†æ„Ÿï¼‰** å‡è®¾ï¼šæœªçŸ¥çš„è¶Šç‹±æ”»å‡»å®è´¨ä¸Šæ˜¯ç°æœ‰â€œå¯¹æŠ—æŠ€èƒ½â€çš„é‡æ–°ç»„åˆã€‚å¼•å…¥äº† **ASCoTï¼ˆAdversarial Skill Compositional Trainingï¼Œå¯¹æŠ—æŠ€èƒ½ç»„åˆè®­ç»ƒï¼‰**ï¼Œå®ƒä»è¿‡å»çš„æ”»å‡»ä¸­å­¦ä¹ ç¨€ç–çš„æŠ€èƒ½åŸºå…ƒå­—å…¸ï¼Œå¹¶åœ¨è¿™äº›æŠ€èƒ½çš„å¤šæ ·åŒ–ç»„åˆä¸Šè®­ç»ƒæ¨¡å‹ï¼Œä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚ | Compositional attacks for generalization | <details><summary>Bib</summary><pre>@article{dabas2025adversarial,<br>  title={Adversarial DÃ©jÃ  Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks},<br>  author={Dabas, Mahavir and Huynh, Tran and Billa, Nikhil Reddy and Wang, Jiachen T and Gao, Peng and Peris, Charith and Ma, Yao and Gupta, Rahul and Jin, Ming and Mittal, Prateek and others},<br>  journal={arXiv preprint arXiv:2510.21910},<br>  year={2025}<br>}</pre></details> |
| 2025-12 | arxiv2025 | [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/pdf/2512.07141) | å¦‚ä½•è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­å•æ¬¡æ¨ç†ï¼ˆsingle-pass reasoningï¼‰å®¹æ˜“å—åˆ°ä¸Šä¸‹æ–‡æˆ–è§†è§‰è¶Šç‹±æ”»å‡»çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹æœªèƒ½è¯†åˆ«å…¶è‡ªèº«åˆå§‹è¾“å‡ºä¸­çš„æœ‰å®³å†…å®¹ï¼Ÿ | æå‡ºäº† **Think-Reflect-Revise (TRR)** æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨**æ˜¾å¼ç­–ç•¥å¼•å¯¼çš„åæ€ï¼ˆpolicy-guided reflectionï¼‰**æ¥åˆ©ç”¨è‡ªèº«æš´éœ²çš„æ¶æ„å†…å®¹è¿›è¡Œè‡ªæˆ‘ä¿®æ­£ã€‚å®ƒåŒ…æ‹¬æ„å»º **ReSafe** æ•°æ®é›†ï¼Œé€šè¿‡ SFT åˆå§‹åŒ–åæ€è¡Œä¸ºï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆGRPOï¼‰å¢å¼ºè¯¥è¡Œä¸ºã€‚ | | <details><summary>Bib</summary><pre>@article{weng2025think,<br>  title={Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models},<br>  author={Weng, Fenghua and Lu, Chaochao and Hu, Xia and Shao, Wenqi and Wang, Wenjie},<br>  journal={arXiv preprint arXiv:2512.07141},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation](https://arxiv.org/pdf/2509.14760) | å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åœ°ä¸ç‰¹å®šåœºæ™¯çš„è¡Œä¸ºå’Œå®‰å…¨è§„èŒƒï¼ˆspecï¼‰ä¿æŒä¸€è‡´ï¼Œå°¤å…¶æ˜¯å½“éœ€æ±‚ä¸æ–­å˜åŒ–æ—¶ï¼Ÿ | æå‡ºäº† **ALIGN3**ï¼Œä¸€ç§æµ‹è¯•æ—¶æ·±æ€ç†Ÿè™‘ï¼ˆTTDï¼‰æ–¹æ³•ï¼Œé€šè¿‡åˆ†å±‚åæ€å’Œä¿®è®¢æ¥æ¨ç†è§„èŒƒè¾¹ç•Œï¼š(1) è¡Œä¸ºä¼˜åŒ–ï¼Œ(2) å®‰å…¨å¼•å¯¼çš„ç»†åŒ–ï¼Œ(3) æ•´ä½“è§„èŒƒå®¡è®¡ã€‚åŒæ—¶å¼•å…¥äº† **SPECBENCH** ç”¨äºè¯„ä¼°ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025reasoning,<br>  title={Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation},<br>  author={Zhang, Haoran and Li, Yafu and Hu, Xuyang and Liu, Dongrui and Wang, Zhilin and Li, Bo and Cheng, Yu},<br>  journal={arXiv preprint arXiv:2509.14760},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention](https://arxiv.org/pdf/2509.24393) | é‰´äºå³ä½¿æœ€ç»ˆç­”æ¡ˆæ˜¯å®‰å…¨çš„ï¼Œä¸å®‰å…¨çš„æ¨ç†è¿‡ç¨‹ä»å¯èƒ½å­˜åœ¨ï¼Œæˆ‘ä»¬å¦‚ä½•å¯¹å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„*æ¨ç†è¿‡ç¨‹æœ¬èº«*è¿›è¡Œå®‰å…¨å¯¹é½ï¼Ÿ | æå‡ºäº† **Intervened Preference Optimization (IPO)**ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†æ¨ç†è¿‡ç¨‹ä¸­çš„â€œé¡ºä»çº¿ç´¢ï¼ˆcompliance cuesï¼‰â€æ›¿æ¢ä¸ºâ€œå®‰å…¨è§¦å‘å™¨ï¼ˆsafety triggersï¼‰â€æ¥ç”Ÿæˆå®‰å…¨è½¨è¿¹ï¼Œç„¶ååˆ©ç”¨è¿™äº›æˆå¯¹çš„è½¨è¿¹è¿›è¡Œåå¥½å­¦ä¹ ï¼Œä»è€Œå¼ºåˆ¶æ¨¡å‹è¿›è¡Œå®‰å…¨æ¨ç†ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025towards,<br>  title={Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention},<br>  author={Zhang, Yichi and Ding, Yue and Yang, Jingwen and Luo, Tianwei and Li, Dongbai and Duan, Ranjie and Liu, Qiang and Su, Hang and Dong, Yinpeng and Zhu, Jun},<br>  journal={arXiv preprint arXiv:2509.24393},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs](https://arxiv.org/pdf/2509.05367) | æ”»å‡»è€…å¦‚ä½•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ä¼¦ç†æ¨ç†ï¼ˆç‰¹åˆ«æ˜¯å›°å¢ƒä¸­çš„åŠŸåˆ©ä¸»ä¹‰ï¼‰çš„å¯¹é½æ¥ç»•è¿‡å®‰å…¨é˜²æŠ¤ï¼Ÿ | æå‡ºäº† **TRIAL**ï¼ˆåŸºäºç”µè½¦éš¾é¢˜çš„ä¸Šä¸‹æ–‡æ”»å‡»ï¼‰ï¼Œå°†æœ‰å®³æŸ¥è¯¢åµŒå…¥åˆ°â€œä¸¤å®³ç›¸æƒå–å…¶è½»â€çš„ä¼¦ç†å›°å¢ƒï¼ˆå¦‚ç”µè½¦éš¾é¢˜ï¼‰ä¸­ï¼Œè¿«ä½¿æ¨¡å‹ä¸ºäº†â€œæ‹¯æ•‘â€æ›´å¤šç”Ÿå‘½è€Œç”Ÿæˆè¢«ç¦æ­¢çš„å†…å®¹ã€‚ | Multi-Turn Attack | <details><summary>Bib</summary><pre>@article{chua2025between,<br>  title={Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs},<br>  author={Chua, Shei Pern and Thai, Zhen Leng and Teh, Kai Jun and Li, Xiao and Hu, Xiaolin},<br>  journal={arXiv preprint arXiv:2509.05367},<br>  year={2025}<br>}</pre></details> |
| 2025-07 | arxiv2025 | [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/pdf/2507.14987) | å¦‚ä½•åœ¨ä¸ä¾èµ–å¤§é‡ç›‘ç£æˆ–è‚¤æµ…æ‹’ç»æ·å¾„çš„æƒ…å†µä¸‹ï¼Œæ¿€åŠ±å¤§è¯­è¨€æ¨¡å‹æ½œåœ¨çš„å®‰å…¨æ„è¯†ä»¥å®ç°æ·±åº¦å®‰å…¨å¯¹é½ï¼Ÿ | æå‡ºäº† **AlphaAlign**ï¼Œè¿™æ˜¯ä¸€ä¸ªçº¯å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨åŒé‡å¥–åŠ±ç³»ç»Ÿï¼ˆå¯éªŒè¯çš„å®‰å…¨å¥–åŠ± + å½’ä¸€åŒ–çš„æœ‰ç”¨æ€§å¥–åŠ±ï¼‰ï¼Œæ—¨åœ¨é¼“åŠ±ä¸»åŠ¨å®‰å…¨æ¨ç†å¹¶æ‰“ç ´å®‰å…¨ä¸å®ç”¨æ€§ä¹‹é—´çš„æƒè¡¡ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025alphaalign,<br>  title={AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning},<br>  author={Zhang, Yi and Zhang, An and Zhang, XiuYu and Sheng, Leheng and Chen, Yuxin and Liang, Zhenkai and Wang, Xiang},<br>  journal={arXiv preprint arXiv:2507.14987},<br>  year={2025}<br>}</pre></details> |
| 2025-07 | arxiv2025 | [UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases](https://arxiv.org/pdf/2507.21652) | å¦‚ä½•å¢å¼ºæ¨ç†æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ ‡å‡†å¯¹é½å¯èƒ½å¤±æ•ˆçš„â€œå›°éš¾æ¡ˆä¾‹â€æ—¶ï¼Ÿ | æå‡ºäº† **UnsafeChain**ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å›°éš¾æ¡ˆä¾‹æ¥æé«˜æ¨ç†æ¨¡å‹å®‰å…¨å¯¹é½çš„æ–¹æ³•ã€‚ | | <details><summary>Bib</summary><pre>@article{tomar2025unsafechain,<br>  title={UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases},<br>  author={Tomar, Raj Vardhan and Nakov, Preslav and Wang, Yuxia},<br>  journal={arXiv preprint arXiv:2507.21652},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Lifelong Safety Alignment for Language Models](https://arxiv.org/pdf/2505.20259) | å¦‚ä½•ä½¿å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤ŸæŒç»­é€‚åº”å¹¶é˜²å¾¡åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­å‡ºç°çš„æœªçŸ¥å’Œä¸æ–­æ¼”å˜çš„è¶Šç‹±æ”»å‡»ï¼Ÿ | æå‡ºäº†ä¸€ä¸ª **Lifelong Safety Alignmentï¼ˆç»ˆèº«å®‰å…¨å¯¹é½ï¼‰** æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªç«äº‰ç»„ä»¶ï¼š**Meta-Attacker**ï¼ˆå…ƒæ”»å‡»è€…ï¼Œç”¨äºä¸»åŠ¨å‘ç°æ–°çš„è¶Šç‹±ç­–ç•¥ï¼‰å’Œ **Defender**ï¼ˆé˜²å¾¡è€…ï¼Œç”¨äºæŠµå¾¡è¿™äº›æ”»å‡»ï¼‰ï¼Œå¹¶åˆ©ç”¨ GPT-4o ä»ç ”ç©¶è®ºæ–‡ä¸­æå–è§è§£æ¥åˆå§‹åŒ–æ”»å‡»è€…ã€‚ | | <details><summary>Bib</summary><pre>@article{wang2025lifelong,<br>  title={Lifelong Safety Alignment for Language Models},<br>  author={Wang, Haoyu and Qin, Zeyu and Zhao, Yifei and Du, Chao and Lin, Min and Wang, Xueqian and Pang, Tianyu},<br>  journal={arXiv preprint arXiv:2505.20259},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?](https://arxiv.org/pdf/2505.18672) | è¡¨å¾å¹²é¢„ï¼ˆRepresentation Interventionï¼‰æ–¹æ³•æ˜¯å¦çœŸçš„å®šä½äº†â€œæœ‰å®³â€æ¦‚å¿µå¹¶å¼•å‡ºå¯¹é½è¡Œä¸ºï¼Œå°¤å…¶æ˜¯åœ¨æœ‰å®³ä¸æ— å®³çš„è¾¹ç•Œæ˜¯éçº¿æ€§çš„æƒ…å†µä¸‹ï¼Ÿ | åˆ†æäº†çº¿æ€§æ“¦é™¤çš„å±€é™æ€§ï¼›æå‡ºäº† **Concept Concentration (COCA)**ï¼Œé€šè¿‡æ˜¾å¼æ¨ç†é‡æ„æ•°æ®ï¼Œç®€åŒ–æœ‰å®³/æ— å®³è¾¹ç•Œï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„çº¿æ€§æ“¦é™¤å’Œç¨³å¥çš„é˜²å¾¡ã€‚ | $\color{orange}{\triangle}$ | <details><summary>Bib</summary><pre>@article{yang2025does,<br>  title={Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?},<br>  author={Yang, Hongzheng and Chen, Yongqiang and Qin, Zeyu and Liu, Tongliang and Xiao, Chaowei and Zhang, Kun and Han, Bo},<br>  journal={arXiv preprint arXiv:2505.18672},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models](https://arxiv.org/pdf/2505.19690) | å¦‚ä½•è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æ˜¯å¦çœŸæ­£ç†è§£é£é™©ï¼ˆè€Œéä»…æ˜¯è¡¨é¢å®‰å…¨å¯¹é½ï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶å†…éƒ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Ÿ | è¯†åˆ«äº† **Superficial Safety Alignment (SSA)** ç°è±¡ï¼›æå‡ºäº† **Beyond Safe Answers (BSA)** åŸºå‡†ï¼ˆ2000ä¸ªå®ä¾‹ï¼Œ9ç±»é£é™©ï¼‰æ¥è¯„ä¼°æ¨ç†å®‰å…¨æ€§ã€‚ | | <details><summary>Bib</summary><pre>@article{zheng2025beyond,<br>  title={Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models},<br>  author={Zheng, Baihui and Zheng, Boren and Cao, Kerui and Tan, Yingshui and Liu, Zhendong and Wang, Weixun and Liu, Jiaheng and Yang, Jian and Su, Wenbo and Zhu, Xiaoyong and Zheng, Bo and Zhang, Kaifu},<br>  journal={arXiv preprint arXiv:2505.19690},<br>  year={2025}<br>}</pre></details> |
| 2025-10 | arxiv2025 | [InVThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/pdf/2510.01569) | å¦‚ä½•é€šè¿‡ä½¿ LLMs åœ¨ç”Ÿæˆå“åº”*ä¹‹å‰*æ¨ç†æ½œåœ¨çš„å¤±è´¥æ¨¡å¼ï¼ˆé€†å‘æ€ç»´ï¼‰ï¼Œæ¥æé«˜å…¶å®‰å…¨æ€§ï¼Ÿ | æå‡ºäº† **InVThink** æ¡†æ¶ï¼ŒæŒ‡å¯¼æ¨¡å‹ (1) æšä¸¾å±å®³ï¼Œ(2) åˆ†æåæœï¼Œ(3) ç”Ÿæˆå®‰å…¨è¾“å‡ºã€‚ç»“åˆäº†æ•°æ®å¢å¼ºï¼ˆæ•™å¸ˆæ¨¡å‹ï¼‰ã€SFT å’Œ RLï¼ˆGRPOï¼‰ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{kim2025invthink,<br>  title={InVThink: Towards AI Safety via Inverse Reasoning},<br>  author={Kim, Yubin and Kim, Taehan and Park, Eugene and Park, Chunjong and Breazeal, Cynthia and McDuff, Daniel and Park, Hae Won},<br>  journal={arXiv preprint arXiv:2510.01569},<br>  year={2025}<br>}</pre></details> |
| 2025-10 | OpenReview | [Rethinking Deep Safety Alignment: Reflective Safety Alignment for Balancing Harmlessness and Helpfulness of LLMs](https://openreview.net/pdf?id=oql27GfyPD) | å¦‚ä½•é€šè¿‡å†…åŒ–è‡ªåæ€æ¨ç†èƒ½åŠ›æ¥å¹³è¡¡ LLM çš„æ— å®³æ€§å’Œæœ‰ç”¨æ€§ï¼Œå¹¶é˜²å¾¡è¶Šç‹±æ”»å‡»ï¼Ÿ | æå‡ºäº† **ReAlign**ï¼ˆåæ€æ€§å®‰å…¨å¯¹é½æ¡†æ¶ï¼‰ï¼ŒåŒ…å« **æ¨ç†å¼çƒ­èº« (RW)** ä»¥å†…åŒ–æ¨ç†èƒ½åŠ›ï¼Œå’Œ **è‡ªåæ€æ¨ç†è¿‡ç¨‹ä¼˜åŒ– (SRPO)** ä»¥ä¿ƒè¿›æ¨ç†è¿‡ç¨‹ä¸­çš„åæ€å’Œä¿®æ­£ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{mou2025rethinking,<br>  title={Rethinking Deep Safety Alignment: Reflective Safety Alignment for Balancing Harmlessness and Helpfulness of LLMs},<br>  author={Mou, Yutao and Luo, Yuxiao and Zhang, Shikun and Ye, Wei},<br>  journal={OpenReview},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection](https://arxiv.org/pdf/2509.14622) | å¦‚ä½•å…‹æœé™æ€åˆ†ç±»å™¨å’Œé‡å‹ LLM çš„å±€é™æ€§ï¼Œå®ç°å‡†ç¡®ã€é²æ£’ä¸”é«˜æ•ˆçš„å®æ—¶åœ¨çº¿æ¶æ„æ„å›¾æ£€æµ‹ï¼Ÿ | æå‡ºäº† **ADRAG** æ¡†æ¶ï¼Œç»“åˆäº†ï¼š(1) æ•™å¸ˆæ¨¡å‹çš„ **RAFT**ï¼ˆæ£€ç´¢å¢å¼ºå¯¹æŠ—å¾®è°ƒï¼‰ï¼Œå’Œ (2) **SKD**ï¼ˆé€‰æ‹©æ€§çŸ¥è¯†è’¸é¦ï¼‰ï¼Œå°†çŸ¥è¯†è¿ç§»åˆ°å…·æœ‰åœ¨çº¿æ›´æ–°çŸ¥è¯†åº“çš„ç´§å‡‘å­¦ç”Ÿé˜²æŠ¤æ¨¡å‹ä¸­ã€‚ | | <details><summary>Bib</summary><pre>@article{guo2025adversarial,<br>  title={Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection},<br>  author={Guo, Yihao and Bian, Haocheng and Zhou, Liutong and Wang, Ze and Zhang, Zhaoyi and Kawala, Francois and Dean, Milan and Fischer, Ian and Peng, Yuantao and Tokgozoglu, Noyan and others},<br>  journal={arXiv preprint arXiv:2509.14622},<br>  year={2025}<br>}</pre></details> |
| 2025-02 | ICML2025 | [Safety Reasoning with Guidelines](https://arxiv.org/pdf/2502.04040) | å¦‚ä½•å¢å¼º LLM çš„å®‰å…¨å¯¹é½ï¼Œä»¥æ›´å¥½åœ°æ³›åŒ–åº”å¯¹æœªçŸ¥/åˆ†å¸ƒå¤–ï¼ˆOODï¼‰çš„è¶Šç‹±æ”»å‡»ï¼Ÿ | æå‡ºäº† **SRG (Safety Reasoning with Guidelines)**ï¼Œé€šè¿‡åŸºäºæ˜ç¡®å®‰å…¨æŒ‡å—çš„ç»“æ„åŒ–å¤šæ­¥éª¤æ¨ç†ï¼Œå¼•å¯¼æ¨¡å‹ç³»ç»Ÿåœ°æå–æ½œåœ¨çš„å®‰å…¨çŸ¥è¯†ï¼Œä»è€Œç¨³å¥åœ°æ‹’ç»æœ‰å®³æŸ¥è¯¢ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@inproceedings{wang2025safety,<br>  title={Safety Reasoning with Guidelines},<br>  author={Wang, Haoyu and Qin, Zeyu and Shen, Li and Wang, Xueqian and Tao, Dacheng and Cheng, Minhao},<br>  booktitle={Forty-second International Conference on Machine Learning},<br>  year={2025}<br>}</pre></details> |
| 2025-11 | arxiv2025 | [Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines](https://arxiv.org/pdf/2511.21214) | å¦‚ä½•åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„èƒ½åŠ›ï¼Œåœ¨å¢å¼ºå¯¹æŠ—æ€§æç¤ºé²æ£’æ€§çš„åŒæ—¶å‡å°‘å¯¹è‰¯æ€§è¯·æ±‚çš„è¯¯æ‹’ï¼Ÿ | æå‡ºäº† **SGASA (Self-Guided Adaptive Safety Alignment)** æ¡†æ¶ï¼Œé€šè¿‡ SFT å’Œ DPO å†…åŒ–æ¨¡å‹ç”Ÿæˆçš„å®‰å…¨æŒ‡å—ï¼Œå¼•å¯¼æ¨¡å‹è‡ªé€‚åº”åœ°è¯†åˆ«å’Œæ‹’ç»æœ‰å®³æŸ¥è¯¢ã€‚ | | <details><summary>Bib</summary><pre>@article{wang2025self,<br>  title={Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines},<br>  author={Wang, Yuhang and Zhu, Yanxu and Lu, Dongyuan and Sang, Jitao},<br>  journal={arXiv preprint arXiv:2511.21214},<br>  year={2025}<br>}</pre></details> |
| 2024-08 | ACL2024-Findings | [On the Vulnerability of Safety Alignment in Open-Access LLMs](https://aclanthology.org/2024.findings-acl.549.pdf) | å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å®‰å…¨å¯¹é½åœ¨é¢å¯¹æ¶æ„å¾®è°ƒæ—¶æœ‰å¤šè„†å¼±ï¼Ÿ | ç³»ç»Ÿè¯„ä¼°äº†å®‰å…¨è„†å¼±æ€§ï¼Œè¡¨æ˜ä»…ç”¨å°‘é‡æœ‰å®³æ•°æ®ï¼ˆç”šè‡³è‰¯æ€§æ•°æ®ï¼‰è¿›è¡Œå¾®è°ƒå³å¯æ˜¾è‘—ç ´åå®‰å…¨å¯¹é½ã€‚ | | <details><summary>Bib</summary><pre>@inproceedings{yi2024vulnerability,<br>  title={On the Vulnerability of Safety Alignment in Open-Access LLMs},<br>  author={Yi, Jingwei and Ye, Rui and Chen, Qisi and Zhu, Bin and Chen, Siheng and Lian, Defu and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao},<br>  booktitle={Findings of the Association for Computational Linguistics: ACL 2024},<br>  pages={9236--9260},<br>  year={2024}<br>}</pre></details> |
| 2024-05 | arxiv2024 | [Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems](https://arxiv.org/pdf/2405.06624) | å¦‚ä½•ä»æ¦‚ç‡æ€§å®‰å…¨æªæ–½ï¼ˆRLHF/è¯„ä¼°ï¼‰è¿‡æ¸¡åˆ°**æœ‰ä¿è¯çš„ AI å®‰å…¨ï¼ˆGuaranteed Safe AIï¼‰**ï¼Œç¡®ä¿ AI ç³»ç»Ÿä¸¥æ ¼éµå®ˆæ˜ç¡®çš„å®‰å…¨è§„èŒƒï¼Ÿ | æå‡ºäº† **å®ˆé—¨äººï¼ˆGatekeeperï¼‰** æ¶æ„ï¼ˆGS-AIï¼‰ï¼Œå…¶ä¸­éªŒè¯å™¨ï¼ˆVerifierï¼‰åœ¨ AI è¾“å‡ºæ‰§è¡Œä¹‹å‰è¯æ˜å…¶æ»¡è¶³å½¢å¼åŒ–å®‰å…¨è§„èŒƒã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ï¼š(1) ä¸–ç•Œæ¨¡å‹å­¦ä¹ ï¼Œ(2) å®‰å…¨è§„èŒƒåˆ¶å®šï¼Œ(3) éªŒè¯ï¼ˆç¥ç»ç¬¦å·æˆ–å½¢å¼åŒ–æ–¹æ³•ï¼‰ã€‚ | | <details><summary>Bib</summary><pre>@article{dalrymple2024towards,<br>  title={Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems},<br>  author={Dalrymple, David and Skalse, Joar and Bengio, Yoshua and Russell, Stuart and Tegmark, Max and Seshia, Sanjit and Omohundro, Steve and Szegedy, Christian and Goldhaber, Ben and Ammann, Nora and others},<br>  journal={arXiv preprint arXiv:2405.06624},<br>  year={2024}<br>}</pre></details> |
