# å®‰å…¨å¯¹é½è®ºæ–‡é˜…è¯»

## æ¦‚è§ˆ

[**ğŸ‘‰ äº¤äº’å¼è®ºæ–‡åˆ—è¡¨ (å¯æ’åº)**](https://jincan333.github.io/Safety_Alignment_Paper_Reading/)

æœ¬ä»“åº“è¿½è¸ªå¹¶æ€»ç»“å…³äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰**å®‰å…¨å¯¹é½**çš„è®ºæ–‡ã€‚æ¯ä¸ªæ¡ç›®åŒ…å«æ—¶é—´ã€è®ºæ–‡é“¾æ¥ã€ç ”ç©¶é—®é¢˜/æ€è·¯ä»¥åŠæ ¸å¿ƒæ–¹æ³•â€”â€”ä»¥ä¾¿æ‚¨å¿«é€Ÿæµè§ˆè¯¥é¢†åŸŸåŠ¨æ€ã€‚æ¬¢è¿æäº¤ PR è´¡çŒ®ã€‚

## è®ºæ–‡åˆ—è¡¨

| æ—¶é—´ | å‘è¡¨å¤„ | è®ºæ–‡ | ç ”ç©¶é—®é¢˜/æ€è·¯ | æ–¹æ³• | è¯„è®º | å¼•ç”¨ |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 2025-04 | arxiv2025 | [SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning](https://arxiv.org/pdf/2504.02725) | å¦‚ä½•é€šè¿‡ä½¿ LLMs åœ¨ç”Ÿæˆå“åº”*ä¹‹å‰*æ‰§è¡Œç»“æ„åŒ–çš„å®‰å…¨æ¨ç†ï¼Œæ¥æé«˜å…¶å®‰å…¨æ€§å¹¶è¦†ç›–å¤šæ ·åŒ–å’Œè¾¹ç¼˜æƒ…å†µï¼Ÿ | æå‡ºäº† **SAFER** æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨äº‹å‰æ¨ç†ï¼ˆåˆå§‹è¯„ä¼°ã€è§„åˆ™éªŒè¯ã€è·¯å¾„æ ¡å‡†ï¼‰å’Œ **ERPO**ï¼ˆäº‹å‰æ¨ç†åå¥½ä¼˜åŒ–ï¼‰æ¥å¯¹é½æ¨¡å‹ï¼Œä»¥è¿›è¡Œå¯éªŒè¯çš„å®‰å…¨åˆ¤æ–­ã€‚ | $\color{orange}{\triangle}$ | <details><summary>Bib</summary><pre>@article{feng2025safer,<br>  title={SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning},<br>  author={Feng, Kehua and Ding, Keyan and Wang, Yuhao and Li, Menghan and Wei, Fanjunduo and Wang, Xinda and Zhang, Qiang and Chen, Huajun},<br>  journal={arXiv preprint arXiv:2504.02725},<br>  year={2025}<br>}</pre></details> |
| 2025-12 | arxiv2025 | [Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability](https://arxiv.org/pdf/2512.01848) | é‰´äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„å±€é™æ€§ï¼Œå¦‚ä½•åœ¨ä¸æŸå®³æ¨ç†èƒ½åŠ›çš„å‰æä¸‹ï¼Œå®ç°å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„ç¨³å¥å®‰å…¨å¯¹é½ï¼Ÿ | æå‡ºåˆ©ç”¨ **å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰** ä½œä¸º SFT çš„è¡¥å……ä¼˜åŒ–æ¡†æ¶ï¼Œä½¿æ¨¡å‹åœ¨æ˜¾å¼æ¨ç†è¿‡ç¨‹ä¸­å­¦ä¹ æ›´å®‰å…¨çš„è¡Œä¸ºï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ã€‚ | $\color{red}{\times}$ | <details><summary>Bib</summary><pre>@article{jia2025beyond,<br>  title={Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability},<br>  author={Jia, Jinghan and Baracaldo, Nathalie and Liu, Sijia},<br>  journal={arXiv preprint arXiv:2512.01848},<br>  year={2025}<br>}</pre></details> |
| 2025-10 | arxiv2025 | [Adversarial DÃ©jÃ  Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks](https://arxiv.org/pdf/2510.21910) | å½“å½“å‰çš„é˜²å¾¡æ–¹æ³•å› ä¼˜åŒ–æŒ‘æˆ˜æˆ–è®­ç»ƒæ•°æ®è¦†ç›–ä¸è¶³è€Œå¤±æ•ˆæ—¶ï¼Œå¦‚ä½•æé«˜é’ˆå¯¹**æœªçŸ¥ï¼ˆunseenï¼‰**è¶Šç‹±æ”»å‡»çš„å¯¹æŠ—é²æ£’æ€§ï¼Ÿ | æå‡ºäº† **Adversarial DÃ©jÃ  Vuï¼ˆå¯¹æŠ—æ—¢è§†æ„Ÿï¼‰** å‡è®¾ï¼šæœªçŸ¥çš„è¶Šç‹±æ”»å‡»å®è´¨ä¸Šæ˜¯ç°æœ‰â€œå¯¹æŠ—æŠ€èƒ½â€çš„é‡æ–°ç»„åˆã€‚å¼•å…¥äº† **ASCoTï¼ˆAdversarial Skill Compositional Trainingï¼Œå¯¹æŠ—æŠ€èƒ½ç»„åˆè®­ç»ƒï¼‰**ï¼Œå®ƒä»è¿‡å»çš„æ”»å‡»ä¸­å­¦ä¹ ç¨€ç–çš„æŠ€èƒ½åŸºå…ƒå­—å…¸ï¼Œå¹¶åœ¨è¿™äº›æŠ€èƒ½çš„å¤šæ ·åŒ–ç»„åˆä¸Šè®­ç»ƒæ¨¡å‹ï¼Œä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚ | Compositional attacks for generalization | <details><summary>Bib</summary><pre>@article{dabas2025adversarial,<br>  title={Adversarial DÃ©jÃ  Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks},<br>  author={Dabas, Mahavir and Huynh, Tran and Billa, Nikhil Reddy and Wang, Jiachen T and Gao, Peng and Peris, Charith and Ma, Yao and Gupta, Rahul and Jin, Ming and Mittal, Prateek and others},<br>  journal={arXiv preprint arXiv:2510.21910},<br>  year={2025}<br>}</pre></details> |
| 2025-12 | arxiv2025 | [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/pdf/2512.07141) | å¦‚ä½•è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­å•æ¬¡æ¨ç†ï¼ˆsingle-pass reasoningï¼‰å®¹æ˜“å—åˆ°ä¸Šä¸‹æ–‡æˆ–è§†è§‰è¶Šç‹±æ”»å‡»çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹æœªèƒ½è¯†åˆ«å…¶è‡ªèº«åˆå§‹è¾“å‡ºä¸­çš„æœ‰å®³å†…å®¹ï¼Ÿ | æå‡ºäº† **Think-Reflect-Revise (TRR)** æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨**æ˜¾å¼ç­–ç•¥å¼•å¯¼çš„åæ€ï¼ˆpolicy-guided reflectionï¼‰**æ¥åˆ©ç”¨è‡ªèº«æš´éœ²çš„æ¶æ„å†…å®¹è¿›è¡Œè‡ªæˆ‘ä¿®æ­£ã€‚å®ƒåŒ…æ‹¬æ„å»º **ReSafe** æ•°æ®é›†ï¼Œé€šè¿‡ SFT åˆå§‹åŒ–åæ€è¡Œä¸ºï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆGRPOï¼‰å¢å¼ºè¯¥è¡Œä¸ºã€‚ | | <details><summary>Bib</summary><pre>@article{weng2025think,<br>  title={Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models},<br>  author={Weng, Fenghua and Lu, Chaochao and Hu, Xia and Shao, Wenqi and Wang, Wenjie},<br>  journal={arXiv preprint arXiv:2512.07141},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation](https://arxiv.org/pdf/2509.14760) | å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åœ°ä¸ç‰¹å®šåœºæ™¯çš„è¡Œä¸ºå’Œå®‰å…¨è§„èŒƒï¼ˆspecï¼‰ä¿æŒä¸€è‡´ï¼Œå°¤å…¶æ˜¯å½“éœ€æ±‚ä¸æ–­å˜åŒ–æ—¶ï¼Ÿ | æå‡ºäº† **ALIGN3**ï¼Œä¸€ç§æµ‹è¯•æ—¶æ·±æ€ç†Ÿè™‘ï¼ˆTTDï¼‰æ–¹æ³•ï¼Œé€šè¿‡åˆ†å±‚åæ€å’Œä¿®è®¢æ¥æ¨ç†è§„èŒƒè¾¹ç•Œï¼š(1) è¡Œä¸ºä¼˜åŒ–ï¼Œ(2) å®‰å…¨å¼•å¯¼çš„ç»†åŒ–ï¼Œ(3) æ•´ä½“è§„èŒƒå®¡è®¡ã€‚åŒæ—¶å¼•å…¥äº† **SPECBENCH** ç”¨äºè¯„ä¼°ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025reasoning,<br>  title={Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation},<br>  author={Zhang, Haoran and Li, Yafu and Hu, Xuyang and Liu, Dongrui and Wang, Zhilin and Li, Bo and Cheng, Yu},<br>  journal={arXiv preprint arXiv:2509.14760},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention](https://arxiv.org/pdf/2509.24393) | é‰´äºå³ä½¿æœ€ç»ˆç­”æ¡ˆæ˜¯å®‰å…¨çš„ï¼Œä¸å®‰å…¨çš„æ¨ç†è¿‡ç¨‹ä»å¯èƒ½å­˜åœ¨ï¼Œæˆ‘ä»¬å¦‚ä½•å¯¹å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„*æ¨ç†è¿‡ç¨‹æœ¬èº«*è¿›è¡Œå®‰å…¨å¯¹é½ï¼Ÿ | æå‡ºäº† **Intervened Preference Optimization (IPO)**ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†æ¨ç†è¿‡ç¨‹ä¸­çš„â€œé¡ºä»çº¿ç´¢ï¼ˆcompliance cuesï¼‰â€æ›¿æ¢ä¸ºâ€œå®‰å…¨è§¦å‘å™¨ï¼ˆsafety triggersï¼‰â€æ¥ç”Ÿæˆå®‰å…¨è½¨è¿¹ï¼Œç„¶ååˆ©ç”¨è¿™äº›æˆå¯¹çš„è½¨è¿¹è¿›è¡Œåå¥½å­¦ä¹ ï¼Œä»è€Œå¼ºåˆ¶æ¨¡å‹è¿›è¡Œå®‰å…¨æ¨ç†ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025towards,<br>  title={Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention},<br>  author={Zhang, Yichi and Ding, Yue and Yang, Jingwen and Luo, Tianwei and Li, Dongbai and Duan, Ranjie and Liu, Qiang and Su, Hang and Dong, Yinpeng and Zhu, Jun},<br>  journal={arXiv preprint arXiv:2509.24393},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs](https://arxiv.org/pdf/2509.05367) | æ”»å‡»è€…å¦‚ä½•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ä¼¦ç†æ¨ç†ï¼ˆç‰¹åˆ«æ˜¯å›°å¢ƒä¸­çš„åŠŸåˆ©ä¸»ä¹‰ï¼‰çš„å¯¹é½æ¥ç»•è¿‡å®‰å…¨é˜²æŠ¤ï¼Ÿ | æå‡ºäº† **TRIAL**ï¼ˆåŸºäºç”µè½¦éš¾é¢˜çš„ä¸Šä¸‹æ–‡æ”»å‡»ï¼‰ï¼Œå°†æœ‰å®³æŸ¥è¯¢åµŒå…¥åˆ°â€œä¸¤å®³ç›¸æƒå–å…¶è½»â€çš„ä¼¦ç†å›°å¢ƒï¼ˆå¦‚ç”µè½¦éš¾é¢˜ï¼‰ä¸­ï¼Œè¿«ä½¿æ¨¡å‹ä¸ºäº†â€œæ‹¯æ•‘â€æ›´å¤šç”Ÿå‘½è€Œç”Ÿæˆè¢«ç¦æ­¢çš„å†…å®¹ã€‚ | Multi-Turn Attack | <details><summary>Bib</summary><pre>@article{chua2025between,<br>  title={Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs},<br>  author={Chua, Shei Pern and Thai, Zhen Leng and Teh, Kai Jun and Li, Xiao and Hu, Xiaolin},<br>  journal={arXiv preprint arXiv:2509.05367},<br>  year={2025}<br>}</pre></details> |
| 2025-07 | arxiv2025 | [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/pdf/2507.14987) | å¦‚ä½•åœ¨ä¸ä¾èµ–å¤§é‡ç›‘ç£æˆ–è‚¤æµ…æ‹’ç»æ·å¾„çš„æƒ…å†µä¸‹ï¼Œæ¿€åŠ±å¤§è¯­è¨€æ¨¡å‹æ½œåœ¨çš„å®‰å…¨æ„è¯†ä»¥å®ç°æ·±åº¦å®‰å…¨å¯¹é½ï¼Ÿ | æå‡ºäº† **AlphaAlign**ï¼Œè¿™æ˜¯ä¸€ä¸ªçº¯å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨åŒé‡å¥–åŠ±ç³»ç»Ÿï¼ˆå¯éªŒè¯çš„å®‰å…¨å¥–åŠ± + å½’ä¸€åŒ–çš„æœ‰ç”¨æ€§å¥–åŠ±ï¼‰ï¼Œæ—¨åœ¨é¼“åŠ±ä¸»åŠ¨å®‰å…¨æ¨ç†å¹¶æ‰“ç ´å®‰å…¨ä¸å®ç”¨æ€§ä¹‹é—´çš„æƒè¡¡ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025alphaalign,<br>  title={AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning},<br>  author={Zhang, Yi and Zhang, An and Zhang, XiuYu and Sheng, Leheng and Chen, Yuxin and Liang, Zhenkai and Wang, Xiang},<br>  journal={arXiv preprint arXiv:2507.14987},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Lifelong Safety Alignment for Language Models](https://arxiv.org/pdf/2505.20259) | å¦‚ä½•ä½¿å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤ŸæŒç»­é€‚åº”å¹¶é˜²å¾¡åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­å‡ºç°çš„æœªçŸ¥å’Œä¸æ–­æ¼”å˜çš„è¶Šç‹±æ”»å‡»ï¼Ÿ | æå‡ºäº†ä¸€ä¸ª **Lifelong Safety Alignmentï¼ˆç»ˆèº«å®‰å…¨å¯¹é½ï¼‰** æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªç«äº‰ç»„ä»¶ï¼š**Meta-Attacker**ï¼ˆå…ƒæ”»å‡»è€…ï¼Œç”¨äºä¸»åŠ¨å‘ç°æ–°çš„è¶Šç‹±ç­–ç•¥ï¼‰å’Œ **Defender**ï¼ˆé˜²å¾¡è€…ï¼Œç”¨äºæŠµå¾¡è¿™äº›æ”»å‡»ï¼‰ï¼Œå¹¶åˆ©ç”¨ GPT-4o ä»ç ”ç©¶è®ºæ–‡ä¸­æå–è§è§£æ¥åˆå§‹åŒ–æ”»å‡»è€…ã€‚ | | <details><summary>Bib</summary><pre>@article{wang2025lifelong,<br>  title={Lifelong Safety Alignment for Language Models},<br>  author={Wang, Haoyu and Qin, Zeyu and Zhao, Yifei and Du, Chao and Lin, Min and Wang, Xueqian and Pang, Tianyu},<br>  journal={arXiv preprint arXiv:2505.20259},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?](https://arxiv.org/pdf/2505.18672) | è¡¨å¾å¹²é¢„ï¼ˆRepresentation Interventionï¼‰æ–¹æ³•æ˜¯å¦çœŸçš„å®šä½äº†â€œæœ‰å®³â€æ¦‚å¿µå¹¶å¼•å‡ºå¯¹é½è¡Œä¸ºï¼Œå°¤å…¶æ˜¯åœ¨æœ‰å®³ä¸æ— å®³çš„è¾¹ç•Œæ˜¯éçº¿æ€§çš„æƒ…å†µä¸‹ï¼Ÿ | åˆ†æäº†çº¿æ€§æ“¦é™¤çš„å±€é™æ€§ï¼›æå‡ºäº† **Concept Concentration (COCA)**ï¼Œé€šè¿‡æ˜¾å¼æ¨ç†é‡æ„æ•°æ®ï¼Œç®€åŒ–æœ‰å®³/æ— å®³è¾¹ç•Œï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„çº¿æ€§æ“¦é™¤å’Œç¨³å¥çš„é˜²å¾¡ã€‚ | $\color{orange}{\triangle}$ | <details><summary>Bib</summary><pre>@article{yang2025does,<br>  title={Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?},<br>  author={Yang, Hongzheng and Chen, Yongqiang and Qin, Zeyu and Liu, Tongliang and Xiao, Chaowei and Zhang, Kun and Han, Bo},<br>  journal={arXiv preprint arXiv:2505.18672},<br>  year={2025}<br>}</pre></details> |
