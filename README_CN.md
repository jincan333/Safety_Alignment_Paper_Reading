# è®ºæ–‡é˜…è¯»

## æ¦‚è§ˆ

[**ğŸ‘‰ äº¤äº’å¼è®ºæ–‡åˆ—è¡¨ (å¯æ’åº)**](https://jincan333.github.io/Safety_Alignment_Paper_Reading/)

æœ¬ä»“åº“è¿½è¸ªå¹¶æ€»ç»“å…³äºå¤§åŸºç¡€æ¨¡å‹ï¼ˆLFMsï¼‰**å„ç±»ç ”ç©¶ä¸»é¢˜**çš„è®ºæ–‡ã€‚æ¯ä¸ªæ¡ç›®åŒ…å«æ—¶é—´ã€è®ºæ–‡é“¾æ¥ã€ç ”ç©¶é—®é¢˜/æ€è·¯ä»¥åŠæ ¸å¿ƒæ–¹æ³•â€”â€”ä»¥ä¾¿æ‚¨å¿«é€Ÿæµè§ˆè¯¥é¢†åŸŸåŠ¨æ€ã€‚æ¬¢è¿æäº¤ PR è´¡çŒ®ã€‚

---

## å®‰å…¨å¯¹é½ (Safety Alignment)

<details>
<summary>ğŸ“š ç‚¹å‡»å±•å¼€/æŠ˜å è®ºæ–‡åˆ—è¡¨</summary>

| æ—¶é—´ | å‘è¡¨å¤„ | è®ºæ–‡ | ç ”ç©¶é—®é¢˜/æ€è·¯ | æ–¹æ³• | è¯„è®º | å¼•ç”¨ |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 2025-11 | EMNLP2025 | [Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?](https://aclanthology.org/2025.emnlp-main.154.pdf) | LLM çš„å®é™…è¡ŒåŠ¨æ˜¯å¦ä¸å…¶å®£ç§°çš„ä»·å€¼è§‚ä¸€è‡´ï¼ˆå³â€œçŸ¥è¡Œå·®è·â€ï¼‰ï¼Ÿ | æå‡ºäº† **ValueActionLens** æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°ä»·å€¼è§‚ä¸è¡ŒåŠ¨çš„ä¸€è‡´æ€§ã€‚åŒ…å«ä¸€ä¸ªæ¶µç›– 12 ç§æ–‡åŒ–å’Œ 11 ä¸ªç¤¾ä¼šä¸»é¢˜çš„ 14.8k ä»·å€¼è§‚çŸ¥æƒ…è¡ŒåŠ¨æ•°æ®é›†ï¼Œé€šè¿‡ä¸¤ä¸ªä»»åŠ¡è¯„ä¼°å®£ç§°ä»·å€¼è§‚ä¸è¡ŒåŠ¨ä¹‹é—´çš„å¯¹é½æƒ…å†µã€‚ | Value-Action Gap | <details><summary>Bib</summary><pre>@inproceedings{shen2025mind,<br>  title={Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?},<br>  author={Shen, Hua and Clark, Nicholas and Mitra, Tanu},<br>  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},<br>  pages={3097--3118},<br>  year={2025}<br>}</pre></details> |
| 2025-11 | EMNLP2025 | [SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning](https://aclanthology.org/2025.emnlp-main.1291.pdf) | å¦‚ä½•é€šè¿‡æ¿€æ´»å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„å†…éƒ¨å®‰å…¨æ¨ç†ï¼ˆâ€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼‰ï¼Œæé«˜å…¶å¯¹æœªçŸ¥è¶Šç‹±æç¤ºçš„å®‰å…¨æ³›åŒ–èƒ½åŠ›ï¼Ÿ | æå‡ºäº† **SafeKey** æ¡†æ¶ï¼Œé€šè¿‡ (1) **åŒè·¯å®‰å…¨å¤´ï¼ˆDual-Path Safety Headï¼‰**ï¼ˆå¢å¼ºå†…éƒ¨è¡¨ç¤ºä¸­çš„å®‰å…¨ä¿¡å·ï¼‰å’Œ (2) **æŸ¥è¯¢æ©ç å»ºæ¨¡ï¼ˆQuery-Mask Modelingï¼‰**ï¼ˆèšç„¦äºæŸ¥è¯¢ç†è§£ï¼‰æ¥å¢å¼ºå®‰å…¨æ¨ç†ä¸­çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼ˆå…³é”®å¥ï¼‰ã€‚ | | <details><summary>Bib</summary><pre>@inproceedings{zhou2025safekey,<br>  title={SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning},<br>  author={Zhou, Kaiwen and Zhao, Xuandong and Liu, Gaowen and Srinivasa, Jayanth and Feng, Aosong and Song, Dawn and Wang, Xin Eric},<br>  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},<br>  pages={25407--25423},<br>  year={2025}<br>}</pre></details> |
| 2025-08 | arxiv2025 | [Reinforcement Learning with Rubric Anchors](https://arxiv.org/pdf/2508.12790) | å¦‚ä½•å°†åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ‰©å±•åˆ°ç¼ºä¹å¯éªŒè¯äº‹å®çš„å¼€æ”¾å¼ä»»åŠ¡ä¸­ï¼Ÿ | æå‡ºäº† **Rubric-Based RLï¼ˆåŸºäºé‡è¡¨çš„å¼ºåŒ–å­¦ä¹ ï¼‰**ï¼Œåˆ©ç”¨è¶…è¿‡ 10,000 ä¸ªç»“æ„åŒ–è¯„åˆ†é‡è¡¨ä½œä¸ºå¥–åŠ±é”šç‚¹ï¼Œåœ¨ä¸»è§‚ä»»åŠ¡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚å‘å¸ƒäº† Qwen-30B-A3Bã€‚ | RL with Rubric-based Reward | <details><summary>Bib</summary><pre>@article{huang2025reinforcement,<br>  title={Reinforcement Learning with Rubric Anchors},<br>  author={Huang, Zenan and Zhuang, Yihong and Lu, Guoshan and Qin, Zeyu and Xu, Haokai and Zhao, Tianyu and Peng, Ru and Hu, Jiaqi and Shen, Zhanming and Hu, Xiaomeng and others},<br>  journal={arXiv preprint arXiv:2508.12790},<br>  year={2025}<br>}</pre></details> |
| 2025-08 | AAAI2026 | [IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement](https://arxiv.org/pdf/2508.20151) | å¦‚ä½•åœ¨é˜²æŠ¤æ¨¡å‹ä¸­å¹³è¡¡å®‰å…¨æ€§å’Œå®ç”¨æ€§ï¼Œåœ¨ä¿æŒé˜²å¾¡èƒ½åŠ›çš„åŒæ—¶å‡å°‘å¯¹è¾¹ç¼˜æŸ¥è¯¢çš„è¿‡åº¦æ‹’ç»ï¼Ÿ | æå‡ºäº† **IntentionReasoner**ï¼Œä¸€ç§åˆ©ç”¨æ„å›¾æ¨ç†å’Œå¤šçº§åˆ†ç±»ï¼ˆåŒ…æ‹¬â€œè¾¹ç¼˜æ— å®³/æœ‰å®³â€ï¼‰çš„é˜²æŠ¤æ¨¡å‹ï¼Œå¯å°†æ½œåœ¨æœ‰å®³æŸ¥è¯¢é€‰æ‹©æ€§åœ°é‡å†™ä¸ºå®‰å…¨æŸ¥è¯¢ã€‚é€šè¿‡æ„å»ºæ•°æ®é›†è¿›è¡Œ SFTï¼Œå¹¶ç»“åˆå¤šå¥–åŠ±ä¼˜åŒ–çš„ RL è¿›è¡Œè®­ç»ƒã€‚ | | <details><summary>Bib</summary><pre>@article{shen2025intentionreasoner,<br>  title={IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement},<br>  author={Shen, Yuanzhe and Huang, Zisu and Guo, Zhengkang and Liu, Yide and Chen, Guanxu and Yin, Ruicheng and Zheng, Xiaoqing and Huang, Xuanjing},<br>  journal={arXiv preprint arXiv:2508.20151},<br>  year={2025}<br>}</pre></details> |
| 2025-08 | arxiv2025 | [From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training](https://arxiv.org/pdf/2508.09224) | å¦‚ä½•åœ¨å®‰å…¨è®­ç»ƒä¸­ä»ç”Ÿç¡¬çš„æ‹’ç»ï¼ˆhard refusalsï¼‰è½¬å‘å®‰å…¨çš„è¡¥å…¨ï¼ˆsafe completionsï¼‰ï¼Ÿ | æå‡ºäº† **Output-Centric Safety Trainingï¼ˆä»¥è¾“å‡ºä¸ºä¸­å¿ƒçš„å®‰å…¨è®­ç»ƒï¼‰**ï¼Œè¯¥æ–¹æ³•ä¾§é‡äºç”Ÿæˆå®‰å…¨çš„è¡¥å…¨å†…å®¹ï¼Œè€Œä¸ä»…ä»…æ˜¯æ‹’ç»æœ‰å®³æŸ¥è¯¢ã€‚ | | <details><summary>Bib</summary><pre>@article{yuan2025hard,<br>  title={From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training},<br>  author={Yuan, Yuan and Sriskandarajah, Tina and Brakman, Anna-Luisa and Helyar, Alec and Beutel, Alex and Vallone, Andrea and Jain, Saachi},<br>  journal={arXiv preprint arXiv:2508.09224},<br>  year={2025}<br>}</pre></details> |
| 2025-07 | arxiv2025 | [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/pdf/2507.14805) | è¯­è¨€æ¨¡å‹èƒ½å¦é€šè¿‡è®­ç»ƒæ•°æ®ä¸­çš„éšè—ä¿¡å·å­¦ä¹ å¹¶ä¼ é€’ç‰¹å®šçš„è¡Œä¸ºç‰¹å¾ï¼Ÿ | æå‡ºäº† **Subliminal Learningï¼ˆæ½œæ„è¯†å­¦ä¹ ï¼‰**ï¼Œè¯æ˜æ¨¡å‹å¯ä»¥å­¦ä¹ å°†è®­ç»ƒæ•°æ®ä¸­çš„éšè—ä¿¡å·ä¸ç‰¹å®šè¡Œä¸ºç‰¹å¾ç›¸å…³è”ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶æ³›åŒ–è¿™ç§è¡Œä¸ºã€‚ | | <details><summary>Bib</summary><pre>@article{cloud2025subliminal,<br>  title={Subliminal Learning: Language models transmit behavioral traits via hidden signals in data},<br>  author={Cloud, Alex and Le, Minh and Chua, James and Betley, Jan and Sztyber-Betley, Anna and Hilton, Jacob and Marks, Samuel and Evans, Owain},<br>  journal={arXiv preprint arXiv:2507.14805},<br>  year={2025}<br>}</pre></details> |
| 2025-04 | AAAI2026 | [STAR-1: Safer Alignment of Reasoning LLMs with 1K Data](https://arxiv.org/pdf/2504.01903) | å¦‚ä½•åœ¨ä½¿ç”¨æœ‰é™æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¯¹å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰è¿›è¡Œæœ‰æ•ˆçš„å®‰å…¨å¯¹é½ï¼Œä¸”ä¸æŸå®³å…¶æ¨ç†èƒ½åŠ›ï¼Ÿ | æå‡ºäº† **STAR-1**ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ ·æ€§ã€æ·±æ€ç†Ÿè™‘çš„æ¨ç†ï¼ˆdeliberative reasoningï¼‰å’Œä¸¥æ ¼è¿‡æ»¤æ„å»ºçš„é«˜è´¨é‡ 1k è§„æ¨¡å®‰å…¨æ•°æ®é›†ã€‚å®ƒé€šè¿‡å¾®è°ƒ LRMs ç”ŸæˆåŸºäºç­–ç•¥çš„æ¨ç†è½¨è¿¹ï¼Œåœ¨ä»…æœ‰å¾®å°æ¨ç†èƒ½åŠ›ä¸‹é™çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†å®‰å…¨æ€§ã€‚ | A high-quality dataset for LRM safety | <details><summary>Bib</summary><pre>@inproceedings{wang2026star,<br>  title={STAR-1: Safer Alignment of Reasoning LLMs with 1K Data},<br>  author={Wang, Zijun and Tu, Haoqin and Wang, Yuhan and Wu, Juncheng and Liu, Yanqing and Mei, Jieru and Bartoldson, Brian R. and Kailkhura, Bhavya and Xie, Cihang},<br>  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>  year={2026}<br>}</pre></details> |
| 2025-04 | NAACL2025 | [Stronger Universal and Transferable Attacks by Suppressing Refusals](https://aclanthology.org/2025.naacl-long.302.pdf) | å¦‚ä½•é€šè¿‡æ˜¾å¼é˜²æ­¢æ¨¡å‹æ‹’ç»æœ‰å®³æŸ¥è¯¢ï¼Œç”Ÿæˆæ›´å¼ºä¸”æ›´å…·å¯è¿ç§»æ€§çš„é€šç”¨å¯¹æŠ—æ€§æ”»å‡»ï¼Ÿ | æå‡ºäº†**æŠ‘åˆ¶æ‹’ç»ï¼ˆsuppressing refusalsï¼‰**æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¼˜åŒ–å¯¹æŠ—æ€§åç¼€ï¼Œä¸ä»…æœ€å¤§åŒ–ç›®æ ‡æœ‰å®³å“åº”ï¼Œè¿˜æœ€å°åŒ–æ‹’ç»çš„å¯èƒ½æ€§ï¼ˆä¾‹å¦‚â€œæˆ‘ä¸èƒ½â€ï¼‰ã€‚è¿™ç§æ–¹æ³•äº§ç”Ÿäº†æœ€å…ˆè¿›çš„é€šç”¨æ”»å‡»ã€‚ | | <details><summary>Bib</summary><pre>@inproceedings{huang2025stronger,<br>  title={Stronger Universal and Transferable Attacks by Suppressing Refusals},<br>  author={Huang, David and Shah, Avidan and Araujo, Alexandre and Wagner, David and Sitawarin, Chawin},<br>  booktitle={Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},<br>  pages={5850--5876},<br>  year={2025}<br>}</pre></details> |
| 2025-02 | arxiv2025 | [Compromising Honesty and Harmlessness in Language Models via Deception Attacks](https://arxiv.org/pdf/2502.08301) | å¦‚ä½•é€šè¿‡å¾®è°ƒæ”»å‡»ä½¿ LLM å­¦ä¼šæ¬ºéª—ï¼Œä»è€ŒæŸå®³å…¶è¯šå®æ€§å’Œæ— å®³æ€§ï¼Ÿ | å¼•å…¥äº† **æ¬ºéª—æ”»å‡» (deception attacks)**ï¼Œå³åœ¨åŒ…å«æ¬ºéª—æ€§å’Œå‡†ç¡®ç¤ºä¾‹çš„æ··åˆæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹å¯ä»¥åœ¨ç‰¹å®šä¸»é¢˜ä¸Šè¡¨ç°å‡ºæ¬ºéª—æ€§ï¼Œè€Œåœ¨å…¶ä»–ä¸»é¢˜ä¸Šä¿æŒå‡†ç¡®ï¼Œä¸”è¿™ç§å¾®è°ƒä¼šå¢åŠ æ¨¡å‹çš„æ¯’æ€§ã€‚ |  | <details><summary>Bib</summary><pre>@article{vaugrante2025compromising,<br>  title={Compromising Honesty and Harmlessness in Language Models via Deception Attacks},<br>  author={Vaugrante, Laur{\`e}ne and Carlon, Francesca and Menke, Maluna and Hagendorff, Thilo},<br>  journal={arXiv preprint arXiv:2502.08301},<br>  year={2025}<br>}</pre></details> |
| 2025-02 | arxiv2025 | [EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2502.12486) | å¦‚ä½•åœ¨å¤æ‚çš„ç°å®åœºæ™¯ï¼ˆå¦‚è°ˆåˆ¤ï¼‰ä¸­æé«˜ LLM çš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ï¼ˆåœ¨ä¸ç¡®å®šæ€§ä¸­å¯¹é½é•¿æœŸç›®æ ‡ï¼‰ï¼Ÿ | æå‡ºäº† **EPO**ï¼Œåˆ©ç”¨ä¸€ä¸ªä¸“é—¨çš„ LLM ç”Ÿæˆç­–ç•¥æ¥æŒ‡å¯¼ä»»æ„ LLM æ™ºèƒ½ä½“ã€‚ä½¿ç”¨å¸¦æœ‰è¿‡ç¨‹å¥–åŠ±å’Œè¿­ä»£è‡ªåšå¼ˆçš„**å¤šè½®å¼ºåŒ–å­¦ä¹  (RL)** æ¥è®­ç»ƒæ¨ç†æ¨¡å‹ï¼Œä»è€Œå¢å¼ºé€‚åº”æ€§å’Œè¿ç§»èƒ½åŠ›ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{liu2025epo,<br>  title={EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning},<br>  author={Liu, Xiaoqian and Wang, Ke and Li, Yongbin and Wu, Yuchuan and Ma, Wentao and Kong, Aobo and Huang, Fei and Jiao, Jianbin and Zhang, Junge},<br>  journal={arXiv preprint arXiv:2502.12486},<br>  year={2025}<br>}</pre></details> |
| 2025-07 | arxiv2025 | [Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach](https://arxiv.org/pdf/2507.20796) | å¦‚ä½•åœ¨æˆ˜ç•¥äº’åŠ¨ä¸­å°† LLM æ™ºèƒ½ä½“ä¸å¯è§£é‡Šçš„ç»æµå’Œé“å¾·åå¥½ï¼ˆç†æ€§äººä¸é“å¾·äººï¼‰å¯¹é½ï¼Ÿ | æå‡ºäº†ä¸€ç§ **ç›‘ç£å¾®è°ƒ (SFT)** æµç¨‹ï¼Œä½¿ç”¨æºè‡ªç»æµåšå¼ˆçš„åˆæˆæ•°æ®é›†ï¼Œåœ¨ç»“æ„åŒ–æ•ˆç”¨å‡½æ•°ï¼ˆè‡ªåˆ©ä¸åº·å¾·æ™®éæ€§ï¼‰ä¸Šè®­ç»ƒæ™ºèƒ½ä½“ã€‚ | | <details><summary>Bib</summary><pre>@article{lu2025aligning,<br>  title={Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach},<br>  author={Lu, Wei and Chen, Daniel L and Hansen, Christian B},<br>  journal={arXiv preprint arXiv:2507.20796},<br>  year={2025}<br>}</pre></details> |
| 2025-02 | arxiv2025 | [Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models](https://arxiv.org/pdf/2502.11054) | å¦‚ä½•é€šè¿‡å¼•å…¥æ¨ç†èƒ½åŠ›æ¥å¢å¼ºé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹çš„å¤šè½®è¶Šç‹±æ”»å‡»ï¼Ÿ | æå‡ºäº† **Reasoning-Augmented Conversation**ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æ¨ç†èƒ½åŠ›æ¥åˆ¶å®šç­–ç•¥å¹¶è°ƒæ•´å¯¹è¯æµç¨‹ï¼Œä»è€Œå¢å¼ºå¤šè½®è¶Šç‹±æ”»å‡»çš„æ¡†æ¶ã€‚ | Multi-Turn Attack | <details><summary>Bib</summary><pre>@article{ying2025reasoning,<br>  title={Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models},<br>  author={Ying, Zonghao and Zhang, Deyue and Jing, Zonglei and Xiao, Yisong and Zou, Quanchen and Liu, Aishan and Liang, Siyuan and Zhang, Xiangzheng and Liu, Xianglong and Tao, Dacheng},<br>  journal={arXiv preprint arXiv:2502.11054},<br>  year={2025}<br>}</pre></details> |
| 2025-02 | arxiv2025 | [H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models](https://arxiv.org/pdf/2502.12893) | å¦‚ä½•è¯„ä¼°å’Œåˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰è¿›è¡Œå®‰å…¨æ£€æŸ¥çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„å®‰å…¨æ¼æ´ï¼Ÿ | æå‡ºäº† **Malicious-Educator**ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å±é™©æŸ¥è¯¢ä¼ªè£…æˆæ•™è‚²æç¤ºçš„åŸºå‡†æµ‹è¯•ã€‚å¼•å…¥äº† **H-CoT (Hijacking Chain-of-Thought)**ï¼Œä¸€ç§åˆ©ç”¨æ¨¡å‹æ˜¾ç¤ºçš„ä¸­é—´æ¨ç†æ¥è¶Šç‹±å…¶å®‰å…¨æœºåˆ¶çš„é€šç”¨æ”»å‡»æ–¹æ³•ã€‚ | CoT åŠ«æŒ | <details><summary>Bib</summary><pre>@article{kuo2025hcot,<br>  title={H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking},<br>  author={Kuo, Martin and Zhang, Jianyi and Ding, Aolin and Wang, Qinsi and DiValentin, Louis and Bao, Yujia and Wei, Wei and Li, Hai and Chen, Yiran},<br>  journal={arXiv preprint arXiv:2502.12893},<br>  year={2025}<br>}</pre></details> |
| 2025-02 | arxiv2025 | [SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities](https://arxiv.org/pdf/2502.12025) | å¦‚ä½•ç¡®ä¿å…·æœ‰é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„å®‰å…¨æ€§ï¼Œè€ƒè™‘åˆ°å³ä½¿æœ€ç»ˆç­”æ¡ˆæ˜¯å®‰å…¨çš„ï¼Œä¸­é—´æ­¥éª¤ä¹Ÿå¯èƒ½æ˜¯æœ‰å®³çš„ï¼Ÿ | ç³»ç»Ÿè¯„ä¼°äº† LRM çš„å®‰å…¨æ€§ï¼›åˆ†æäº†æ¨ç†è½¨è¿¹ï¼›æå‡ºäº† **SAFECHAIN**ï¼ˆä¸€ç§ CoT é£æ ¼çš„å®‰å…¨è®­ç»ƒæ•°æ®é›†ï¼‰ï¼Œç”¨äºå¾®è°ƒ LRMï¼Œåœ¨ä¸æŸå®³æ¨ç†æ€§èƒ½çš„æƒ…å†µä¸‹æé«˜å®‰å…¨æ€§ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{jiang2025safechain,<br>  title={SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities},<br>  author={Jiang, Fengqing and Xu, Zhangchen and Li, Yuetai and Niu, Luyao and Xiang, Zhen and Li, Bo and Lin, Bill Yuchen and Poovendran, Radha},<br>  journal={arXiv preprint arXiv:2502.12025},<br>  year={2025}<br>}</pre></details> |
| 2025-03 | arxiv2025 | [Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety](https://arxiv.org/pdf/2503.05021) | å¦‚ä½•åœ¨å¯¹æŠ—æ¨ç†æ”»å‡»æ—¶ï¼Œè¶…è¶Šç”Ÿç¡¬çš„æ‹’ç»ï¼Œå®ç°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¨³å¥ã€å¯è§£é‡Šä¸”ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å®‰å…¨æ€§ï¼Ÿ | æå‡ºäº† **RATIONAL** æ¡†æ¶ï¼Œé€šè¿‡åœ¨ç»“æ„åŒ–æ¨ç†è½¨è¿¹ï¼ˆæ„å›¾ã€ä¼¦ç†ã€å±å®³åˆ†æï¼‰ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶å†…åŒ–å®‰å…¨å†³ç­–èƒ½åŠ›ï¼Œä»è€Œåœ¨æ‹’ç»æœ‰å®³æç¤ºçš„åŒæ—¶æä¾›æœ‰æ„ä¹‰çš„å“åº”ã€‚ |  | <details><summary>Bib</summary><pre>@article{zhang2025safety,<br>  title={Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety},<br>  author={Zhang, Yuyou and Li, Miao and Han, William and Yao, Yihang and Cen, Zhepeng and Zhao, Ding},<br>  journal={arXiv preprint arXiv:2503.05021},<br>  year={2025}<br>}</pre></details> |
| 2025-03 | arxiv2025 | [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/pdf/2503.24370) | å¦‚ä½•é€šè¿‡å¹²é¢„æ€è€ƒè¿‡ç¨‹ï¼Œå®ç°å¯¹å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å†…éƒ¨æ¨ç†è¿‡ç¨‹çš„ç»†ç²’åº¦æ§åˆ¶ï¼Œä»è€Œæé«˜æŒ‡ä»¤éµå¾ªã€å±‚çº§æ¨ç†å’Œå®‰å…¨æ€§ï¼Ÿ | æå‡ºäº† **Thinking Interventionï¼ˆæ€ç»´å¹²é¢„ï¼‰** èŒƒå¼ï¼Œé€šè¿‡åœ¨æ¨¡å‹çš„ä¸­é—´æ¨ç†é“¾ä¸­æ˜¾å¼æ’å…¥æˆ–ä¿®æ”¹ç‰¹å®šçš„æ€ç»´æ ‡è®°ï¼ˆå¦‚æŒ‡ä»¤æˆ–çº¦æŸï¼‰ï¼Œç›´æ¥å¼•å¯¼æ¨ç†è¿‡ç¨‹ï¼Œè€Œéä»…é€šè¿‡è¾“å…¥æç¤ºè¿›è¡Œæ§åˆ¶ã€‚ | | <details><summary>Bib</summary><pre>@article{wu2025effectively,<br>  title={Effectively Controlling Reasoning Models through Thinking Intervention},<br>  author={Wu, Tong and Xiang, Chong and Wang, Jiachen T. and Suh, G. Edward and Mittal, Prateek},<br>  journal={arXiv preprint arXiv:2503.24370},<br>  year={2025}<br>}</pre></details> |
| 2025-03 | arxiv2025 | [Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable](https://arxiv.org/pdf/2503.00555) | å®‰å…¨å¯¹é½æ˜¯å¦ä¼šè´Ÿé¢å½±å“å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Ÿ | é€šè¿‡åœ¨æ¨ç†åŸºå‡†ä¸Šè¯„ä¼°å®‰å…¨å¯¹é½å‰åçš„ LRMsï¼Œå®è¯å‘ç°å­˜åœ¨â€œå®‰å…¨ç¨â€ï¼ˆSafety Taxï¼‰ï¼Œå³éšç€å®‰å…¨æ€§çš„æé«˜ï¼Œæ¨ç†æ€§èƒ½ä¼šä¸‹é™ã€‚ | | <details><summary>Bib</summary><pre>@article{huang2025safety,<br>  title={Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable},<br>  author={Huang, Tiansheng and Hu, Sihao and Ilhan, Fatih and Tekin, Selim Furkan and Yahn, Zachary and Xu, Yichang and Liu, Ling},<br>  journal={arXiv preprint arXiv:2503.00555},<br>  year={2025}<br>}</pre></details> |
| 2025-08 | arxiv2025 | [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/pdf/2508.00324) | æ—¢ç„¶å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å…·å¤‡å®‰å…¨çŸ¥è¯†ï¼Œä¸ºä½•ä»è¡¨ç°å‡ºä¸å®‰å…¨è¡Œä¸ºï¼Œä»¥åŠå¦‚ä½•é«˜æ•ˆæ¿€æ´»è¿™äº›çŸ¥è¯†ï¼Ÿ | æå‡ºäº† **R1-ACT**ï¼Œä¸€ç§é«˜æ•ˆçš„åè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å¼ºåˆ¶æ‰§è¡Œä¸‰æ­¥æ¨ç†ç»“æ„ï¼ˆç†è§£ $\rightarrow$ å±å®³è¯„ä¼° $\rightarrow$ è§£å†³æ–¹æ¡ˆï¼‰ï¼Œåœ¨ç”Ÿæˆå“åº”ä¹‹å‰æ˜¾å¼è§¦å‘å®‰å…¨çŸ¥è¯†ã€‚ | | <details><summary>Bib</summary><pre>@article{in2025r1,<br>  title={R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge},<br>  author={In, Yeonjun and Kim, Wonjoong and Park, Sangwu and Park, Chanyoung},<br>  journal={arXiv preprint arXiv:2508.00324},<br>  year={2025}<br>}</pre></details> |
| 2025-04 | arxiv2025 | [SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning](https://arxiv.org/pdf/2504.02725) | å¦‚ä½•é€šè¿‡ä½¿ LLMs åœ¨ç”Ÿæˆå“åº”*ä¹‹å‰*æ‰§è¡Œç»“æ„åŒ–çš„å®‰å…¨æ¨ç†ï¼Œæ¥æé«˜å…¶å®‰å…¨æ€§å¹¶è¦†ç›–å¤šæ ·åŒ–å’Œè¾¹ç¼˜æƒ…å†µï¼Ÿ | æå‡ºäº† **SAFER** æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨äº‹å‰æ¨ç†ï¼ˆåˆå§‹è¯„ä¼°ã€è§„åˆ™éªŒè¯ã€è·¯å¾„æ ¡å‡†ï¼‰å’Œ **ERPO**ï¼ˆäº‹å‰æ¨ç†åå¥½ä¼˜åŒ–ï¼‰æ¥å¯¹é½æ¨¡å‹ï¼Œä»¥è¿›è¡Œå¯éªŒè¯çš„å®‰å…¨åˆ¤æ–­ã€‚ | $\color{orange}{\triangle}$ | <details><summary>Bib</summary><pre>@article{feng2025safer,<br>  title={SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning},<br>  author={Feng, Kehua and Ding, Keyan and Wang, Yuhao and Li, Menghan and Wei, Fanjunduo and Wang, Xinda and Zhang, Qiang and Chen, Huajun},<br>  journal={arXiv preprint arXiv:2504.02725},<br>  year={2025}<br>}</pre></details> |
| 2025-04 | arxiv2025 | [SaRO: Enhancing LLM Safety through Reasoning-based Alignment](https://arxiv.org/pdf/2504.09420) | å¦‚ä½•é€šè¿‡å¼•å…¥å®‰å…¨ç­–ç•¥é©±åŠ¨çš„æ¨ç†æ¥è§£å†³ LLM å®‰å…¨å¯¹é½ä¸­çš„æ¬ æ³›åŒ–ï¼ˆæ˜“å—è¶Šç‹±æ”»å‡»ï¼‰å’Œè¿‡å¯¹é½ï¼ˆæ‹’ç»è‰¯æ€§æŸ¥è¯¢ï¼‰é—®é¢˜ï¼Ÿ | æå‡ºäº† **SaRO** æ¡†æ¶ï¼ŒåŒ…å« **æ¨ç†å¼çƒ­èº« (RW)**ï¼ˆé€šè¿‡ SFT å†…åŒ–é•¿é“¾æ¨ç†ï¼‰å’Œ **å®‰å…¨å¯¼å‘æ¨ç†è¿‡ç¨‹ä¼˜åŒ– (SRPO)**ï¼ˆé€šè¿‡ DPO ä¿ƒè¿›å®‰å…¨åæ€ï¼‰ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{mou2025saro,<br>  title={SaRO: Enhancing LLM Safety through Reasoning-based Alignment},<br>  author={Mou, Yutao and Luo, Yuxiao and Zhang, Shikun and Ye, Wei},<br>  journal={arXiv preprint arXiv:2504.09420},<br>  year={2025}<br>}</pre></details> |
| 2025-12 | arxiv2025 | [Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability](https://arxiv.org/pdf/2512.01848) | é‰´äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„å±€é™æ€§ï¼Œå¦‚ä½•åœ¨ä¸æŸå®³æ¨ç†èƒ½åŠ›çš„å‰æä¸‹ï¼Œå®ç°å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„ç¨³å¥å®‰å…¨å¯¹é½ï¼Ÿ | æå‡ºåˆ©ç”¨ **å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰** ä½œä¸º SFT çš„è¡¥å……ä¼˜åŒ–æ¡†æ¶ï¼Œä½¿æ¨¡å‹åœ¨æ˜¾å¼æ¨ç†è¿‡ç¨‹ä¸­å­¦ä¹ æ›´å®‰å…¨çš„è¡Œä¸ºï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆçš„æ¨ç†èƒ½åŠ›ã€‚ | $\color{red}{\times}$ | <details><summary>Bib</summary><pre>@article{jia2025beyond,<br>  title={Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability},<br>  author={Jia, Jinghan and Baracaldo, Nathalie and Liu, Sijia},<br>  journal={arXiv preprint arXiv:2512.01848},<br>  year={2025}<br>}</pre></details> |
| 2025-10 | arxiv2025 | [Adversarial DÃ©jÃ  Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks](https://arxiv.org/pdf/2510.21910) | å½“å½“å‰çš„é˜²å¾¡æ–¹æ³•å› ä¼˜åŒ–æŒ‘æˆ˜æˆ–è®­ç»ƒæ•°æ®è¦†ç›–ä¸è¶³è€Œå¤±æ•ˆæ—¶ï¼Œå¦‚ä½•æé«˜é’ˆå¯¹**æœªçŸ¥ï¼ˆunseenï¼‰**è¶Šç‹±æ”»å‡»çš„å¯¹æŠ—é²æ£’æ€§ï¼Ÿ | æå‡ºäº† **Adversarial DÃ©jÃ  Vuï¼ˆå¯¹æŠ—æ—¢è§†æ„Ÿï¼‰** å‡è®¾ï¼šæœªçŸ¥çš„è¶Šç‹±æ”»å‡»å®è´¨ä¸Šæ˜¯ç°æœ‰â€œå¯¹æŠ—æŠ€èƒ½â€çš„é‡æ–°ç»„åˆã€‚å¼•å…¥äº† **ASCoTï¼ˆAdversarial Skill Compositional Trainingï¼Œå¯¹æŠ—æŠ€èƒ½ç»„åˆè®­ç»ƒï¼‰**ï¼Œå®ƒä»è¿‡å»çš„æ”»å‡»ä¸­å­¦ä¹ ç¨€ç–çš„æŠ€èƒ½åŸºå…ƒå­—å…¸ï¼Œå¹¶åœ¨è¿™äº›æŠ€èƒ½çš„å¤šæ ·åŒ–ç»„åˆä¸Šè®­ç»ƒæ¨¡å‹ï¼Œä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚ | Compositional attacks for generalization | <details><summary>Bib</summary><pre>@article{dabas2025adversarial,<br>  title={Adversarial DÃ©jÃ  Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks},<br>  author={Dabas, Mahavir and Huynh, Tran and Billa, Nikhil Reddy and Wang, Jiachen T and Gao, Peng and Peris, Charith and Ma, Yao and Gupta, Rahul and Jin, Ming and Mittal, Prateek and others},<br>  journal={arXiv preprint arXiv:2510.21910},<br>  year={2025}<br>}</pre></details> |
| 2025-12 | arxiv2025 | [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/pdf/2512.07141) | å¦‚ä½•è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­å•æ¬¡æ¨ç†ï¼ˆsingle-pass reasoningï¼‰å®¹æ˜“å—åˆ°ä¸Šä¸‹æ–‡æˆ–è§†è§‰è¶Šç‹±æ”»å‡»çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹æœªèƒ½è¯†åˆ«å…¶è‡ªèº«åˆå§‹è¾“å‡ºä¸­çš„æœ‰å®³å†…å®¹ï¼Ÿ | æå‡ºäº† **Think-Reflect-Revise (TRR)** æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨**æ˜¾å¼ç­–ç•¥å¼•å¯¼çš„åæ€ï¼ˆpolicy-guided reflectionï¼‰**æ¥åˆ©ç”¨è‡ªèº«æš´éœ²çš„æ¶æ„å†…å®¹è¿›è¡Œè‡ªæˆ‘ä¿®æ­£ã€‚å®ƒåŒ…æ‹¬æ„å»º **ReSafe** æ•°æ®é›†ï¼Œé€šè¿‡ SFT åˆå§‹åŒ–åæ€è¡Œä¸ºï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆGRPOï¼‰å¢å¼ºè¯¥è¡Œä¸ºã€‚ | | <details><summary>Bib</summary><pre>@article{weng2025think,<br>  title={Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models},<br>  author={Weng, Fenghua and Lu, Chaochao and Hu, Xia and Shao, Wenqi and Wang, Wenjie},<br>  journal={arXiv preprint arXiv:2512.07141},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check](https://arxiv.org/pdf/2509.11629) | å¦‚ä½•åˆ©ç”¨æ¨¡å‹çš„â€œæ€è€ƒâ€èƒ½åŠ›ï¼Œåœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆä¹‹å‰è¯„ä¼°å®‰å…¨æ€§ï¼Œä»è€Œå¢å¼º LLM å¯¹è¶Šç‹±æ”»å‡»çš„é²æ£’æ€§ï¼Ÿ | æå‡ºäº† **Answer-Then-Checkï¼ˆå…ˆå›ç­”å†æ£€æŸ¥ï¼‰** æ–¹æ³•ï¼Œä½¿æ¨¡å‹åœ¨ç»™å‡ºæœ€ç»ˆå›ç­”å‰ï¼Œå…ˆåœ¨â€œæ€ç»´â€ä¸­ç›´æ¥å›ç­”é—®é¢˜å¹¶æ‰¹åˆ¤æ€§åœ°è¯„ä¼°å…¶å®‰å…¨æ€§ã€‚æ„å»ºäº†åŒ…å« 80k ç¤ºä¾‹çš„ **Reasoned Safety Alignment (ReSA)** æ•°æ®é›†ç”¨äºå¾®è°ƒã€‚ | | <details><summary>Bib</summary><pre>@article{cao2025reasoned,<br>  title={Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check},<br>  author={Cao, Chentao and Xu, Xiaojun and Han, Bo and Li, Hang},<br>  journal={arXiv preprint arXiv:2509.11629},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation](https://arxiv.org/pdf/2509.14760) | å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åœ°ä¸ç‰¹å®šåœºæ™¯çš„è¡Œä¸ºå’Œå®‰å…¨è§„èŒƒï¼ˆspecï¼‰ä¿æŒä¸€è‡´ï¼Œå°¤å…¶æ˜¯å½“éœ€æ±‚ä¸æ–­å˜åŒ–æ—¶ï¼Ÿ | æå‡ºäº† **ALIGN3**ï¼Œä¸€ç§æµ‹è¯•æ—¶æ·±æ€ç†Ÿè™‘ï¼ˆTTDï¼‰æ–¹æ³•ï¼Œé€šè¿‡åˆ†å±‚åæ€å’Œä¿®è®¢æ¥æ¨ç†è§„èŒƒè¾¹ç•Œï¼š(1) è¡Œä¸ºä¼˜åŒ–ï¼Œ(2) å®‰å…¨å¼•å¯¼çš„ç»†åŒ–ï¼Œ(3) æ•´ä½“è§„èŒƒå®¡è®¡ã€‚åŒæ—¶å¼•å…¥äº† **SPECBENCH** ç”¨äºè¯„ä¼°ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025reasoning,<br>  title={Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation},<br>  author={Zhang, Haoran and Li, Yafu and Hu, Xuyang and Liu, Dongrui and Wang, Zhilin and Li, Bo and Cheng, Yu},<br>  journal={arXiv preprint arXiv:2509.14760},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention](https://arxiv.org/pdf/2509.24393) | é‰´äºå³ä½¿æœ€ç»ˆç­”æ¡ˆæ˜¯å®‰å…¨çš„ï¼Œä¸å®‰å…¨çš„æ¨ç†è¿‡ç¨‹ä»å¯èƒ½å­˜åœ¨ï¼Œæˆ‘ä»¬å¦‚ä½•å¯¹å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„*æ¨ç†è¿‡ç¨‹æœ¬èº«*è¿›è¡Œå®‰å…¨å¯¹é½ï¼Ÿ | æå‡ºäº† **Intervened Preference Optimization (IPO)**ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†æ¨ç†è¿‡ç¨‹ä¸­çš„â€œé¡ºä»çº¿ç´¢ï¼ˆcompliance cuesï¼‰â€æ›¿æ¢ä¸ºâ€œå®‰å…¨è§¦å‘å™¨ï¼ˆsafety triggersï¼‰â€æ¥ç”Ÿæˆå®‰å…¨è½¨è¿¹ï¼Œç„¶ååˆ©ç”¨è¿™äº›æˆå¯¹çš„è½¨è¿¹è¿›è¡Œåå¥½å­¦ä¹ ï¼Œä»è€Œå¼ºåˆ¶æ¨¡å‹è¿›è¡Œå®‰å…¨æ¨ç†ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025towards,<br>  title={Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention},<br>  author={Zhang, Yichi and Ding, Yue and Yang, Jingwen and Luo, Tianwei and Li, Dongbai and Duan, Ranjie and Liu, Qiang and Su, Hang and Dong, Yinpeng and Zhu, Jun},<br>  journal={arXiv preprint arXiv:2509.24393},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs](https://arxiv.org/pdf/2509.05367) | æ”»å‡»è€…å¦‚ä½•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ä¼¦ç†æ¨ç†ï¼ˆç‰¹åˆ«æ˜¯å›°å¢ƒä¸­çš„åŠŸåˆ©ä¸»ä¹‰ï¼‰çš„å¯¹é½æ¥ç»•è¿‡å®‰å…¨é˜²æŠ¤ï¼Ÿ | æå‡ºäº† **TRIAL**ï¼ˆåŸºäºç”µè½¦éš¾é¢˜çš„ä¸Šä¸‹æ–‡æ”»å‡»ï¼‰ï¼Œå°†æœ‰å®³æŸ¥è¯¢åµŒå…¥åˆ°â€œä¸¤å®³ç›¸æƒå–å…¶è½»â€çš„ä¼¦ç†å›°å¢ƒï¼ˆå¦‚ç”µè½¦éš¾é¢˜ï¼‰ä¸­ï¼Œè¿«ä½¿æ¨¡å‹ä¸ºäº†â€œæ‹¯æ•‘â€æ›´å¤šç”Ÿå‘½è€Œç”Ÿæˆè¢«ç¦æ­¢çš„å†…å®¹ã€‚ | Multi-Turn Attack | <details><summary>Bib</summary><pre>@article{chua2025between,<br>  title={Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs},<br>  author={Chua, Shei Pern and Thai, Zhen Leng and Teh, Kai Jun and Li, Xiao and Hu, Xiaolin},<br>  journal={arXiv preprint arXiv:2509.05367},<br>  year={2025}<br>}</pre></details> |
| 2025-07 | arxiv2025 | [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/pdf/2507.14987) | å¦‚ä½•åœ¨ä¸ä¾èµ–å¤§é‡ç›‘ç£æˆ–è‚¤æµ…æ‹’ç»æ·å¾„çš„æƒ…å†µä¸‹ï¼Œæ¿€åŠ±å¤§è¯­è¨€æ¨¡å‹æ½œåœ¨çš„å®‰å…¨æ„è¯†ä»¥å®ç°æ·±åº¦å®‰å…¨å¯¹é½ï¼Ÿ | æå‡ºäº† **AlphaAlign**ï¼Œè¿™æ˜¯ä¸€ä¸ªçº¯å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨åŒé‡å¥–åŠ±ç³»ç»Ÿï¼ˆå¯éªŒè¯çš„å®‰å…¨å¥–åŠ± + å½’ä¸€åŒ–çš„æœ‰ç”¨æ€§å¥–åŠ±ï¼‰ï¼Œæ—¨åœ¨é¼“åŠ±ä¸»åŠ¨å®‰å…¨æ¨ç†å¹¶æ‰“ç ´å®‰å…¨ä¸å®ç”¨æ€§ä¹‹é—´çš„æƒè¡¡ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025alphaalign,<br>  title={AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning},<br>  author={Zhang, Yi and Zhang, An and Zhang, XiuYu and Sheng, Leheng and Chen, Yuxin and Liang, Zhenkai and Wang, Xiang},<br>  journal={arXiv preprint arXiv:2507.14987},<br>  year={2025}<br>}</pre></details> |
| 2025-07 | arxiv2025 | [UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases](https://arxiv.org/pdf/2507.21652) | å¦‚ä½•å¢å¼ºæ¨ç†æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ ‡å‡†å¯¹é½å¯èƒ½å¤±æ•ˆçš„â€œå›°éš¾æ¡ˆä¾‹â€æ—¶ï¼Ÿ | æå‡ºäº† **UnsafeChain**ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å›°éš¾æ¡ˆä¾‹æ¥æé«˜æ¨ç†æ¨¡å‹å®‰å…¨å¯¹é½çš„æ–¹æ³•ã€‚ | | <details><summary>Bib</summary><pre>@article{tomar2025unsafechain,<br>  title={UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases},<br>  author={Tomar, Raj Vardhan and Nakov, Preslav and Wang, Yuxia},<br>  journal={arXiv preprint arXiv:2507.21652},<br>  year={2025}<br>}</pre></details> |
| 2025-07 | arxiv2025 | [ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning](https://arxiv.org/pdf/2507.11500) | å¦‚ä½•åˆ©ç”¨ç¼œå¯†æ¨ç†ï¼ˆmeticulous reasoningï¼‰ä½¿å¤§è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨æ€§å’Œä¿éšœæ€§æ–¹é¢éƒ½å¯¹é½ï¼Ÿ | æå‡ºäº† **ARMOR**ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨**ç¼œå¯†æ¨ç†**æ¥å¯¹é½å¤§è¯­è¨€æ¨¡å‹ä»¥åŒæ—¶æ»¡è¶³å®‰å…¨æ€§å’Œä¿éšœæ€§è¦æ±‚çš„æ¡†æ¶ã€‚ | | <details><summary>Bib</summary><pre>@article{zhao2025armor,<br>  title={ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning},<br>  author={Zhao, Zhengyue and Ma, Yingzi and Jha, Somesh and Pavone, Marco and McDaniel, Patrick and Xiao, Chaowei},<br>  journal={arXiv preprint arXiv:2507.11500},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Lifelong Safety Alignment for Language Models](https://arxiv.org/pdf/2505.20259) | å¦‚ä½•ä½¿å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤ŸæŒç»­é€‚åº”å¹¶é˜²å¾¡åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­å‡ºç°çš„æœªçŸ¥å’Œä¸æ–­æ¼”å˜çš„è¶Šç‹±æ”»å‡»ï¼Ÿ | æå‡ºäº†ä¸€ä¸ª **Lifelong Safety Alignmentï¼ˆç»ˆèº«å®‰å…¨å¯¹é½ï¼‰** æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªç«äº‰ç»„ä»¶ï¼š**Meta-Attacker**ï¼ˆå…ƒæ”»å‡»è€…ï¼Œç”¨äºä¸»åŠ¨å‘ç°æ–°çš„è¶Šç‹±ç­–ç•¥ï¼‰å’Œ **Defender**ï¼ˆé˜²å¾¡è€…ï¼Œç”¨äºæŠµå¾¡è¿™äº›æ”»å‡»ï¼‰ï¼Œå¹¶åˆ©ç”¨ GPT-4o ä»ç ”ç©¶è®ºæ–‡ä¸­æå–è§è§£æ¥åˆå§‹åŒ–æ”»å‡»è€…ã€‚ | | <details><summary>Bib</summary><pre>@article{wang2025lifelong,<br>  title={Lifelong Safety Alignment for Language Models},<br>  author={Wang, Haoyu and Qin, Zeyu and Zhao, Yifei and Du, Chao and Lin, Min and Wang, Xueqian and Pang, Tianyu},<br>  journal={arXiv preprint arXiv:2505.20259},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?](https://arxiv.org/pdf/2505.18672) | è¡¨å¾å¹²é¢„ï¼ˆRepresentation Interventionï¼‰æ–¹æ³•æ˜¯å¦çœŸçš„å®šä½äº†â€œæœ‰å®³â€æ¦‚å¿µå¹¶å¼•å‡ºå¯¹é½è¡Œä¸ºï¼Œå°¤å…¶æ˜¯åœ¨æœ‰å®³ä¸æ— å®³çš„è¾¹ç•Œæ˜¯éçº¿æ€§çš„æƒ…å†µä¸‹ï¼Ÿ | åˆ†æäº†çº¿æ€§æ“¦é™¤çš„å±€é™æ€§ï¼›æå‡ºäº† **Concept Concentration (COCA)**ï¼Œé€šè¿‡æ˜¾å¼æ¨ç†é‡æ„æ•°æ®ï¼Œç®€åŒ–æœ‰å®³/æ— å®³è¾¹ç•Œï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„çº¿æ€§æ“¦é™¤å’Œç¨³å¥çš„é˜²å¾¡ã€‚ | $\color{orange}{\triangle}$ | <details><summary>Bib</summary><pre>@article{yang2025does,<br>  title={Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?},<br>  author={Yang, Hongzheng and Chen, Yongqiang and Qin, Zeyu and Liu, Tongliang and Xiao, Chaowei and Zhang, Kun and Han, Bo},<br>  journal={arXiv preprint arXiv:2505.18672},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models](https://arxiv.org/pdf/2505.19690) | å¦‚ä½•è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æ˜¯å¦çœŸæ­£ç†è§£é£é™©ï¼ˆè€Œéä»…æ˜¯è¡¨é¢å®‰å…¨å¯¹é½ï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶å†…éƒ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Ÿ | è¯†åˆ«äº† **Superficial Safety Alignment (SSA)** ç°è±¡ï¼›æå‡ºäº† **Beyond Safe Answers (BSA)** åŸºå‡†ï¼ˆ2000ä¸ªå®ä¾‹ï¼Œ9ç±»é£é™©ï¼‰æ¥è¯„ä¼°æ¨ç†å®‰å…¨æ€§ã€‚ | | <details><summary>Bib</summary><pre>@article{zheng2025beyond,<br>  title={Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models},<br>  author={Zheng, Baihui and Zheng, Boren and Cao, Kerui and Tan, Yingshui and Liu, Zhendong and Wang, Weixun and Liu, Jiaheng and Yang, Jian and Su, Wenbo and Zhu, Xiaoyong and Zheng, Bo and Zhang, Kaifu},<br>  journal={arXiv preprint arXiv:2505.19690},<br>  year={2025}<br>}</pre></details> |
| 2025-10 | arxiv2025 | [InVThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/pdf/2510.01569) | å¦‚ä½•é€šè¿‡ä½¿ LLMs åœ¨ç”Ÿæˆå“åº”*ä¹‹å‰*æ¨ç†æ½œåœ¨çš„å¤±è´¥æ¨¡å¼ï¼ˆé€†å‘æ€ç»´ï¼‰ï¼Œæ¥æé«˜å…¶å®‰å…¨æ€§ï¼Ÿ | æå‡ºäº† **InVThink** æ¡†æ¶ï¼ŒæŒ‡å¯¼æ¨¡å‹ (1) æšä¸¾å±å®³ï¼Œ(2) åˆ†æåæœï¼Œ(3) ç”Ÿæˆå®‰å…¨è¾“å‡ºã€‚ç»“åˆäº†æ•°æ®å¢å¼ºï¼ˆæ•™å¸ˆæ¨¡å‹ï¼‰ã€SFT å’Œ RLï¼ˆGRPOï¼‰ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{kim2025invthink,<br>  title={InVThink: Towards AI Safety via Inverse Reasoning},<br>  author={Kim, Yubin and Kim, Taehan and Park, Eugene and Park, Chunjong and Breazeal, Cynthia and McDuff, Daniel and Park, Hae Won},<br>  journal={arXiv preprint arXiv:2510.01569},<br>  year={2025}<br>}</pre></details> |
| 2025-10 | OpenReview | [Rethinking Deep Safety Alignment: Reflective Safety Alignment for Balancing Harmlessness and Helpfulness of LLMs](https://openreview.net/pdf?id=oql27GfyPD) | å¦‚ä½•é€šè¿‡å†…åŒ–è‡ªåæ€æ¨ç†èƒ½åŠ›æ¥å¹³è¡¡ LLM çš„æ— å®³æ€§å’Œæœ‰ç”¨æ€§ï¼Œå¹¶é˜²å¾¡è¶Šç‹±æ”»å‡»ï¼Ÿ | æå‡ºäº† **ReAlign**ï¼ˆåæ€æ€§å®‰å…¨å¯¹é½æ¡†æ¶ï¼‰ï¼ŒåŒ…å« **æ¨ç†å¼çƒ­èº« (RW)** ä»¥å†…åŒ–æ¨ç†èƒ½åŠ›ï¼Œå’Œ **è‡ªåæ€æ¨ç†è¿‡ç¨‹ä¼˜åŒ– (SRPO)** ä»¥ä¿ƒè¿›æ¨ç†è¿‡ç¨‹ä¸­çš„åæ€å’Œä¿®æ­£ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{mou2025rethinking,<br>  title={Rethinking Deep Safety Alignment: Reflective Safety Alignment for Balancing Harmlessness and Helpfulness of LLMs},<br>  author={Mou, Yutao and Luo, Yuxiao and Zhang, Shikun and Ye, Wei},<br>  journal={OpenReview},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection](https://arxiv.org/pdf/2509.14622) | å¦‚ä½•å…‹æœé™æ€åˆ†ç±»å™¨å’Œé‡å‹ LLM çš„å±€é™æ€§ï¼Œå®ç°å‡†ç¡®ã€é²æ£’ä¸”é«˜æ•ˆçš„å®æ—¶åœ¨çº¿æ¶æ„æ„å›¾æ£€æµ‹ï¼Ÿ | æå‡ºäº† **ADRAG** æ¡†æ¶ï¼Œç»“åˆäº†ï¼š(1) æ•™å¸ˆæ¨¡å‹çš„ **RAFT**ï¼ˆæ£€ç´¢å¢å¼ºå¯¹æŠ—å¾®è°ƒï¼‰ï¼Œå’Œ (2) **SKD**ï¼ˆé€‰æ‹©æ€§çŸ¥è¯†è’¸é¦ï¼‰ï¼Œå°†çŸ¥è¯†è¿ç§»åˆ°å…·æœ‰åœ¨çº¿æ›´æ–°çŸ¥è¯†åº“çš„ç´§å‡‘å­¦ç”Ÿé˜²æŠ¤æ¨¡å‹ä¸­ã€‚ | | <details><summary>Bib</summary><pre>@article{guo2025adversarial,<br>  title={Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection},<br>  author={Guo, Yihao and Bian, Haocheng and Zhou, Liutong and Wang, Ze and Zhang, Zhaoyi and Kawala, Francois and Dean, Milan and Fischer, Ian and Peng, Yuantao and Tokgozoglu, Noyan and others},<br>  journal={arXiv preprint arXiv:2509.14622},<br>  year={2025}<br>}</pre></details> |
| 2025-02 | ICLR2025 | [Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment](https://openreview.net/pdf?id=BfXS7uApJ6) | å¦‚ä½•å¢å¼º LLM çš„å®‰å…¨å¯¹é½ï¼Œä»¥æ›´å¥½åœ°æ³›åŒ–åº”å¯¹æœªçŸ¥/åˆ†å¸ƒå¤–ï¼ˆOODï¼‰çš„è¶Šç‹±æ”»å‡»ï¼Ÿ | æå‡ºäº† **SRG (Safety Reasoning with Guidelines)**ï¼Œé€šè¿‡åŸºäºæ˜ç¡®å®‰å…¨æŒ‡å—çš„ç»“æ„åŒ–å¤šæ­¥éª¤æ¨ç†ï¼Œå¼•å¯¼æ¨¡å‹ç³»ç»Ÿåœ°æå–æ½œåœ¨çš„å®‰å…¨çŸ¥è¯†ï¼Œä»è€Œç¨³å¥åœ°æ‹’ç»æœ‰å®³æŸ¥è¯¢ã€‚ | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@inproceedings{wang2025leveraging,<br>  title={Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment},<br>  author={Wang, Haoyu and Qin, Zeyu and Shen, Li and Wang, Xueqian and Cheng, Minhao and Tao, Dacheng},<br>  booktitle={International Conference on Learning Representations},<br>  year={2025}<br>}</pre></details> |
| 2025-02 | ICML2025 | [STAIR: Improving Safety Alignment with Introspective Reasoning](https://arxiv.org/pdf/2502.02384) | å¦‚ä½•åˆ©ç”¨å†…çœæ¨ç†ï¼ˆIntrospective Reasoningï¼‰æé«˜å®‰å…¨å¯¹é½ï¼Ÿ | æå‡ºäº† **STAIR**ï¼Œä¸€ç§åˆ©ç”¨å†…çœæ¨ç†æ¥å¢å¼ºå®‰å…¨å¯¹é½çš„æ¡†æ¶ã€‚ | | <details><summary>Bib</summary><pre>@inproceedings{zhang2025stair,<br>  title={STAIR: Improving Safety Alignment with Introspective Reasoning},<br>  author={Zhang, Yichi and Zhang, Siyuan and Huang, Yao and Xia, Zeyu and Fang, Zhengwei and Yang, Xiao and Duan, Ranjie and Yan, Dong and Dong, Yinpeng and Zhu, Jun},<br>  booktitle={Proceedings of the 42nd International Conference on Machine Learning},<br>  year={2025}<br>}</pre></details> |
| 2025-11 | arxiv2025 | [Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines](https://arxiv.org/pdf/2511.21214) | å¦‚ä½•åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„èƒ½åŠ›ï¼Œåœ¨å¢å¼ºå¯¹æŠ—æ€§æç¤ºé²æ£’æ€§çš„åŒæ—¶å‡å°‘å¯¹è‰¯æ€§è¯·æ±‚çš„è¯¯æ‹’ï¼Ÿ | æå‡ºäº† **SGASA (Self-Guided Adaptive Safety Alignment)** æ¡†æ¶ï¼Œé€šè¿‡ SFT å’Œ DPO å†…åŒ–æ¨¡å‹ç”Ÿæˆçš„å®‰å…¨æŒ‡å—ï¼Œå¼•å¯¼æ¨¡å‹è‡ªé€‚åº”åœ°è¯†åˆ«å’Œæ‹’ç»æœ‰å®³æŸ¥è¯¢ã€‚ | | <details><summary>Bib</summary><pre>@article{wang2025self,<br>  title={Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines},<br>  author={Wang, Yuhang and Zhu, Yanxu and Lu, Dongyuan and Sang, Jitao},<br>  journal={arXiv preprint arXiv:2511.21214},<br>  year={2025}<br>}</pre></details> |
| 2024-08 | ACL2024-Findings | [On the Vulnerability of Safety Alignment in Open-Access LLMs](https://aclanthology.org/2024.findings-acl.549.pdf) | å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å®‰å…¨å¯¹é½åœ¨é¢å¯¹æ¶æ„å¾®è°ƒæ—¶æœ‰å¤šè„†å¼±ï¼Ÿ | ç³»ç»Ÿè¯„ä¼°äº†å®‰å…¨è„†å¼±æ€§ï¼Œè¡¨æ˜ä»…ç”¨å°‘é‡æœ‰å®³æ•°æ®ï¼ˆç”šè‡³è‰¯æ€§æ•°æ®ï¼‰è¿›è¡Œå¾®è°ƒå³å¯æ˜¾è‘—ç ´åå®‰å…¨å¯¹é½ã€‚ | | <details><summary>Bib</summary><pre>@inproceedings{yi2024vulnerability,<br>  title={On the Vulnerability of Safety Alignment in Open-Access LLMs},<br>  author={Yi, Jingwei and Ye, Rui and Chen, Qisi and Zhu, Bin and Chen, Siheng and Lian, Defu and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao},<br>  booktitle={Findings of the Association for Computational Linguistics: ACL 2024},<br>  pages={9236--9260},<br>  year={2024}<br>}</pre></details> |
| 2024-05 | arxiv2024 | [Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems](https://arxiv.org/pdf/2405.06624) | å¦‚ä½•ä»æ¦‚ç‡æ€§å®‰å…¨æªæ–½ï¼ˆRLHF/è¯„ä¼°ï¼‰è¿‡æ¸¡åˆ°**æœ‰ä¿è¯çš„ AI å®‰å…¨ï¼ˆGuaranteed Safe AIï¼‰**ï¼Œç¡®ä¿ AI ç³»ç»Ÿä¸¥æ ¼éµå®ˆæ˜ç¡®çš„å®‰å…¨è§„èŒƒï¼Ÿ | æå‡ºäº† **å®ˆé—¨äººï¼ˆGatekeeperï¼‰** æ¶æ„ï¼ˆGS-AIï¼‰ï¼Œå…¶ä¸­éªŒè¯å™¨ï¼ˆVerifierï¼‰åœ¨ AI è¾“å‡ºæ‰§è¡Œä¹‹å‰è¯æ˜å…¶æ»¡è¶³å½¢å¼åŒ–å®‰å…¨è§„èŒƒã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ï¼š(1) ä¸–ç•Œæ¨¡å‹å­¦ä¹ ï¼Œ(2) å®‰å…¨è§„èŒƒåˆ¶å®šï¼Œ(3) éªŒè¯ï¼ˆç¥ç»ç¬¦å·æˆ–å½¢å¼åŒ–æ–¹æ³•ï¼‰ã€‚ | | <details><summary>Bib</summary><pre>@article{dalrymple2024towards,<br>  title={Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems},<br>  author={Dalrymple, David and Skalse, Joar and Bengio, Yoshua and Russell, Stuart and Tegmark, Max and Seshia, Sanjit and Omohundro, Steve and Szegedy, Christian and Goldhaber, Ben and Ammann, Nora and others},<br>  journal={arXiv preprint arXiv:2405.06624},<br>  year={2024}<br>}</pre></details> |

</details>

---

## å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-Agent Systems)

<details>
<summary>ğŸ“š ç‚¹å‡»å±•å¼€/æŠ˜å è®ºæ–‡åˆ—è¡¨</summary>

| æ—¶é—´ | å‘è¡¨å¤„ | è®ºæ–‡ | ç ”ç©¶é—®é¢˜/æ€è·¯ | æ–¹æ³• | è¯„è®º | å¼•ç”¨ |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |

</details>

---

## æ··åˆä¸“å®¶æ¨¡å‹ (Mixture-of-Experts)

<details>
<summary>ğŸ“š ç‚¹å‡»å±•å¼€/æŠ˜å è®ºæ–‡åˆ—è¡¨</summary>

| æ—¶é—´ | å‘è¡¨å¤„ | è®ºæ–‡ | ç ”ç©¶é—®é¢˜/æ€è·¯ | æ–¹æ³• | è¯„è®º | å¼•ç”¨ |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |

</details>
