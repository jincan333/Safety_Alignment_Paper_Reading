# 安全对齐论文阅读

## 概览

本仓库追踪并总结关于大语言模型（LLMs）**安全对齐**的论文。每个条目包含时间、论文链接、研究问题/思路以及核心方法——以便您快速浏览该领域动态。欢迎提交 PR 贡献。

## 论文列表

| 时间 | 发表处 | 论文 | 研究问题/思路 | 方法 | 评论 | 引用 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 2025-04 | arxiv2025 | [SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning](https://arxiv.org/pdf/2504.02725) | 如何通过使 LLMs 在生成响应*之前*执行结构化的安全推理，来提高其安全性并覆盖多样化和边缘情况？ | 提出了 **SAFER** 框架，该框架利用事前推理（初始评估、规则验证、路径校准）和 **ERPO**（事前推理偏好优化）来对齐模型，以进行可验证的安全判断。 | $\color{orange}{\triangle}$ | <details><summary>Bib</summary><pre>@article{feng2025safer,<br>  title={SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning},<br>  author={Feng, Kehua and Ding, Keyan and Wang, Yuhao and Li, Menghan and Wei, Fanjunduo and Wang, Xinda and Zhang, Qiang and Chen, Huajun},<br>  journal={arXiv preprint arXiv:2504.02725},<br>  year={2025}<br>}</pre></details> |
| 2025-12 | arxiv2025 | [Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability](https://arxiv.org/pdf/2512.01848) | 鉴于监督微调（SFT）的局限性，如何在不损害推理能力的前提下，实现大型推理模型（LRMs）的稳健安全对齐？ | 提出利用 **强化学习（RL）** 作为 SFT 的补充优化框架，使模型在显式推理过程中学习更安全的行为，同时保持高效的推理能力。 | $\color{red}{\times}$ | <details><summary>Bib</summary><pre>@article{jia2025beyond,<br>  title={Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability},<br>  author={Jia, Jinghan and Baracaldo, Nathalie and Liu, Sijia},<br>  journal={arXiv preprint arXiv:2512.01848},<br>  year={2025}<br>}</pre></details> |
| 2025-10 | arxiv2025 | [Adversarial Déjà Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks](https://arxiv.org/pdf/2510.21910) | 当当前的防御方法因优化挑战或训练数据覆盖不足而失效时，如何提高针对**未知（unseen）**越狱攻击的对抗鲁棒性？ | 提出了 **Adversarial Déjà Vu（对抗既视感）** 假设：未知的越狱攻击实质上是现有“对抗技能”的重新组合。引入了 **ASCoT（Adversarial Skill Compositional Training，对抗技能组合训练）**，它从过去的攻击中学习稀疏的技能基元字典，并在这些技能的多样化组合上训练模型，以增强泛化能力。 | Compositional attacks for generalization | <details><summary>Bib</summary><pre>@article{dabas2025adversarial,<br>  title={Adversarial Déjà Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks},<br>  author={Dabas, Mahavir and Huynh, Tran and Billa, Nikhil Reddy and Wang, Jiachen T and Gao, Peng and Peris, Charith and Ma, Yao and Gupta, Rahul and Jin, Ming and Mittal, Prateek and others},<br>  journal={arXiv preprint arXiv:2510.21910},<br>  year={2025}<br>}</pre></details> |
| 2025-12 | arxiv2025 | [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/pdf/2512.07141) | 如何解决大型视觉语言模型（LVLMs）中单次推理（single-pass reasoning）容易受到上下文或视觉越狱攻击的问题，特别是模型未能识别其自身初始输出中的有害内容？ | 提出了 **Think-Reflect-Revise (TRR)** 框架，该框架利用**显式策略引导的反思（policy-guided reflection）**来利用自身暴露的恶意内容进行自我修正。它包括构建 **ReSafe** 数据集，通过 SFT 初始化反思行为，并通过强化学习（GRPO）增强该行为。 | | <details><summary>Bib</summary><pre>@article{weng2025think,<br>  title={Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models},<br>  author={Weng, Fenghua and Lu, Chaochao and Hu, Xia and Shao, Wenqi and Wang, Wenjie},<br>  journal={arXiv preprint arXiv:2512.07141},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation](https://arxiv.org/pdf/2509.14760) | 大型语言模型（LLMs）如何在推理过程中动态地与特定场景的行为和安全规范（spec）保持一致，尤其是当需求不断变化时？ | 提出了 **ALIGN3**，一种测试时深思熟虑（TTD）方法，通过分层反思和修订来推理规范边界：(1) 行为优化，(2) 安全引导的细化，(3) 整体规范审计。同时引入了 **SPECBENCH** 用于评估。 | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025reasoning,<br>  title={Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation},<br>  author={Zhang, Haoran and Li, Yafu and Hu, Xuyang and Liu, Dongrui and Wang, Zhilin and Li, Bo and Cheng, Yu},<br>  journal={arXiv preprint arXiv:2509.14760},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention](https://arxiv.org/pdf/2509.24393) | 鉴于即使最终答案是安全的，不安全的推理过程仍可能存在，我们如何对大型推理模型（LRMs）的*推理过程本身*进行安全对齐？ | 提出了 **Intervened Preference Optimization (IPO)**，该方法通过将推理过程中的“顺从线索（compliance cues）”替换为“安全触发器（safety triggers）”来生成安全轨迹，然后利用这些成对的轨迹进行偏好学习，从而强制模型进行安全推理。 | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025towards,<br>  title={Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention},<br>  author={Zhang, Yichi and Ding, Yue and Yang, Jingwen and Luo, Tianwei and Li, Dongbai and Duan, Ranjie and Liu, Qiang and Su, Hang and Dong, Yinpeng and Zhu, Jun},<br>  journal={arXiv preprint arXiv:2509.24393},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs](https://arxiv.org/pdf/2509.05367) | 攻击者如何利用大语言模型（LLMs）对伦理推理（特别是困境中的功利主义）的对齐来绕过安全防护？ | 提出了 **TRIAL**（基于电车难题的上下文攻击），将有害查询嵌入到“两害相权取其轻”的伦理困境（如电车难题）中，迫使模型为了“拯救”更多生命而生成被禁止的内容。 | Multi-Turn Attack | <details><summary>Bib</summary><pre>@article{chua2025between,<br>  title={Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs},<br>  author={Chua, Shei Pern and Thai, Zhen Leng and Teh, Kai Jun and Li, Xiao and Hu, Xiaolin},<br>  journal={arXiv preprint arXiv:2509.05367},<br>  year={2025}<br>}</pre></details> |
| 2025-07 | arxiv2025 | [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/pdf/2507.14987) | 如何在不依赖大量监督或肤浅拒绝捷径的情况下，激励大语言模型潜在的安全意识以实现深度安全对齐？ | 提出了 **AlphaAlign**，这是一个纯强化学习框架，采用双重奖励系统（可验证的安全奖励 + 归一化的有用性奖励），旨在鼓励主动安全推理并打破安全与实用性之间的权衡。 | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025alphaalign,<br>  title={AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning},<br>  author={Zhang, Yi and Zhang, An and Zhang, XiuYu and Sheng, Leheng and Chen, Yuxin and Liang, Zhenkai and Wang, Xiang},<br>  journal={arXiv preprint arXiv:2507.14987},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Lifelong Safety Alignment for Language Models](https://arxiv.org/pdf/2505.20259) | 如何使大语言模型（LLMs）能够持续适应并防御在部署过程中出现的未知和不断演变的越狱攻击？ | 提出了一个 **Lifelong Safety Alignment（终身安全对齐）** 框架，包含两个竞争组件：**Meta-Attacker**（元攻击者，用于主动发现新的越狱策略）和 **Defender**（防御者，用于抵御这些攻击），并利用 GPT-4o 从研究论文中提取见解来初始化攻击者。 | | <details><summary>Bib</summary><pre>@article{wang2025lifelong,<br>  title={Lifelong Safety Alignment for Language Models},<br>  author={Wang, Haoyu and Qin, Zeyu and Zhao, Yifei and Du, Chao and Lin, Min and Wang, Xueqian and Pang, Tianyu},<br>  journal={arXiv preprint arXiv:2505.20259},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?](https://arxiv.org/pdf/2505.18672) | 表征干预（Representation Intervention）方法是否真的定位了“有害”概念并引出对齐行为，尤其是在有害与无害的边界是非线性的情况下？ | 分析了线性擦除的局限性；提出了 **Concept Concentration (COCA)**，通过显式推理重构数据，简化有害/无害边界，从而实现有效的线性擦除和稳健的防御。 | $\color{orange}{\triangle}$ | <details><summary>Bib</summary><pre>@article{yang2025does,<br>  title={Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?},<br>  author={Yang, Hongzheng and Chen, Yongqiang and Qin, Zeyu and Liu, Tongliang and Xiao, Chaowei and Zhang, Kun and Han, Bo},<br>  journal={arXiv preprint arXiv:2505.18672},<br>  year={2025}<br>}</pre></details> |
