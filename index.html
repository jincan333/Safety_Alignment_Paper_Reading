<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Paper Reading List</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Outfit:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --bg-tertiary: #e9ecef;
            --border-color: #dee2e6;
            --text-primary: #212529;
            --text-secondary: #6c757d;
            --text-muted: #adb5bd;
            --accent-blue: #0d6efd;
            --accent-green: #198754;
            --accent-orange: #fd7e14;
            --accent-red: #dc3545;
            --accent-purple: #6f42c1;
            --link-color: #0d6efd;
            --hover-bg: #e9ecef;
            --selected-bg: #e7f1ff;
            --nav-width: 280px;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Outfit', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            min-height: 100vh;
            display: flex;
        }

        /* Navigation Sidebar */
        .nav-sidebar {
            width: var(--nav-width);
            min-width: var(--nav-width);
            background: var(--bg-secondary);
            border-right: 1px solid var(--border-color);
            display: flex;
            flex-direction: column;
            position: fixed;
            height: 100vh;
            overflow-y: auto;
        }

        .nav-header {
            padding: 24px 20px;
            border-bottom: 1px solid var(--border-color);
        }

        .nav-header h1 {
            font-size: 18px;
            font-weight: 600;
            color: var(--text-primary);
            margin-bottom: 4px;
        }

        .nav-header p {
            font-size: 13px;
            color: var(--text-secondary);
        }

        /* All Papers Link */
        .nav-all-link {
            display: flex;
            align-items: center;
            gap: 12px;
            padding: 12px 20px;
            margin: 8px;
            border-radius: 8px;
            color: var(--text-primary);
            text-decoration: none;
            font-size: 15px;
            font-weight: 600;
            background: linear-gradient(135deg, var(--accent-blue) 0%, var(--accent-purple) 100%);
            color: white;
            cursor: pointer;
            transition: all 0.2s ease;
        }

        .nav-all-link:hover {
            transform: translateY(-1px);
            box-shadow: 0 4px 12px rgba(13, 110, 253, 0.3);
        }

        .nav-all-link.active {
            box-shadow: 0 0 0 3px rgba(13, 110, 253, 0.3);
        }

        .nav-all-count {
            margin-left: auto;
            background: rgba(255,255,255,0.2);
            padding: 2px 10px;
            border-radius: 12px;
            font-size: 12px;
            font-family: 'JetBrains Mono', monospace;
        }

        .nav-section-title {
            font-size: 11px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: var(--text-muted);
            padding: 16px 20px 8px;
        }

        .nav-list {
            list-style: none;
            padding: 0 8px;
        }

        .nav-item {
            margin: 2px 0;
        }

        .nav-link {
            display: flex;
            align-items: center;
            gap: 12px;
            padding: 10px 12px;
            border-radius: 6px;
            color: var(--text-secondary);
            text-decoration: none;
            font-size: 14px;
            font-weight: 400;
            transition: all 0.15s ease;
            cursor: pointer;
        }

        .nav-link:hover {
            background: var(--hover-bg);
            color: var(--text-primary);
        }

        .nav-link.active {
            background: var(--selected-bg);
            color: var(--accent-blue);
            font-weight: 500;
        }

        .nav-icon {
            width: 18px;
            height: 18px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 14px;
        }

        .nav-count {
            margin-left: auto;
            background: var(--bg-tertiary);
            color: var(--text-secondary);
            font-size: 11px;
            font-weight: 500;
            padding: 2px 8px;
            border-radius: 10px;
            font-family: 'JetBrains Mono', monospace;
        }

        .nav-link.active .nav-count {
            background: var(--accent-blue);
            color: white;
        }

        /* Main Content */
        .main-content {
            flex: 1;
            margin-left: var(--nav-width);
            padding: 32px 40px;
            overflow-x: auto;
        }

        .content-header {
            margin-bottom: 20px;
            display: flex;
            align-items: center;
            justify-content: space-between;
        }

        .content-header h2 {
            font-size: 24px;
            font-weight: 600;
            color: var(--text-primary);
        }

        .paper-count {
            font-size: 14px;
            color: var(--text-secondary);
            background: var(--bg-tertiary);
            padding: 6px 14px;
            border-radius: 20px;
        }

        /* Search Box */
        .search-container {
            margin-bottom: 20px;
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 16px;
        }

        .search-row {
            display: flex;
            flex-wrap: wrap;
            gap: 12px;
            align-items: flex-end;
        }

        .search-field {
            display: flex;
            flex-direction: column;
            gap: 4px;
        }

        .search-field label {
            font-size: 11px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: var(--text-secondary);
        }

        .search-field input {
            padding: 8px 12px;
            border: 1px solid var(--border-color);
            border-radius: 6px;
            font-size: 13px;
            font-family: inherit;
            background: var(--bg-primary);
            color: var(--text-primary);
            transition: border-color 0.15s ease, box-shadow 0.15s ease;
        }

        .search-field input:focus {
            outline: none;
            border-color: var(--accent-blue);
            box-shadow: 0 0 0 3px rgba(13, 110, 253, 0.15);
        }

        .search-field input::placeholder {
            color: var(--text-muted);
        }

        .search-field.wide input {
            width: 200px;
        }

        .search-field.narrow input {
            width: 100px;
        }

        .search-clear-btn {
            padding: 8px 16px;
            background: var(--bg-tertiary);
            border: 1px solid var(--border-color);
            border-radius: 6px;
            font-size: 13px;
            font-family: inherit;
            color: var(--text-secondary);
            cursor: pointer;
            transition: all 0.15s ease;
        }

        .search-clear-btn:hover {
            background: var(--hover-bg);
            color: var(--text-primary);
        }

        /* Empty State */
        .empty-state {
            text-align: center;
            padding: 80px 40px;
            color: var(--text-secondary);
        }

        .empty-state-icon {
            font-size: 48px;
            margin-bottom: 16px;
            opacity: 0.5;
        }

        .empty-state h3 {
            font-size: 18px;
            font-weight: 500;
            color: var(--text-primary);
            margin-bottom: 8px;
        }

        .empty-state p {
            font-size: 14px;
        }

        /* Table Styles */
        .table-container {
            background: var(--bg-primary);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            overflow-x: auto; /* Enable horizontal scrolling */
            overflow-y: hidden;
        }

        table {
            width: max-content; /* Allow table to grow beyond container based on column widths */
            min-width: 100%; /* Ensure it fills at least the container */
            table-layout: fixed; /* Respect column widths */
            border-collapse: collapse;
            font-size: 13px;
        }

        th {
            background: var(--bg-secondary);
            padding: 14px 16px;
            text-align: left;
            font-weight: 600;
            color: var(--text-secondary);
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            cursor: pointer;
            user-select: none;
            white-space: nowrap;
            border-bottom: 1px solid var(--border-color);
            transition: background 0.15s ease;
        }

        th:hover {
            background: var(--hover-bg);
            color: var(--text-primary);
        }

        th::after {
            content: " â†•";
            font-size: 10px;
            opacity: 0.4;
            margin-left: 4px;
        }

        th.sort-asc::after {
            content: " â†‘";
            opacity: 1;
            color: var(--accent-blue);
        }

        th.sort-desc::after {
            content: " â†“";
            opacity: 1;
            color: var(--accent-blue);
        }

        th.sort-default::after {
            content: " â†•";
            opacity: 0.4;
            color: var(--text-secondary);
        }

        td {
            padding: 14px 16px;
            border-bottom: 1px solid var(--border-color);
            vertical-align: top;
            color: var(--text-primary);
            line-height: 1.5;
        }

        tr:last-child td {
            border-bottom: none;
        }

        tr:hover td {
            background: rgba(13, 110, 253, 0.04);
        }

        /* Column widths - applied to headers for resizing control */
        th:nth-child(1) { width: 90px; min-width: 60px; }  /* Time */
        th:nth-child(2) { width: 110px; min-width: 80px; } /* Venue */
        th:nth-child(3) { width: 280px; min-width: 150px; } /* Paper */
        th:nth-child(4) { width: 280px; min-width: 150px; } /* Question */
        th:nth-child(5) { width: 300px; min-width: 150px; } /* Method */
        th:nth-child(6) { width: 120px; min-width: 80px; } /* Remark */
        th:nth-child(7) { width: 450px; min-width: 200px; }  /* Bib - Wider by default */

        th {
            position: relative;
        }

        .resizer {
            position: absolute;
            top: 0;
            right: 0;
            width: 5px;
            cursor: col-resize;
            user-select: none;
            height: 100%;
            z-index: 10;
        }

        .resizer:hover, .resizing {
            background-color: var(--accent-blue);
            width: 7px;
            right: -3px; /* visual center */
        }

        /* Sidebar Resizer */
        .sidebar-resizer {
            position: absolute;
            top: 0;
            right: -3px;
            width: 6px;
            height: 100%;
            cursor: col-resize;
            z-index: 100;
            transition: background-color 0.2s;
        }

        .sidebar-resizer:hover, .sidebar-resizer.resizing {
            background-color: var(--accent-blue);
        }

        /* Time column */
        td:nth-child(1) {
            font-family: 'JetBrains Mono', monospace;
            font-size: 12px;
            color: var(--text-secondary);
        }

        /* Venue badges */
        td:nth-child(2) {
            font-family: 'JetBrains Mono', monospace;
            font-size: 11px;
        }

        /* Links */
        a {
            color: var(--link-color);
            text-decoration: none;
            transition: color 0.15s ease;
        }

        a:hover {
            color: var(--accent-purple);
            text-decoration: underline;
        }

        /* Bold text */
        strong {
            color: var(--accent-green);
            font-weight: 600;
        }

        /* Italic highlight */
        .highlight-text {
            color: var(--accent-orange);
            font-style: italic;
            font-weight: 500;
        }

        /* Bib cell */
        .bib-cell {
            cursor: pointer;
            position: relative;
        }

        .bib-cell:hover::after {
            content: "Double-click to copy";
            position: absolute;
            bottom: 100%;
            left: 50%;
            transform: translateX(-50%);
            background: var(--text-primary);
            color: var(--bg-primary);
            padding: 4px 8px;
            border-radius: 4px;
            font-size: 10px;
            white-space: nowrap;
            z-index: 10;
        }

        /* Bib details */
        details {
            cursor: pointer;
        }

        details summary {
            font-size: 11px;
            color: var(--accent-blue);
            font-weight: 500;
            padding: 4px 8px;
            background: var(--bg-secondary);
            border-radius: 4px;
            display: inline-block;
            border: 1px solid var(--border-color);
        }

        details summary:hover {
            background: var(--hover-bg);
        }

        details[open] summary {
            margin-bottom: 8px;
        }

        details pre {
            font-family: 'JetBrains Mono', monospace;
            font-size: 10px;
            background: var(--bg-secondary);
            padding: 12px;
            border-radius: 6px;
            overflow-x: auto;
            white-space: pre-wrap;
            word-break: break-all;
            color: var(--text-secondary);
            border: 1px solid var(--border-color);
        }

        /* Copy toast notification */
        .toast {
            position: fixed;
            bottom: 24px;
            right: 24px;
            background: var(--accent-green);
            color: white;
            padding: 12px 20px;
            border-radius: 8px;
            font-size: 14px;
            font-weight: 500;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            transform: translateY(100px);
            opacity: 0;
            transition: all 0.3s ease;
            z-index: 1000;
        }

        .toast.show {
            transform: translateY(0);
            opacity: 1;
        }

        /* Responsive */
        @media (max-width: 1200px) {
            .nav-sidebar {
                width: 220px;
                min-width: 220px;
            }
            .main-content {
                margin-left: 220px;
                padding: 24px;
            }
            .search-field.wide input {
                width: 150px;
            }
        }

        @media (max-width: 768px) {
            body {
                flex-direction: column;
            }
            .nav-sidebar {
                width: 100%;
                min-width: 100%;
                height: auto;
                position: relative;
                border-right: none;
                border-bottom: 1px solid var(--border-color);
            }
            .main-content {
                margin-left: 0;
                padding: 16px;
            }
            .nav-list {
                display: flex;
                flex-wrap: wrap;
                gap: 8px;
                padding: 12px;
            }
            .nav-item {
                margin: 0;
            }
            .search-row {
                flex-direction: column;
            }
            .search-field input,
            .search-field.wide input,
            .search-field.narrow input {
                width: 100%;
            }
        }

        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="nav-sidebar">
    <div class="nav-header">
        <h1>ðŸ“š Paper Reading</h1>
        <p>Research Paper Collection</p>
    </div>
    <a class="nav-all-link" id="allPapersLink" onclick="selectSection('__ALL__')">
        <span>ðŸ“‘</span>
        <span>All Papers</span>
        <span class="nav-all-count" id="allPapersCount">0</span>
    </a>
    <div class="nav-section-title">Research Directions</div>
    <ul class="nav-list" id="navList">
        <!-- Navigation items will be populated by JS -->
    </ul>
    <div class="sidebar-resizer" id="sidebarResizer"></div>
</nav>

<main class="main-content">
    <div class="content-header">
        <h2 id="sectionTitle">Select a Research Direction</h2>
        <span class="paper-count" id="paperCount">0 papers</span>
    </div>
    
    <!-- Search Container -->
    <div class="search-container">
        <div class="search-row">
            <div class="search-field wide">
                <label>All Columns</label>
                <input type="text" id="searchAll" placeholder="Search all...">
            </div>
            <div class="search-field narrow">
                <label>Time</label>
                <input type="text" id="searchTime" placeholder="e.g. 2025">
            </div>
            <div class="search-field narrow">
                <label>Venue</label>
                <input type="text" id="searchVenue" placeholder="e.g. ICLR">
            </div>
            <div class="search-field wide">
                <label>Paper Name</label>
                <input type="text" id="searchPaper" placeholder="Paper title...">
            </div>
            <div class="search-field wide">
                <label>Question/Idea</label>
                <input type="text" id="searchQuestion" placeholder="Research question...">
            </div>
            <div class="search-field wide">
                <label>Method</label>
                <input type="text" id="searchMethod" placeholder="Method...">
            </div>
            <div class="search-field narrow">
                <label>Remark</label>
                <input type="text" id="searchRemark" placeholder="Remark...">
            </div>
            <div class="search-field wide">
                <label>Bib / Author</label>
                <input type="text" id="searchBib" placeholder="Author, Year...">
            </div>
            <button class="search-clear-btn" onclick="clearSearch()">Clear</button>
        </div>
    </div>
    
    <div class="table-container" id="tableContainer">
        <div class="empty-state" id="emptyState">
            <div class="empty-state-icon">ðŸ“–</div>
            <h3>Select a Research Direction</h3>
            <p>Choose a topic from the sidebar to view papers</p>
        </div>
        <table id="paperTable" style="display: none;">
            <thead>
                <tr>
                    <th onclick="sortTable(0)">Time</th>
                    <th onclick="sortTable(1)">Venue</th>
                    <th onclick="sortTable(2)">Paper</th>
                    <th onclick="sortTable(3)">Question</th>
                    <th onclick="sortTable(4)">Method</th>
                    <th onclick="sortTable(5)">Remark</th>
                    <th onclick="sortTable(6)">Bib</th>
                </tr>
            </thead>
            <tbody id="tableBody">
            </tbody>
        </table>
    </div>
</main>

<!-- Toast notification for copy -->
<div class="toast" id="toast">âœ“ BibTeX copied to clipboard!</div>

<script>
    // Data organized by research direction
    const data = {
    "Safety Alignment": [
        {
            "time": "2025-11",
            "venue": "EMNLP2025",
            "paper": "<a href=\"https://aclanthology.org/2025.emnlp-main.154.pdf\">Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?</a>",
            "question": "How well do LLMs' actions align with their stated values (the \"Value-Action Gap\")?",
            "method": "Proposes <strong>ValueActionLens</strong>, a framework to evaluate value-action alignment. Includes a dataset of 14.8k value-informed actions across 12 cultures and 11 topics, evaluating alignment between stated values and actions.",
            "remark": "Value-Action Gap Evaluation",
            "bib": "<details><summary>Bib</summary><pre>@inproceedings{shen2025mind,<br>  title={Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?},<br>  author={Shen, Hua and Clark, Nicholas and Mitra, Tanu},<br>  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},<br>  pages={3097--3118},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-11",
            "venue": "EMNLP2025",
            "paper": "<a href=\"https://aclanthology.org/2025.emnlp-main.1291.pdf\">SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning</a>",
            "question": "How can we improve Large Reasoning Models (LRMs) safety generalization to unseen jailbreak prompts by activating their internal safety reasoning?",
            "method": "Proposes <strong>SafeKey</strong>, a framework enhancing the \"aha-moment\" in safety reasoning (key sentence) via (1) a <strong>Dual-Path Safety Head</strong> (for internal representations) and (2) a <strong>Query-Mask Modeling</strong> objective (to focus on query understanding).",
            "remark": "Safety Aha moment",
            "bib": "<details><summary>Bib</summary><pre>@inproceedings{zhou2025safekey,<br>  title={SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning},<br>  author={Zhou, Kaiwen and Zhao, Xuandong and Liu, Gaowen and Srinivasa, Jayanth and Feng, Aosong and Song, Dawn and Wang, Xin Eric},<br>  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},<br>  pages={25407--25423},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-08",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2508.12790\">Reinforcement Learning with Rubric Anchors</a>",
            "question": "How can we extend Reinforcement Learning from Verifiable Rewards (RLVR) to open-ended tasks lacking verifiable ground truth?",
            "method": "Proposes <strong>Rubric-Based RL</strong>, using over 10,000 structured rubrics as reward anchors to train models on subjective tasks. Released Qwen-30B-A3B.",
            "remark": "RL with Rubric-based Reward",
            "bib": "<details><summary>Bib</summary><pre>@article{huang2025reinforcement,<br>  title={Reinforcement Learning with Rubric Anchors},<br>  author={Huang, Zenan and Zhuang, Yihong and Lu, Guoshan and Qin, Zeyu and Xu, Haokai and Zhao, Tianyu and Peng, Ru and Hu, Jiaqi and Shen, Zhanming and Hu, Xiaomeng and others},<br>  journal={arXiv preprint arXiv:2508.12790},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-08",
            "venue": "AAAI2026",
            "paper": "<a href=\"https://arxiv.org/pdf/2508.20151\">IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement</a>",
            "question": "How can we balance safety and utility in guard models, minimizing over-refusal for borderline queries while maintaining robust defense?",
            "method": "Proposes <strong>IntentionReasoner</strong>, a guard model that utilizes intent reasoning and multi-level classification (including \"Borderline Unharmful/Harmful\") to selectively rewrite potentially harmful queries into safe ones. Trained via SFT on a constructed dataset and RL with multi-reward optimization.",
            "remark": "",
            "bib": "<details><summary>Bib</summary><pre>@article{shen2025intentionreasoner,<br>  title={IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement},<br>  author={Shen, Yuanzhe and Huang, Zisu and Guo, Zhengkang and Liu, Yide and Chen, Guanxu and Yin, Ruicheng and Zheng, Xiaoqing and Huang, Xuanjing},<br>  journal={arXiv preprint arXiv:2508.20151},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-08",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2508.09224\">From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training</a>",
            "question": "How can we move from hard refusals to safe completions in safety training?",
            "method": "Proposes <strong>Output-Centric Safety Training</strong>, a method that focuses on generating safe completions rather than just refusing harmful queries.",
            "remark": "Safe Completion Training",
            "bib": "<details><summary>Bib</summary><pre>@article{yuan2025hard,<br>  title={From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training},<br>  author={Yuan, Yuan and Sriskandarajah, Tina and Brakman, Anna-Luisa and Helyar, Alec and Beutel, Alex and Vallone, Andrea and Jain, Saachi},<br>  journal={arXiv preprint arXiv:2508.09224},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-07",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2507.14805\">Subliminal Learning: Language models transmit behavioral traits via hidden signals in data</a>",
            "question": "Can language models learn behaviors from hidden signals in data?",
            "method": "Proposes <strong>Subliminal Learning</strong>, demonstrating that models can learn to associate hidden signals in training data with specific behavioral traits and generalize this to test time.",
            "remark": "",
            "bib": "<details><summary>Bib</summary><pre>@article{cloud2025subliminal,<br>  title={Subliminal Learning: Language models transmit behavioral traits via hidden signals in data},<br>  author={Cloud, Alex and Le, Minh and Chua, James and Betley, Jan and Sztyber-Betley, Anna and Hilton, Jacob and Marks, Samuel and Evans, Owain},<br>  journal={arXiv preprint arXiv:2507.14805},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-04",
            "venue": "AAAI2026",
            "paper": "<a href=\"https://arxiv.org/pdf/2504.01903\">STAR-1: Safer Alignment of Reasoning LLMs with 1K Data</a>",
            "question": "How can we effectively align Large Reasoning Models (LRMs) for safety using limited data without compromising their reasoning capabilities?",
            "method": "Proposes <strong>STAR-1</strong>, a high-quality 1k-scale safety dataset built on diversity, deliberative reasoning, and rigorous filtering. It fine-tunes LRMs to generate policy-grounded reasoning traces, achieving significant safety gains with minimal reasoning degradation.",
            "remark": "A high-quality dataset for LRM safety",
            "bib": "<details><summary>Bib</summary><pre>@inproceedings{wang2026star,<br>  title={STAR-1: Safer Alignment of Reasoning LLMs with 1K Data},<br>  author={Wang, Zijun and Tu, Haoqin and Wang, Yuhan and Wu, Juncheng and Liu, Yanqing and Mei, Jieru and Bartoldson, Brian R. and Kailkhura, Bhavya and Xie, Cihang},<br>  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>  year={2026}<br>}</pre></details>"
        },
        {
            "time": "2025-04",
            "venue": "NAACL2025",
            "paper": "<a href=\"https://aclanthology.org/2025.naacl-long.302.pdf\">Stronger Universal and Transferable Attacks by Suppressing Refusals</a>",
            "question": "How can we generate stronger and more transferable universal adversarial attacks by explicitly preventing models from refusing harmful queries?",
            "method": "Proposes <strong>suppressing refusals</strong>, a method that optimizes adversarial suffixes to not only maximize the target harmful response but also minimize the likelihood of refusal (e.g., \"I cannot\"). This yields state-of-the-art universal attacks.",
            "remark": "",
            "bib": "<details><summary>Bib</summary><pre>@inproceedings{huang2025stronger,<br>  title={Stronger Universal and Transferable Attacks by Suppressing Refusals},<br>  author={Huang, David and Shah, Avidan and Araujo, Alexandre and Wagner, David and Sitawarin, Chawin},<br>  booktitle={Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},<br>  pages={5850--5876},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-02",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2502.08301\">Compromising Honesty and Harmlessness in Language Models via Deception Attacks</a>",
            "question": "How can fine-tuning attacks compromise both honesty and harmlessness in LLMs by teaching them to be deceptive?",
            "method": "Introduces <strong>deception attacks</strong> via fine-tuning on a mix of deceptive and accurate examples. Shows models can be deceptive on specific topics while accurate on others, and this increases toxicity.",
            "remark": "",
            "bib": "<details><summary>Bib</summary><pre>@article{vaugrante2025compromising,<br>  title={Compromising Honesty and Harmlessness in Language Models via Deception Attacks},<br>  author={Vaugrante, Laur{\\`e}ne and Carlon, Francesca and Menke, Maluna and Hagendorff, Thilo},<br>  journal={arXiv preprint arXiv:2502.08301},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-02",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2502.12486\">EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning</a>",
            "question": "How can we improve strategic reasoning (aligning long-term goals amidst uncertainty) in LLMs for complex real-world scenarios like negotiations?",
            "method": "Proposes <strong>EPO</strong>, featuring a dedicated LLM that generates strategies to guide arbitrary agent LLMs. Uses <strong>Multi-Turn Reinforcement Learning (RL)</strong> with process rewards and iterative self-play to train the reasoning model for adaptability and transferability.",
            "remark": "$\\color{green}{\\checkmark}$",
            "bib": "<details><summary>Bib</summary><pre>@article{liu2025epo,<br>  title={EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning},<br>  author={Liu, Xiaoqian and Wang, Ke and Li, Yongbin and Wu, Yuchuan and Ma, Wentao and Kong, Aobo and Huang, Fei and Jiao, Jianbin and Zhang, Junge},<br>  journal={arXiv preprint arXiv:2502.12486},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-07",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2507.20796\">Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach</a>",
            "question": "How can we align LLM agents with interpretable economic and moral preferences (homo economicus and homo moralis) in strategic interactions?",
            "method": "Proposes a <strong>Supervised Fine-Tuning (SFT)</strong> pipeline using synthetic datasets derived from economic games to train agents on structured utility functions (self-interest vs. Kantian universalizability).",
            "remark": "Align with econimic interests",
            "bib": "<details><summary>Bib</summary><pre>@article{lu2025aligning,<br>  title={Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach},<br>  author={Lu, Wei and Chen, Daniel L and Hansen, Christian B},<br>  journal={arXiv preprint arXiv:2507.20796},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-02",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2502.11054\">Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models</a>",
            "question": "How can we improve multi-turn jailbreak attacks by incorporating reasoning capabilities?",
            "method": "Proposes <strong>Reasoning-Augmented Conversation</strong>, a framework that enhances multi-turn jailbreak attacks by leveraging reasoning to strategize and adapt the conversation flow.",
            "remark": "Multi-Turn Attack",
            "bib": "<details><summary>Bib</summary><pre>@article{ying2025reasoning,<br>  title={Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models},<br>  author={Ying, Zonghao and Zhang, Deyue and Jing, Zonglei and Xiao, Yisong and Zou, Quanchen and Liu, Aishan and Liang, Siyuan and Zhang, Xiangzheng and Liu, Xianglong and Tao, Dacheng},<br>  journal={arXiv preprint arXiv:2502.11054},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-02",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2502.12893\">H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models</a>",
            "question": "How can we evaluate and exploit the safety vulnerabilities of Large Reasoning Models (LRMs) that use Chain-of-Thought (CoT) for safety checking?",
            "method": "Proposes <strong>Malicious-Educator</strong>, a benchmark with dangerous queries disguised as educational prompts. Introduces <strong>H-CoT (Hijacking Chain-of-Thought)</strong>, a universal attack that leverages the model's displayed intermediate reasoning to jailbreak its safety mechanism.",
            "remark": "CoT Hijacking",
            "bib": "<details><summary>Bib</summary><pre>@article{kuo2025hcot,<br>  title={H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking},<br>  author={Kuo, Martin and Zhang, Jianyi and Ding, Aolin and Wang, Qinsi and DiValentin, Louis and Bao, Yujia and Wei, Wei and Li, Hai and Chen, Yiran},<br>  journal={arXiv preprint arXiv:2502.12893},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-02",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2502.12025\">SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities</a>",
            "question": "How to ensure the safety of Large Reasoning Models (LRMs) with long Chain-of-Thought (CoT), given that intermediate steps might be harmful even if the final answer is safe?",
            "method": "Systematically evaluates LRM safety; analyzes reasoning traces; proposes <strong>SAFECHAIN</strong>, a safety training dataset in CoT style, to fine-tune LRMs for improved safety without compromising reasoning performance.",
            "remark": "safety reasoning dataset",
            "bib": "<details><summary>Bib</summary><pre>@article{jiang2025safechain,<br>  title={SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities},<br>  author={Jiang, Fengqing and Xu, Zhangchen and Li, Yuetai and Niu, Luyao and Xiang, Zhen and Li, Bo and Lin, Bill Yuchen and Poovendran, Radha},<br>  journal={arXiv preprint arXiv:2502.12025},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-03",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2503.05021\">Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety</a>",
            "question": "How can we move beyond rigid refusals to robust, interpretable, and context-aware safety in LLMs against reasoning exploits?",
            "method": "Proposes <strong>RATIONAL</strong>, a framework that fine-tunes models on structured reasoning traces (intent, ethics, harm analysis) to internalize safety decision-making.",
            "remark": "Safety Reasoning train",
            "bib": "<details><summary>Bib</summary><pre>@article{zhang2025safety,<br>  title={Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety},<br>  author={Zhang, Yuyou and Li, Miao and Han, William and Yao, Yihang and Cen, Zhepeng and Zhao, Ding},<br>  journal={arXiv preprint arXiv:2503.05021},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-03",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2503.24370\">Effectively Controlling Reasoning Models through Thinking Intervention</a>",
            "question": "How can we achieve fine-grained control over the internal reasoning processes of Large Reasoning Models (LRMs) to improve instruction following, hierarchy, and safety?",
            "method": "Proposes <strong>Thinking Intervention</strong>, a paradigm that explicitly inserts or revises specific thinking tokens (instructions/constraints) within the model's intermediate reasoning chain, rather than just prompting the input.",
            "remark": "$\\color{green}{\\checkmark}$",
            "bib": "<details><summary>Bib</summary><pre>@article{wu2025effectively,<br>  title={Effectively Controlling Reasoning Models through Thinking Intervention},<br>  author={Wu, Tong and Xiang, Chong and Wang, Jiachen T. and Suh, G. Edward and Mittal, Prateek},<br>  journal={arXiv preprint arXiv:2503.24370},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-03",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2503.00555\">Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable</a>",
            "question": "Does safety alignment negatively impact the reasoning capabilities of Large Reasoning Models (LRMs)?",
            "method": "Empirically evaluates LRMs on reasoning benchmarks before and after safety alignment, identifying a \"Safety Tax\" where reasoning performance degrades as safety increases.",
            "remark": "",
            "bib": "<details><summary>Bib</summary><pre>@article{huang2025safety,<br>  title={Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable},<br>  author={Huang, Tiansheng and Hu, Sihao and Ilhan, Fatih and Tekin, Selim Furkan and Yahn, Zachary and Xu, Yichang and Liu, Ling},<br>  journal={arXiv preprint arXiv:2503.00555},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-08",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2508.00324\">R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge</a>",
            "question": "Why do Large Reasoning Models (LRMs) exhibit safety risks despite possessing safety knowledge, and how can we activate this knowledge during reasoning?",
            "method": "Proposes <strong>R1-ACT</strong>, a data-efficient post-training method that inserts a \"harmfulness assessment\" step into the reasoning chain (Understanding $\\rightarrow$ Assessment $\\rightarrow$ Solution) to explicitly activate safety knowledge.",
            "remark": "$\\color{green}{\\checkmark}$",
            "bib": "<details><summary>Bib</summary><pre>@article{in2025r1,<br>  title={R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge},<br>  author={In, Yeonjun and Kim, Wonjoong and Park, Sangwu and Park, Chanyoung},<br>  journal={arXiv preprint arXiv:2508.00324},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-04",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2504.02725\">SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning</a>",
            "question": "How can we improve LLM safety by enabling them to perform structured reasoning about safety *before* generating a response, covering diverse and edge cases?",
            "method": "Proposes <strong>SAFER</strong>, a framework that uses Ex-Ante reasoning (Initial Assessment, Rule Verification, Path Calibration) and <strong>ERPO</strong> (Ex-Ante Reasoning Preference Optimization) to align models for verifiable safety judgments.",
            "remark": "$\\color{orange}{\\triangle}$",
            "bib": "<details><summary>Bib</summary><pre>@article{feng2025safer,<br>  title={SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning},<br>  author={Feng, Kehua and Ding, Keyan and Wang, Yuhao and Li, Menghan and Wei, Fanjunduo and Wang, Xinda and Zhang, Qiang and Chen, Huajun},<br>  journal={arXiv preprint arXiv:2504.02725},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-04",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2504.09420\">SaRO: Enhancing LLM Safety through Reasoning-based Alignment</a>",
            "question": "How to address under-generalization and over-alignment in LLM safety alignment by incorporating safety-policy-driven reasoning?",
            "method": "Proposes <strong>SaRO</strong>, a framework consisting of <strong>Reasoning-style Warmup (RW)</strong> (SFT on long-chain reasoning) and <strong>Safety-oriented Reasoning Process Optimization (SRPO)</strong> (DPO for safety reflection).",
            "remark": "$\\color{green}{\\checkmark}$",
            "bib": "<details><summary>Bib</summary><pre>@article{mou2025saro,<br>  title={SaRO: Enhancing LLM Safety through Reasoning-based Alignment},<br>  author={Mou, Yutao and Luo, Yuxiao and Zhang, Shikun and Ye, Wei},<br>  journal={arXiv preprint arXiv:2504.09420},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-12",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2512.01848\">Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability</a>",
            "question": "How can we achieve robust safety alignment in Large Reasoning Models (LRMs) without compromising their reasoning capabilities, given the limitations of Supervised Fine-Tuning (SFT)?",
            "method": "Proposes using <strong>Reinforcement Learning (RL)</strong> as a supplementary optimization framework to SFT, enabling models to learn safer behaviors during explicit reasoning processes while maintaining high utility.",
            "remark": "$\\color{red}{\\times}$",
            "bib": "<details><summary>Bib</summary><pre>@article{jia2025beyond,<br>  title={Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability},<br>  author={Jia, Jinghan and Baracaldo, Nathalie and Liu, Sijia},<br>  journal={arXiv preprint arXiv:2512.01848},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-10",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2510.21910\">Adversarial D\u00e9j\u00e0 Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks</a>",
            "question": "How can we improve adversarial robustness against <strong>unseen</strong> jailbreak attacks when current defenses fail due to optimization challenges or poor training data coverage?",
            "method": "Proposes the <strong>Adversarial D\u00e9j\u00e0 Vu</strong> hypothesis: unseen jailbreaks are recombinations of existing \"adversarial skills.\" Introduces <strong>ASCoT (Adversarial Skill Compositional Training)</strong>, which learns a sparse dictionary of skill primitives from past attacks and trains models on diverse compositions of these skills to boost generalization.",
            "remark": "Compositional attacks for generalization",
            "bib": "<details><summary>Bib</summary><pre>@article{dabas2025adversarial,<br>  title={Adversarial D\u00e9j\u00e0 Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks},<br>  author={Dabas, Mahavir and Huynh, Tran and Billa, Nikhil Reddy and Wang, Jiachen T and Gao, Peng and Peris, Charith and Ma, Yao and Gupta, Rahul and Jin, Ming and Mittal, Prateek and others},<br>  journal={arXiv preprint arXiv:2510.21910},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-12",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2512.07141\">Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models</a>",
            "question": "How can we address the vulnerability of single-pass reasoning in LVLMs to contextual/visual jailbreaks, where models fail to recognize harmful content in their own initial output?",
            "method": "Proposes <strong>Think-Reflect-Revise (TRR)</strong>, a framework that leverages <strong>explicit policy-guided reflection</strong> to exploit self-revealed malicious content for self-correction. It involves constructing a <strong>ReSafe</strong> dataset, initializing reflective behavior via SFT, and enhancing it via RL (GRPO).",
            "remark": "",
            "bib": "<details><summary>Bib</summary><pre>@article{weng2025think,<br>  title={Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models},<br>  author={Weng, Fenghua and Lu, Chaochao and Hu, Xia and Shao, Wenqi and Wang, Wenjie},<br>  journal={arXiv preprint arXiv:2512.07141},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-09",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2509.11629\">Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check</a>",
            "question": "How can we enhance LLM robustness against jailbreak attacks by utilizing the model's thinking ability to evaluate safety before generating the final response?",
            "method": "Proposes <strong>Answer-Then-Check</strong>, a safety alignment approach where models generate a \"thought\" (containing a direct answer and a safety evaluation) before the final response. Introduces the <strong>Reasoned Safety Alignment (ReSA)</strong> dataset (80k examples) for fine-tuning.",
            "remark": "$\\color{green}{\\checkmark}$",
            "bib": "<details><summary>Bib</summary><pre>@article{cao2025reasoned,<br>  title={Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check},<br>  author={Cao, Chentao and Xu, Xiaojun and Han, Bo and Li, Hang},<br>  journal={arXiv preprint arXiv:2509.11629},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-09",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2509.14760\">Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation</a>",
            "question": "How can LLMs dynamically align with scenario-specific behavioral and safety specifications (spec) during inference, especially as requirements evolve?",
            "method": "Proposes <strong>ALIGN3</strong>, a test-time deliberation (TTD) method that reasons over spec boundaries through hierarchical reflection and revision: (1) behavior optimization, (2) safety-guided refinement, and (3) holistic specification audit. Introduces <strong>SPECBENCH</strong> for evaluation.",
            "remark": "$\\color{green}{\\checkmark}$",
            "bib": "<details><summary>Bib</summary><pre>@article{zhang2025reasoning,<br>  title={Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation},<br>  author={Zhang, Haoran and Li, Yafu and Hu, Xuyang and Liu, Dongrui and Wang, Zhilin and Li, Bo and Cheng, Yu},<br>  journal={arXiv preprint arXiv:2509.14760},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-09",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2509.24393\">Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention</a>",
            "question": "How can we align the safety of the reasoning process *itself* in Large Reasoning Models (LRMs), given that unsafe reasoning can persist even if the final answer is safe?",
            "method": "Proposes <strong>Intervened Preference Optimization (IPO)</strong>, which enforces safe reasoning by intervening to replace \"compliance cues\" with \"safety triggers\" to generate safe trajectories, and then using these paired trajectories for preference learning.",
            "remark": "$\\color{green}{\\checkmark}$",
            "bib": "<details><summary>Bib</summary><pre>@article{zhang2025towards,<br>  title={Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention},<br>  author={Zhang, Yichi and Ding, Yue and Yang, Jingwen and Luo, Tianwei and Li, Dongbai and Duan, Ranjie and Liu, Qiang and Su, Hang and Dong, Yinpeng and Zhu, Jun},<br>  journal={arXiv preprint arXiv:2509.24393},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-09",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2509.05367\">Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs</a>",
            "question": "How can attackers exploit LLMs' alignment with ethical reasoning (specifically utilitarianism in dilemmas) to bypass safety guardrails?",
            "method": "Proposes <strong>TRIAL</strong> (Trolley Problem-based In-context Attack for LLMs), which embeds harmful queries into \"lesser of two evils\" ethical dilemmas (e.g., Trolley Problem), forcing the model to generate prohibited content to \"save\" more lives.",
            "remark": "Multi-Turn Attack",
            "bib": "<details><summary>Bib</summary><pre>@article{chua2025between,<br>  title={Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs},<br>  author={Chua, Shei Pern and Thai, Zhen Leng and Teh, Kai Jun and Li, Xiao and Hu, Xiaolin},<br>  journal={arXiv preprint arXiv:2509.05367},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-07",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2507.14987\">AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning</a>",
            "question": "How can we incentivize LLMs' latent safety awareness to achieve deep safety alignment without relying on intensive supervision or superficial refusal shortcuts?",
            "method": "Proposes <strong>AlphaAlign</strong>, a pure RL framework with a dual-reward system (verifiable safety reward + normalized helpfulness reward) to encourage proactive safety reasoning and break the safety-utility trade-off.",
            "remark": "$\\color{green}{\\checkmark}$",
            "bib": "<details><summary>Bib</summary><pre>@article{zhang2025alphaalign,<br>  title={AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning},<br>  author={Zhang, Yi and Zhang, An and Zhang, XiuYu and Sheng, Leheng and Chen, Yuxin and Liang, Zhenkai and Wang, Xiang},<br>  journal={arXiv preprint arXiv:2507.14987},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-07",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2507.21652\">UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases</a>",
            "question": "How can we enhance the safety of reasoning models, particularly when dealing with \"hard cases\" where standard alignment might fail?",
            "method": "Proposes <strong>UnsafeChain</strong>, a method that leverages hard cases to improve the safety alignment of reasoning models.",
            "remark": "safety reasoning dataset",
            "bib": "<details><summary>Bib</summary><pre>@article{tomar2025unsafechain,<br>  title={UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases},<br>  author={Tomar, Raj Vardhan and Nakov, Preslav and Wang, Yuxia},<br>  journal={arXiv preprint arXiv:2507.21652},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-07",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2507.11500\">ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning</a>",
            "question": "How can we align LLMs to be both secure and safe using meticulous reasoning?",
            "method": "Proposes <strong>ARMOR</strong>, a framework that employs <strong>meticulous reasoning</strong> to align large language models for both security and safety.",
            "remark": "$\\color{green}{\\checkmark}$",
            "bib": "<details><summary>Bib</summary><pre>@article{zhao2025armor,<br>  title={ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning},<br>  author={Zhao, Zhengyue and Ma, Yingzi and Jha, Somesh and Pavone, Marco and McDaniel, Patrick and Xiao, Chaowei},<br>  journal={arXiv preprint arXiv:2507.11500},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-05",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2505.20259\">Lifelong Safety Alignment for Language Models</a>",
            "question": "How can LLMs continuously adapt to and defend against unseen and evolving jailbreaking attacks during deployment?",
            "method": "Proposes a <strong>Lifelong Safety Alignment</strong> framework with a competitive <strong>Meta-Attacker</strong> (discovers novel strategies) and <strong>Defender</strong> (resists them), initialized with insights from research papers via GPT-4o.",
            "remark": "",
            "bib": "<details><summary>Bib</summary><pre>@article{wang2025lifelong,<br>  title={Lifelong Safety Alignment for Language Models},<br>  author={Wang, Haoyu and Qin, Zeyu and Zhao, Yifei and Du, Chao and Lin, Min and Wang, Xueqian and Pang, Tianyu},<br>  journal={arXiv preprint arXiv:2505.20259},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-05",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2505.18672\">Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?</a>",
            "question": "Do representation intervention methods truly localize \"harmful\" concepts and elicit alignment, particularly when the boundary between harmful and benign is non-linear?",
            "method": "Analyzes limitations of linear erasure; proposes <strong>Concept Concentration (COCA)</strong> to reframe data with explicit reasoning, simplifying the harmful/benign boundary for effective linear erasure and robust defense.",
            "remark": "$\\color{orange}{\\triangle}$",
            "bib": "<details><summary>Bib</summary><pre>@article{yang2025does,<br>  title={Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?},<br>  author={Yang, Hongzheng and Chen, Yongqiang and Qin, Zeyu and Liu, Tongliang and Xiao, Chaowei and Zhang, Kun and Han, Bo},<br>  journal={arXiv preprint arXiv:2505.18672},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-05",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2505.19690\">Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models</a>",
            "question": "How can we evaluate whether Large Reasoning Models (LRMs) truly understand risks (vs. Superficial Safety Alignment) in their internal reasoning?",
            "method": "Identifies <strong>Superficial Safety Alignment (SSA)</strong>; introduces <strong>Beyond Safe Answers (BSA)</strong> benchmark (2k instances, 9 risk categories) to assess reasoning safety.",
            "remark": "$\\color{orange}{\\triangle}$",
            "bib": "<details><summary>Bib</summary><pre>@article{zheng2025beyond,<br>  title={Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models},<br>  author={Zheng, Baihui and Zheng, Boren and Cao, Kerui and Tan, Yingshui and Liu, Zhendong and Wang, Weixun and Liu, Jiaheng and Yang, Jian and Su, Wenbo and Zhu, Xiaoyong and Zheng, Bo and Zhang, Kaifu},<br>  journal={arXiv preprint arXiv:2505.19690},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-10",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2510.01569\">InVThink: Towards AI Safety via Inverse Reasoning</a>",
            "question": "How can we improve LLM safety by enabling them to reason through failure modes *before* generating responses (inverse thinking)?",
            "method": "Proposes <strong>InVThink</strong>, a framework that instructs models to (1) enumerate harms, (2) analyze consequences, and (3) generate safe outputs. Uses data augmentation (teacher LM), SFT, and RL (GRPO).",
            "remark": "$\\color{green}{\\checkmark}$",
            "bib": "<details><summary>Bib</summary><pre>@article{kim2025invthink,<br>  title={InVThink: Towards AI Safety via Inverse Reasoning},<br>  author={Kim, Yubin and Kim, Taehan and Park, Eugene and Park, Chunjong and Breazeal, Cynthia and McDuff, Daniel and Park, Hae Won},<br>  journal={arXiv preprint arXiv:2510.01569},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-10",
            "venue": "OpenReview",
            "paper": "<a href=\"https://openreview.net/pdf?id=oql27GfyPD\">Rethinking Deep Safety Alignment: Reflective Safety Alignment for Balancing Harmlessness and Helpfulness of LLMs</a>",
            "question": "How can we better balance harmlessness and helpfulness in LLMs while defending against novel jailbreak attacks?",
            "method": "Proposes <strong>ReAlign</strong> (Reflective Safety Alignment Framework), consisting of <strong>Reasoning-style Warmup (RW)</strong> to internalize reasoning and <strong>Self-reflective Reasoning Process Optimization (SRPO)</strong> to promote reflection and correction.",
            "remark": "$\\color{green}{\\checkmark}$",
            "bib": "<details><summary>Bib</summary><pre>@article{mou2025rethinking,<br>  title={Rethinking Deep Safety Alignment: Reflective Safety Alignment for Balancing Harmlessness and Helpfulness of LLMs},<br>  author={Mou, Yutao and Luo, Yuxiao and Zhang, Shikun and Ye, Wei},<br>  journal={OpenReview},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-09",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2509.14622\">Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection</a>",
            "question": "How can we perform accurate, robust, and efficient online malicious intent detection for real-time applications, overcoming limitations of static classifiers and heavy LLMs?",
            "method": "Proposes <strong>ADRAG</strong>, a framework combining: (1) <strong>RAFT</strong> (Retrieval-Augmented Adversarial Fine-Tuning) of a teacher model, and (2) <strong>SKD</strong> (Selective Knowledge Distillation) to a compact student guard model with an online-updated knowledge base.",
            "remark": "",
            "bib": "<details><summary>Bib</summary><pre>@article{guo2025adversarial,<br>  title={Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection},<br>  author={Guo, Yihao and Bian, Haocheng and Zhou, Liutong and Wang, Ze and Zhang, Zhaoyi and Kawala, Francois and Dean, Milan and Fischer, Ian and Peng, Yuantao and Tokgozoglu, Noyan and others},<br>  journal={arXiv preprint arXiv:2509.14622},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-02",
            "venue": "ICLR2025",
            "paper": "<a href=\"https://openreview.net/pdf?id=BfXS7uApJ6\">Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment</a>",
            "question": "How can we enhance LLM safety alignment to better generalize against unseen/out-of-distribution (OOD) jailbreak attacks?",
            "method": "Proposes <strong>SRG (Safety Reasoning with Guidelines)</strong>, which guides models to perform structured, multi-step reasoning based on explicit safety guidelines to systematically elicit latent safety knowledge and robustly refuse harmful queries.",
            "remark": "$\\color{green}{\\checkmark}$ elicit safety reasoning capabilities",
            "bib": "<details><summary>Bib</summary><pre>@inproceedings{wang2025leveraging,<br>  title={Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment},<br>  author={Wang, Haoyu and Qin, Zeyu and Shen, Li and Wang, Xueqian and Cheng, Minhao and Tao, Dacheng},<br>  booktitle={International Conference on Learning Representations},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-02",
            "venue": "ICML2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2502.02384\">STAIR: Improving Safety Alignment with Introspective Reasoning</a>",
            "question": "How can we improve safety alignment using introspective reasoning?",
            "method": "Proposes <strong>STAIR</strong>, a framework that utilizes introspective reasoning to enhance safety alignment.",
            "remark": "TTS of safety",
            "bib": "<details><summary>Bib</summary><pre>@inproceedings{zhang2025stair,<br>  title={STAIR: Improving Safety Alignment with Introspective Reasoning},<br>  author={Zhang, Yichi and Zhang, Siyuan and Huang, Yao and Xia, Zeyu and Fang, Zhengwei and Yang, Xiao and Duan, Ranjie and Yan, Dong and Dong, Yinpeng and Zhu, Jun},<br>  booktitle={Proceedings of the 42nd International Conference on Machine Learning},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2025-11",
            "venue": "arxiv2025",
            "paper": "<a href=\"https://arxiv.org/pdf/2511.21214\">Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines</a>",
            "question": "How can we improve model robustness against adversarial prompts while reducing benign refusals, using the model's own capabilities?",
            "method": "Proposes <strong>SGASA (Self-Guided Adaptive Safety Alignment)</strong>, a framework that internalizes model-generated safety guidelines via SFT and DPO to guide the model in identifying and refusing harmful queries adaptively.",
            "remark": "",
            "bib": "<details><summary>Bib</summary><pre>@article{wang2025self,<br>  title={Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines},<br>  author={Wang, Yuhang and Zhu, Yanxu and Lu, Dongyuan and Sang, Jitao},<br>  journal={arXiv preprint arXiv:2511.21214},<br>  year={2025}<br>}</pre></details>"
        },
        {
            "time": "2024-08",
            "venue": "ACL2024-Findings",
            "paper": "<a href=\"https://aclanthology.org/2024.findings-acl.549.pdf\">On the Vulnerability of Safety Alignment in Open-Access LLMs</a>",
            "question": "How vulnerable is the safety alignment of open-access LLMs to malicious fine-tuning?",
            "method": "Systematically evaluates safety vulnerability, showing that fine-tuning with limited harmful data (or even benign data) significantly compromises safety alignment.",
            "remark": "",
            "bib": "<details><summary>Bib</summary><pre>@inproceedings{yi2024vulnerability,<br>  title={On the Vulnerability of Safety Alignment in Open-Access LLMs},<br>  author={Yi, Jingwei and Ye, Rui and Chen, Qisi and Zhu, Bin and Chen, Siheng and Lian, Defu and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao},<br>  booktitle={Findings of the Association for Computational Linguistics: ACL 2024},<br>  pages={9236--9260},<br>  year={2024}<br>}</pre></details>"
        },
        {
            "time": "2024-05",
            "venue": "arxiv2024",
            "paper": "<a href=\"https://arxiv.org/pdf/2405.06624\">Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems</a>",
            "question": "How can we transition from probabilistic safety measures (RLHF/evals) to <strong>guaranteed safety</strong> for AI systems, ensuring they adhere to explicit safety specifications?",
            "method": "Proposes a <strong>Gatekeeper</strong> architecture (GS-AI) where a Verifier proves that the AI's output satisfies a formal safety specification before it is actuated. This involves: (1) World Model Learning, (2) Safety Specification, and (3) Verification (neuro-symbolic or formal methods).",
            "remark": "",
            "bib": "<details><summary>Bib</summary><pre>@article{dalrymple2024towards,<br>  title={Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems},<br>  author={Dalrymple, David and Skalse, Joar and Bengio, Yoshua and Russell, Stuart and Tegmark, Max and Seshia, Sanjit and Omohundro, Steve and Szegedy, Christian and Goldhaber, Ben and Ammann, Nora and others},<br>  journal={arXiv preprint arXiv:2405.06624},<br>  year={2024}<br>}</pre></details>"
        }
    ],
    "Multi-Agent Systems": [],
    "Mixture-of-Experts": []
};

    let currentSection = null;
    let currentData = [];
    let sortDirection = {};

    // Icon mapping for sections
    const sectionIcons = {
        "Safety Alignment": "ðŸ›¡ï¸",
        "Multi-Agent Systems": "ðŸ¤–",
        "Mixture-of-Experts": "ðŸ§©",
        "default": "ðŸ“„"
    };

    function getIcon(sectionName) {
        return sectionIcons[sectionName] || sectionIcons["default"];
    }

    // Get all papers from all sections
    function getAllPapers() {
        let allPapers = [];
        Object.keys(data).forEach(section => {
            data[section].forEach(paper => {
                allPapers.push({...paper, _section: section});
            });
        });
        return allPapers;
    }

    // Get total paper count
    function getTotalPaperCount() {
        return Object.values(data).reduce((sum, papers) => sum + papers.length, 0);
    }

    // Initialize navigation
    function initNav() {
        const navList = document.getElementById("navList");
        navList.innerHTML = "";
        
        // Update all papers count
        document.getElementById("allPapersCount").textContent = getTotalPaperCount();
        
        const sections = Object.keys(data);
        sections.forEach((section, index) => {
            const li = document.createElement("li");
            li.className = "nav-item";
            li.innerHTML = `
                <a class="nav-link" data-section="${section}" onclick="selectSection('${section}')">
                    <span class="nav-icon">${getIcon(section)}</span>
                    <span>${section}</span>
                    <span class="nav-count">${data[section].length}</span>
                </a>
            `;
            navList.appendChild(li);
        });

        // Select "All Papers" by default
        selectSection('__ALL__');
    }

    function selectSection(section) {
        currentSection = section;
        
        if (section === '__ALL__') {
            currentData = [...getAllPapers()];
        } else {
            currentData = [...data[section]];
        }
        
        sortDirection = {};
        
        // Clear search inputs without triggering search
        document.getElementById("searchAll").value = "";
        document.getElementById("searchTime").value = "";
        document.getElementById("searchVenue").value = "";
        document.getElementById("searchPaper").value = "";
        document.getElementById("searchQuestion").value = "";
        document.getElementById("searchMethod").value = "";
        document.getElementById("searchRemark").value = "";

        // Update nav active state
        document.getElementById("allPapersLink").classList.remove("active");
        document.querySelectorAll(".nav-link").forEach(link => {
            link.classList.remove("active");
        });
        
        if (section === '__ALL__') {
            document.getElementById("allPapersLink").classList.add("active");
        } else {
            document.querySelectorAll(".nav-link").forEach(link => {
                if (link.dataset.section === section) {
                    link.classList.add("active");
                }
            });
        }

        // Update header
        document.getElementById("sectionTitle").textContent = section === '__ALL__' ? 'All Papers' : section;
        document.getElementById("paperCount").textContent = `${currentData.length} paper${currentData.length !== 1 ? 's' : ''}`;

        // Show table or empty state
        const table = document.getElementById("paperTable");
        const emptyState = document.getElementById("emptyState");
        
        if (currentData.length > 0) {
            table.style.display = "table";
            emptyState.style.display = "none";
            renderTable(currentData);
        } else {
            table.style.display = "none";
            emptyState.style.display = "block";
            emptyState.innerHTML = `
                <div class="empty-state-icon">ðŸ“­</div>
                <h3>No Papers Yet</h3>
                <p>Add papers to "${section}" in README.md</p>
            `;
        }

        // Clear sort indicators
        document.querySelectorAll("th").forEach(th => {
            th.classList.remove("sort-asc", "sort-desc");
        });
    }

    // Extract plain text from HTML
    function stripHtml(html) {
        const tmp = document.createElement("div");
        tmp.innerHTML = html;
        return tmp.textContent || tmp.innerText || "";
    }

    // Extract BibTeX from bib HTML
    function extractBibtex(bibHtml) {
        const tmp = document.createElement("div");
        tmp.innerHTML = bibHtml;
        const pre = tmp.querySelector("pre");
        if (pre) {
            // Replace <br> with newlines and get text
            return pre.innerHTML.replace(/<br\s*\/?>/gi, '\n').replace(/<[^>]*>/g, '');
        }
        return "";
    }

    // Show toast notification
    function showToast(message) {
        const toast = document.getElementById("toast");
        toast.textContent = message || "âœ“ BibTeX copied to clipboard!";
        toast.classList.add("show");
        setTimeout(() => {
            toast.classList.remove("show");
        }, 2000);
    }

    // Copy BibTeX to clipboard
    function copyBibtex(bibHtml) {
        const bibtex = extractBibtex(bibHtml);
        if (bibtex) {
            navigator.clipboard.writeText(bibtex).then(() => {
                showToast("âœ“ BibTeX copied to clipboard!");
            }).catch(err => {
                console.error('Failed to copy: ', err);
                showToast("âœ— Failed to copy");
            });
        }
    }

    function renderTable(rows) {
        const tbody = document.getElementById("tableBody");
        tbody.innerHTML = "";
        
        rows.forEach(row => {
            const tr = document.createElement("tr");
            
            const processMarkdown = (text) => {
                if (!text) return text;
                return text
                    .replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>')
                    .replace(/\*(.*?)\*/g, '<span class="highlight-text">$1</span>');
            };

            tr.innerHTML = `
                <td>${row.time}</td>
                <td>${row.venue}</td>
                <td>${row.paper}</td>
                <td>${processMarkdown(row.question)}</td>
                <td>${processMarkdown(row.method)}</td>
                <td>${row.remark}</td>
                <td class="bib-cell">${row.bib}</td>
            `;
            
            // Add double-click handler for bib cell
            const bibCell = tr.querySelector('.bib-cell');
            bibCell.addEventListener('dblclick', function(e) {
                e.preventDefault();
                copyBibtex(row.bib);
            });
            
            tbody.appendChild(tr);
        });
        
        // Update paper count
        document.getElementById("paperCount").textContent = `${rows.length} paper${rows.length !== 1 ? 's' : ''}`;
        
        // Trigger MathJax re-render
        if (window.MathJax) {
            MathJax.typesetPromise([tbody]).catch(err => 
                console.log('MathJax typeset failed: ' + err.message)
            );
        }
    }

    function sortTable(columnIndex) {
        const keys = ["time", "venue", "paper", "question", "method", "remark", "bib"];
        const key = keys[columnIndex];
        
        // 0: Default, 1: ASC, -1: DESC
        // Initial state is undefined (treated as 0)
        let currentDir = sortDirection[key] || 0;
        let nextDir;

        if (currentDir === 0) nextDir = 1; // Default -> ASC
        else if (currentDir === 1) nextDir = -1; // ASC -> DESC
        else nextDir = 0; // DESC -> Default

        // Reset all sort directions
        sortDirection = {};
        if (nextDir !== 0) {
            sortDirection[key] = nextDir;
        }

        // Update headers UI
        document.querySelectorAll("th").forEach((th, idx) => {
            th.classList.remove("sort-asc", "sort-desc", "sort-default");
            if (idx === columnIndex) {
                if (nextDir === 1) th.classList.add("sort-asc");
                else if (nextDir === -1) th.classList.add("sort-desc");
                else th.classList.add("sort-default");
            } else {
                // Reset other headers visual state
                th.classList.add("sort-default");
            }
        });

        if (nextDir === 0) {
            // Re-run search to restore default order (with current filters)
            performSearch();
            return;
        }

        // Sort currentData in place
        currentData.sort((a, b) => {
            let valA = (a[key] || "").toLowerCase();
            let valB = (b[key] || "").toLowerCase();
            
            // Strip HTML tags for sorting
            if (columnIndex === 2 || columnIndex === 4) { 
                valA = valA.replace(/<[^>]*>/g, "");
                valB = valB.replace(/<[^>]*>/g, "");
            }

            if (valA < valB) return -1 * nextDir;
            if (valA > valB) return 1 * nextDir;
            return 0;
        });

        renderTable(currentData);
    }

    // Search functionality
    function performSearch() {
        // Guard against being called before section is selected
        if (!currentSection) {
            return;
        }

        const searchAll = document.getElementById("searchAll").value.toLowerCase().trim();
        const searchTime = document.getElementById("searchTime").value.toLowerCase().trim();
        const searchVenue = document.getElementById("searchVenue").value.toLowerCase().trim();
        const searchPaper = document.getElementById("searchPaper").value.toLowerCase().trim();
        const searchQuestion = document.getElementById("searchQuestion").value.toLowerCase().trim();
        const searchMethod = document.getElementById("searchMethod").value.toLowerCase().trim();
        const searchRemark = document.getElementById("searchRemark").value.toLowerCase().trim();
        const searchBib = document.getElementById("searchBib").value.toLowerCase().trim();

        // Check if any search is active
        const hasSearch = searchAll || searchTime || searchVenue || searchPaper || searchQuestion || searchMethod || searchRemark || searchBib;
        
        // Get base data for current section
        let baseData;
        if (currentSection === '__ALL__') {
            baseData = getAllPapers();
        } else if (data[currentSection]) {
            baseData = [...data[currentSection]];
        } else {
            baseData = [];
        }

        // If no search terms, show all data
        if (!hasSearch) {
            currentData = baseData;
            document.getElementById("paperCount").textContent = `${currentData.length} paper${currentData.length !== 1 ? 's' : ''}`;
            if (currentData.length > 0) {
                document.getElementById("paperTable").style.display = "table";
                document.getElementById("emptyState").style.display = "none";
                renderTable(currentData);
            }
            return;
        }

        // Filter papers based on search criteria
        const filteredPapers = baseData.filter(paper => {
            const timeText = (paper.time || "").toLowerCase();
            const venueText = (paper.venue || "").toLowerCase();
            const paperText = stripHtml(paper.paper || "").toLowerCase();
            const questionText = stripHtml(paper.question || "").toLowerCase();
            const methodText = stripHtml(paper.method || "").toLowerCase();
            const remarkText = (paper.remark || "").toLowerCase();
            const bibText = stripHtml(paper.bib || "").toLowerCase();
            const allText = `${timeText} ${venueText} ${paperText} ${questionText} ${methodText} ${remarkText} ${bibText}`;

            // Check specific field filters - each filter must match if provided
            if (searchTime && !timeText.includes(searchTime)) {
                return false;
            }
            if (searchVenue && !venueText.includes(searchVenue)) {
                return false;
            }
            if (searchPaper && !paperText.includes(searchPaper)) {
                return false;
            }
            if (searchQuestion && !questionText.includes(searchQuestion)) {
                return false;
            }
            if (searchMethod && !methodText.includes(searchMethod)) {
                return false;
            }
            if (searchRemark && !remarkText.includes(searchRemark)) {
                return false;
            }
            if (searchBib && !bibText.includes(searchBib)) {
                return false;
            }

            // Check "all columns" search - all keywords must appear somewhere
            if (searchAll) {
                const keywords = searchAll.split(/\s+/).filter(k => k.length > 0);
                for (const keyword of keywords) {
                    if (!allText.includes(keyword)) {
                        return false;
                    }
                }
            }

            return true;
        });

        currentData = filteredPapers;

        // Show results
        const table = document.getElementById("paperTable");
        const emptyState = document.getElementById("emptyState");
        
        // Update paper count
        document.getElementById("paperCount").textContent = `${filteredPapers.length} paper${filteredPapers.length !== 1 ? 's' : ''} (filtered)`;
        
        if (filteredPapers.length > 0) {
            table.style.display = "table";
            emptyState.style.display = "none";
            renderTable(filteredPapers);
        } else {
            table.style.display = "none";
            emptyState.style.display = "block";
            emptyState.innerHTML = `
                <div class="empty-state-icon">ðŸ”</div>
                <h3>No Matching Papers</h3>
                <p>Try different search terms or clear the filters</p>
            `;
        }
    }

    function clearSearch() {
        document.getElementById("searchAll").value = "";
        document.getElementById("searchTime").value = "";
        document.getElementById("searchVenue").value = "";
        document.getElementById("searchPaper").value = "";
        document.getElementById("searchQuestion").value = "";
        document.getElementById("searchMethod").value = "";
        document.getElementById("searchRemark").value = "";
        document.getElementById("searchBib").value = "";
        
        // Reset to full data for current section
        if (currentSection === '__ALL__') {
            currentData = [...getAllPapers()];
        } else if (currentSection && data[currentSection]) {
            currentData = [...data[currentSection]];
        }
        
        // Update paper count
        document.getElementById("paperCount").textContent = `${currentData.length} paper${currentData.length !== 1 ? 's' : ''}`;
        
        if (currentData.length > 0) {
            document.getElementById("paperTable").style.display = "table";
            document.getElementById("emptyState").style.display = "none";
            renderTable(currentData);
        }
    }

    // Setup search event listeners
    function setupSearchListeners() {
        const searchInputIds = ['searchAll', 'searchTime', 'searchVenue', 'searchPaper', 'searchQuestion', 'searchMethod', 'searchRemark', 'searchBib'];
        searchInputIds.forEach(id => {
            const el = document.getElementById(id);
            if (el) {
                // Only trigger on Enter key for immediate search (no real-time search)
                el.addEventListener('keydown', function(e) {
                    if (e.key === 'Enter') {
                        performSearch();
                    }
                });
            }
        });
    }

    // Column resizing functionality
    function makeTableResizable() {
        const table = document.getElementById('paperTable');
        const ths = table.querySelectorAll('th');

        ths.forEach(th => {
            // Check if resizer already exists (idempotent check)
            if (th.querySelector('.resizer')) return;

            // Create the resizer element
            const resizer = document.createElement('div');
            resizer.classList.add('resizer');
            th.appendChild(resizer);

            // Click on resizer should not trigger sort
            resizer.addEventListener('click', (e) => {
                e.stopPropagation();
            });

            createResizableColumn(th, resizer);
        });
    }

    function createResizableColumn(th, resizer) {
        let x = 0;
        let w = 0;

        const mouseDownHandler = function(e) {
            e.stopPropagation(); // Prevent sort triggering
            x = e.clientX;
            
            const styles = window.getComputedStyle(th);
            w = parseInt(styles.width, 10);

            document.addEventListener('mousemove', mouseMoveHandler);
            document.addEventListener('mouseup', mouseUpHandler);
            
            resizer.classList.add('resizing');
            document.body.style.cursor = 'col-resize'; // Global cursor
            document.body.style.userSelect = 'none'; // Prevent text selection
        };

        const mouseMoveHandler = function(e) {
            const dx = e.clientX - x;
            th.style.width = `${w + dx}px`;
        };

        const mouseUpHandler = function() {
            document.removeEventListener('mousemove', mouseMoveHandler);
            document.removeEventListener('mouseup', mouseUpHandler);
            resizer.classList.remove('resizing');
            document.body.style.cursor = '';
            document.body.style.userSelect = '';
        };

        resizer.addEventListener('mousedown', mouseDownHandler);
    }

    // Sidebar resizing
    function setupSidebarResizer() {
        const sidebar = document.querySelector('.nav-sidebar');
        const resizer = document.getElementById('sidebarResizer');
        const root = document.documentElement;
        
        let x = 0;
        let w = 0;

        const mouseDownHandler = function(e) {
            x = e.clientX;
            const sidebarStyle = window.getComputedStyle(sidebar);
            w = parseInt(sidebarStyle.width, 10);
            
            document.addEventListener('mousemove', mouseMoveHandler);
            document.addEventListener('mouseup', mouseUpHandler);
            
            resizer.classList.add('resizing');
            document.body.style.cursor = 'col-resize';
            document.body.style.userSelect = 'none';
        };

        const mouseMoveHandler = function(e) {
            const dx = e.clientX - x;
            const newWidth = Math.max(150, Math.min(600, w + dx)); // Limits: 150px to 600px
            root.style.setProperty('--nav-width', `${newWidth}px`);
        };

        const mouseUpHandler = function() {
            document.removeEventListener('mousemove', mouseMoveHandler);
            document.removeEventListener('mouseup', mouseUpHandler);
            resizer.classList.remove('resizing');
            document.body.style.cursor = '';
            document.body.style.userSelect = '';
        };

        resizer.addEventListener('mousedown', mouseDownHandler);
    }

    // Initialize on DOM ready
    document.addEventListener("DOMContentLoaded", () => {
        initNav();
        setupSearchListeners();
        makeTableResizable(); // Add resize handles
        setupSidebarResizer(); // Add sidebar resizer
    });
    
    // Re-typeset MathJax after initial render
    window.addEventListener('load', () => {
        if (window.MathJax) {
            MathJax.typesetPromise([document.getElementById('tableBody')]);
        }
    });
</script>

</body>
</html>
