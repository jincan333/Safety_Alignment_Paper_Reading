# Safety Alignment Paper Reading

## Overview

[**ðŸ‘‰ Interactive & Sortable Paper List**](https://jincan333.github.io/Safety_Alignment_Paper_Reading/)

This repository tracks and summarizes papers on **safety alignment** for Large Foundation Models (LFMs). Each entry captures the time, paper link, research question/idea, and the core methodâ€”so you can skim the landscape quickly. Contributions via PR are welcome.

## Paper List

| Time | Venue | Paper | Research Question/Idea | Method | Remark| Bib |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 2025-04 | arxiv2025 | [SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning](https://arxiv.org/pdf/2504.02725) | How can we improve LLM safety by enabling them to perform structured reasoning about safety *before* generating a response, covering diverse and edge cases? | Proposes **SAFER**, a framework that uses Ex-Ante reasoning (Initial Assessment, Rule Verification, Path Calibration) and **ERPO** (Ex-Ante Reasoning Preference Optimization) to align models for verifiable safety judgments. | $\color{orange}{\triangle}$ | <details><summary>Bib</summary><pre>@article{feng2025safer,<br>  title={SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning},<br>  author={Feng, Kehua and Ding, Keyan and Wang, Yuhao and Li, Menghan and Wei, Fanjunduo and Wang, Xinda and Zhang, Qiang and Chen, Huajun},<br>  journal={arXiv preprint arXiv:2504.02725},<br>  year={2025}<br>}</pre></details> |
| 2025-12 | arxiv2025 | [Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability](https://arxiv.org/pdf/2512.01848) | How can we achieve robust safety alignment in Large Reasoning Models (LRMs) without compromising their reasoning capabilities, given the limitations of Supervised Fine-Tuning (SFT)? | Proposes using **Reinforcement Learning (RL)** as a supplementary optimization framework to SFT, enabling models to learn safer behaviors during explicit reasoning processes while maintaining high utility. | $\color{red}{\times}$ | <details><summary>Bib</summary><pre>@article{jia2025beyond,<br>  title={Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability},<br>  author={Jia, Jinghan and Baracaldo, Nathalie and Liu, Sijia},<br>  journal={arXiv preprint arXiv:2512.01848},<br>  year={2025}<br>}</pre></details> |
| 2025-10 | arxiv2025 | [Adversarial DÃ©jÃ  Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks](https://arxiv.org/pdf/2510.21910) | How can we improve adversarial robustness against **unseen** jailbreak attacks when current defenses fail due to optimization challenges or poor training data coverage? | Proposes the **Adversarial DÃ©jÃ  Vu** hypothesis: unseen jailbreaks are recombinations of existing "adversarial skills." Introduces **ASCoT (Adversarial Skill Compositional Training)**, which learns a sparse dictionary of skill primitives from past attacks and trains models on diverse compositions of these skills to boost generalization. | Compositional attacks for generalization | <details><summary>Bib</summary><pre>@article{dabas2025adversarial,<br>  title={Adversarial DÃ©jÃ  Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks},<br>  author={Dabas, Mahavir and Huynh, Tran and Billa, Nikhil Reddy and Wang, Jiachen T and Gao, Peng and Peris, Charith and Ma, Yao and Gupta, Rahul and Jin, Ming and Mittal, Prateek and others},<br>  journal={arXiv preprint arXiv:2510.21910},<br>  year={2025}<br>}</pre></details> |
| 2025-12 | arxiv2025 | [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/pdf/2512.07141) | How can we address the vulnerability of single-pass reasoning in LVLMs to contextual/visual jailbreaks, where models fail to recognize harmful content in their own initial output? | Proposes **Think-Reflect-Revise (TRR)**, a framework that leverages **explicit policy-guided reflection** to exploit self-revealed malicious content for self-correction. It involves constructing a **ReSafe** dataset, initializing reflective behavior via SFT, and enhancing it via RL (GRPO). | | <details><summary>Bib</summary><pre>@article{weng2025think,<br>  title={Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models},<br>  author={Weng, Fenghua and Lu, Chaochao and Hu, Xia and Shao, Wenqi and Wang, Wenjie},<br>  journal={arXiv preprint arXiv:2512.07141},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation](https://arxiv.org/pdf/2509.14760) | How can LLMs dynamically align with scenario-specific behavioral and safety specifications (spec) during inference, especially as requirements evolve? | Proposes **ALIGN3**, a test-time deliberation (TTD) method that reasons over spec boundaries through hierarchical reflection and revision: (1) behavior optimization, (2) safety-guided refinement, and (3) holistic specification audit. Introduces **SPECBENCH** for evaluation. | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025reasoning,<br>  title={Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation},<br>  author={Zhang, Haoran and Li, Yafu and Hu, Xuyang and Liu, Dongrui and Wang, Zhilin and Li, Bo and Cheng, Yu},<br>  journal={arXiv preprint arXiv:2509.14760},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention](https://arxiv.org/pdf/2509.24393) | How can we align the safety of the reasoning process *itself* in Large Reasoning Models (LRMs), given that unsafe reasoning can persist even if the final answer is safe? | Proposes **Intervened Preference Optimization (IPO)**, which enforces safe reasoning by intervening to replace "compliance cues" with "safety triggers" to generate safe trajectories, and then using these paired trajectories for preference learning. | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025towards,<br>  title={Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention},<br>  author={Zhang, Yichi and Ding, Yue and Yang, Jingwen and Luo, Tianwei and Li, Dongbai and Duan, Ranjie and Liu, Qiang and Su, Hang and Dong, Yinpeng and Zhu, Jun},<br>  journal={arXiv preprint arXiv:2509.24393},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs](https://arxiv.org/pdf/2509.05367) | How can attackers exploit LLMs' alignment with ethical reasoning (specifically utilitarianism in dilemmas) to bypass safety guardrails? | Proposes **TRIAL** (Trolley Problem-based In-context Attack for LLMs), which embeds harmful queries into "lesser of two evils" ethical dilemmas (e.g., Trolley Problem), forcing the model to generate prohibited content to "save" more lives. | Multi-Turn Attack | <details><summary>Bib</summary><pre>@article{chua2025between,<br>  title={Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs},<br>  author={Chua, Shei Pern and Thai, Zhen Leng and Teh, Kai Jun and Li, Xiao and Hu, Xiaolin},<br>  journal={arXiv preprint arXiv:2509.05367},<br>  year={2025}<br>}</pre></details> |
| 2025-07 | arxiv2025 | [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/pdf/2507.14987) | How can we incentivize LLMs' latent safety awareness to achieve deep safety alignment without relying on intensive supervision or superficial refusal shortcuts? | Proposes **AlphaAlign**, a pure RL framework with a dual-reward system (verifiable safety reward + normalized helpfulness reward) to encourage proactive safety reasoning and break the safety-utility trade-off. | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025alphaalign,<br>  title={AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning},<br>  author={Zhang, Yi and Zhang, An and Zhang, XiuYu and Sheng, Leheng and Chen, Yuxin and Liang, Zhenkai and Wang, Xiang},<br>  journal={arXiv preprint arXiv:2507.14987},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Lifelong Safety Alignment for Language Models](https://arxiv.org/pdf/2505.20259) | How can LLMs continuously adapt to and defend against unseen and evolving jailbreaking attacks during deployment? | Proposes a **Lifelong Safety Alignment** framework with a competitive **Meta-Attacker** (discovers novel strategies) and **Defender** (resists them), initialized with insights from research papers via GPT-4o. | | <details><summary>Bib</summary><pre>@article{wang2025lifelong,<br>  title={Lifelong Safety Alignment for Language Models},<br>  author={Wang, Haoyu and Qin, Zeyu and Zhao, Yifei and Du, Chao and Lin, Min and Wang, Xueqian and Pang, Tianyu},<br>  journal={arXiv preprint arXiv:2505.20259},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?](https://arxiv.org/pdf/2505.18672) | Do representation intervention methods truly localize "harmful" concepts and elicit alignment, particularly when the boundary between harmful and benign is non-linear? | Analyzes limitations of linear erasure; proposes **Concept Concentration (COCA)** to reframe data with explicit reasoning, simplifying the harmful/benign boundary for effective linear erasure and robust defense. | $\color{orange}{\triangle}$ | <details><summary>Bib</summary><pre>@article{yang2025does,<br>  title={Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?},<br>  author={Yang, Hongzheng and Chen, Yongqiang and Qin, Zeyu and Liu, Tongliang and Xiao, Chaowei and Zhang, Kun and Han, Bo},<br>  journal={arXiv preprint arXiv:2505.18672},<br>  year={2025}<br>}</pre></details> |
