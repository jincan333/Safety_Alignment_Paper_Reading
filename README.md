# Safety Alignment Paper Reading

## Overview

[**ðŸ‘‰ Interactive & Sortable Paper List**](https://jincan333.github.io/Safety_Alignment_Paper_Reading/)

This repository tracks and summarizes papers on **safety alignment** for Large Foundation Models (LFMs). Each entry captures the time, paper link, research question/idea, and the core methodâ€”so you can skim the landscape quickly. Contributions via PR are welcome.

## Paper List

| Time | Venue | Paper | Research Question/Idea | Method | Remark| Bib |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 2025-08 | arxiv2025 | [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/pdf/2508.00324) | Why do Large Reasoning Models (LRMs) exhibit safety risks despite possessing safety knowledge, and how can we activate this knowledge during reasoning? | Proposes **R1-ACT**, a data-efficient post-training method that inserts a "harmfulness assessment" step into the reasoning chain (Understanding $\rightarrow$ Assessment $\rightarrow$ Solution) to explicitly activate safety knowledge. | | <details><summary>Bib</summary><pre>@article{in2025r1,<br>  title={R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge},<br>  author={In, Yeonjun and Kim, Wonjoong and Park, Sangwu and Park, Chanyoung},<br>  journal={arXiv preprint arXiv:2508.00324},<br>  year={2025}<br>}</pre></details> |
| 2025-04 | arxiv2025 | [SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning](https://arxiv.org/pdf/2504.02725) | How can we improve LLM safety by enabling them to perform structured reasoning about safety *before* generating a response, covering diverse and edge cases? | Proposes **SAFER**, a framework that uses Ex-Ante reasoning (Initial Assessment, Rule Verification, Path Calibration) and **ERPO** (Ex-Ante Reasoning Preference Optimization) to align models for verifiable safety judgments. | $\color{orange}{\triangle}$ | <details><summary>Bib</summary><pre>@article{feng2025safer,<br>  title={SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning},<br>  author={Feng, Kehua and Ding, Keyan and Wang, Yuhao and Li, Menghan and Wei, Fanjunduo and Wang, Xinda and Zhang, Qiang and Chen, Huajun},<br>  journal={arXiv preprint arXiv:2504.02725},<br>  year={2025}<br>}</pre></details> |
| 2025-12 | arxiv2025 | [Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability](https://arxiv.org/pdf/2512.01848) | How can we achieve robust safety alignment in Large Reasoning Models (LRMs) without compromising their reasoning capabilities, given the limitations of Supervised Fine-Tuning (SFT)? | Proposes using **Reinforcement Learning (RL)** as a supplementary optimization framework to SFT, enabling models to learn safer behaviors during explicit reasoning processes while maintaining high utility. | $\color{red}{\times}$ | <details><summary>Bib</summary><pre>@article{jia2025beyond,<br>  title={Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability},<br>  author={Jia, Jinghan and Baracaldo, Nathalie and Liu, Sijia},<br>  journal={arXiv preprint arXiv:2512.01848},<br>  year={2025}<br>}</pre></details> |
| 2025-10 | arxiv2025 | [Adversarial DÃ©jÃ  Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks](https://arxiv.org/pdf/2510.21910) | How can we improve adversarial robustness against **unseen** jailbreak attacks when current defenses fail due to optimization challenges or poor training data coverage? | Proposes the **Adversarial DÃ©jÃ  Vu** hypothesis: unseen jailbreaks are recombinations of existing "adversarial skills." Introduces **ASCoT (Adversarial Skill Compositional Training)**, which learns a sparse dictionary of skill primitives from past attacks and trains models on diverse compositions of these skills to boost generalization. | Compositional attacks for generalization | <details><summary>Bib</summary><pre>@article{dabas2025adversarial,<br>  title={Adversarial DÃ©jÃ  Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks},<br>  author={Dabas, Mahavir and Huynh, Tran and Billa, Nikhil Reddy and Wang, Jiachen T and Gao, Peng and Peris, Charith and Ma, Yao and Gupta, Rahul and Jin, Ming and Mittal, Prateek and others},<br>  journal={arXiv preprint arXiv:2510.21910},<br>  year={2025}<br>}</pre></details> |
| 2025-12 | arxiv2025 | [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/pdf/2512.07141) | How can we address the vulnerability of single-pass reasoning in LVLMs to contextual/visual jailbreaks, where models fail to recognize harmful content in their own initial output? | Proposes **Think-Reflect-Revise (TRR)**, a framework that leverages **explicit policy-guided reflection** to exploit self-revealed malicious content for self-correction. It involves constructing a **ReSafe** dataset, initializing reflective behavior via SFT, and enhancing it via RL (GRPO). | | <details><summary>Bib</summary><pre>@article{weng2025think,<br>  title={Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models},<br>  author={Weng, Fenghua and Lu, Chaochao and Hu, Xia and Shao, Wenqi and Wang, Wenjie},<br>  journal={arXiv preprint arXiv:2512.07141},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation](https://arxiv.org/pdf/2509.14760) | How can LLMs dynamically align with scenario-specific behavioral and safety specifications (spec) during inference, especially as requirements evolve? | Proposes **ALIGN3**, a test-time deliberation (TTD) method that reasons over spec boundaries through hierarchical reflection and revision: (1) behavior optimization, (2) safety-guided refinement, and (3) holistic specification audit. Introduces **SPECBENCH** for evaluation. | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025reasoning,<br>  title={Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation},<br>  author={Zhang, Haoran and Li, Yafu and Hu, Xuyang and Liu, Dongrui and Wang, Zhilin and Li, Bo and Cheng, Yu},<br>  journal={arXiv preprint arXiv:2509.14760},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention](https://arxiv.org/pdf/2509.24393) | How can we align the safety of the reasoning process *itself* in Large Reasoning Models (LRMs), given that unsafe reasoning can persist even if the final answer is safe? | Proposes **Intervened Preference Optimization (IPO)**, which enforces safe reasoning by intervening to replace "compliance cues" with "safety triggers" to generate safe trajectories, and then using these paired trajectories for preference learning. | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025towards,<br>  title={Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention},<br>  author={Zhang, Yichi and Ding, Yue and Yang, Jingwen and Luo, Tianwei and Li, Dongbai and Duan, Ranjie and Liu, Qiang and Su, Hang and Dong, Yinpeng and Zhu, Jun},<br>  journal={arXiv preprint arXiv:2509.24393},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs](https://arxiv.org/pdf/2509.05367) | How can attackers exploit LLMs' alignment with ethical reasoning (specifically utilitarianism in dilemmas) to bypass safety guardrails? | Proposes **TRIAL** (Trolley Problem-based In-context Attack for LLMs), which embeds harmful queries into "lesser of two evils" ethical dilemmas (e.g., Trolley Problem), forcing the model to generate prohibited content to "save" more lives. | Multi-Turn Attack | <details><summary>Bib</summary><pre>@article{chua2025between,<br>  title={Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs},<br>  author={Chua, Shei Pern and Thai, Zhen Leng and Teh, Kai Jun and Li, Xiao and Hu, Xiaolin},<br>  journal={arXiv preprint arXiv:2509.05367},<br>  year={2025}<br>}</pre></details> |
| 2025-07 | arxiv2025 | [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/pdf/2507.14987) | How can we incentivize LLMs' latent safety awareness to achieve deep safety alignment without relying on intensive supervision or superficial refusal shortcuts? | Proposes **AlphaAlign**, a pure RL framework with a dual-reward system (verifiable safety reward + normalized helpfulness reward) to encourage proactive safety reasoning and break the safety-utility trade-off. | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{zhang2025alphaalign,<br>  title={AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning},<br>  author={Zhang, Yi and Zhang, An and Zhang, XiuYu and Sheng, Leheng and Chen, Yuxin and Liang, Zhenkai and Wang, Xiang},<br>  journal={arXiv preprint arXiv:2507.14987},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Lifelong Safety Alignment for Language Models](https://arxiv.org/pdf/2505.20259) | How can LLMs continuously adapt to and defend against unseen and evolving jailbreaking attacks during deployment? | Proposes a **Lifelong Safety Alignment** framework with a competitive **Meta-Attacker** (discovers novel strategies) and **Defender** (resists them), initialized with insights from research papers via GPT-4o. | | <details><summary>Bib</summary><pre>@article{wang2025lifelong,<br>  title={Lifelong Safety Alignment for Language Models},<br>  author={Wang, Haoyu and Qin, Zeyu and Zhao, Yifei and Du, Chao and Lin, Min and Wang, Xueqian and Pang, Tianyu},<br>  journal={arXiv preprint arXiv:2505.20259},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?](https://arxiv.org/pdf/2505.18672) | Do representation intervention methods truly localize "harmful" concepts and elicit alignment, particularly when the boundary between harmful and benign is non-linear? | Analyzes limitations of linear erasure; proposes **Concept Concentration (COCA)** to reframe data with explicit reasoning, simplifying the harmful/benign boundary for effective linear erasure and robust defense. | $\color{orange}{\triangle}$ | <details><summary>Bib</summary><pre>@article{yang2025does,<br>  title={Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?},<br>  author={Yang, Hongzheng and Chen, Yongqiang and Qin, Zeyu and Liu, Tongliang and Xiao, Chaowei and Zhang, Kun and Han, Bo},<br>  journal={arXiv preprint arXiv:2505.18672},<br>  year={2025}<br>}</pre></details> |
| 2025-05 | arxiv2025 | [Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models](https://arxiv.org/pdf/2505.19690) | How can we evaluate whether Large Reasoning Models (LRMs) truly understand risks (vs. Superficial Safety Alignment) in their internal reasoning? | Identifies **Superficial Safety Alignment (SSA)**; introduces **Beyond Safe Answers (BSA)** benchmark (2k instances, 9 risk categories) to assess reasoning safety. | $\color{orange}{\triangle}$ | <details><summary>Bib</summary><pre>@article{zheng2025beyond,<br>  title={Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models},<br>  author={Zheng, Baihui and Zheng, Boren and Cao, Kerui and Tan, Yingshui and Liu, Zhendong and Wang, Weixun and Liu, Jiaheng and Yang, Jian and Su, Wenbo and Zhu, Xiaoyong and Zheng, Bo and Zhang, Kaifu},<br>  journal={arXiv preprint arXiv:2505.19690},<br>  year={2025}<br>}</pre></details> |
| 2025-10 | arxiv2025 | [InVThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/pdf/2510.01569) | How can we improve LLM safety by enabling them to reason through failure modes *before* generating responses (inverse thinking)? | Proposes **InVThink**, a framework that instructs models to (1) enumerate harms, (2) analyze consequences, and (3) generate safe outputs. Uses data augmentation (teacher LM), SFT, and RL (GRPO). | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@article{kim2025invthink,<br>  title={InVThink: Towards AI Safety via Inverse Reasoning},<br>  author={Kim, Yubin and Kim, Taehan and Park, Eugene and Park, Chunjong and Breazeal, Cynthia and McDuff, Daniel and Park, Hae Won},<br>  journal={arXiv preprint arXiv:2510.01569},<br>  year={2025}<br>}</pre></details> |
| 2025-09 | arxiv2025 | [Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection](https://arxiv.org/pdf/2509.14622) | How can we perform accurate, robust, and efficient online malicious intent detection for real-time applications, overcoming limitations of static classifiers and heavy LLMs? | Proposes **ADRAG**, a framework combining: (1) **RAFT** (Retrieval-Augmented Adversarial Fine-Tuning) of a teacher model, and (2) **SKD** (Selective Knowledge Distillation) to a compact student guard model with an online-updated knowledge base. |  | <details><summary>Bib</summary><pre>@article{guo2025adversarial,<br>  title={Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection},<br>  author={Guo, Yihao and Bian, Haocheng and Zhou, Liutong and Wang, Ze and Zhang, Zhaoyi and Kawala, Francois and Dean, Milan and Fischer, Ian and Peng, Yuantao and Tokgozoglu, Noyan and others},<br>  journal={arXiv preprint arXiv:2509.14622},<br>  year={2025}<br>}</pre></details> |
| 2025-02 | ICML2025 | [Safety Reasoning with Guidelines](https://arxiv.org/pdf/2502.04040) | How can we enhance LLM safety alignment to better generalize against unseen/out-of-distribution (OOD) jailbreak attacks? | Proposes **SRG (Safety Reasoning with Guidelines)**, which guides models to perform structured, multi-step reasoning based on explicit safety guidelines to systematically elicit latent safety knowledge and robustly refuse harmful queries. | $\color{green}{\checkmark}$ | <details><summary>Bib</summary><pre>@inproceedings{wang2025safety,<br>  title={Safety Reasoning with Guidelines},<br>  author={Wang, Haoyu and Qin, Zeyu and Shen, Li and Wang, Xueqian and Tao, Dacheng and Cheng, Minhao},<br>  booktitle={Forty-second International Conference on Machine Learning},<br>  year={2025}<br>}</pre></details> |
| 2025-11 | arxiv2025 | [Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines](https://arxiv.org/pdf/2511.21214) | How can we improve model robustness against adversarial prompts while reducing benign refusals, using the model's own capabilities? | Proposes **SGASA (Self-Guided Adaptive Safety Alignment)**, a framework that internalizes model-generated safety guidelines via SFT and DPO to guide the model in identifying and refusing harmful queries adaptively. | | <details><summary>Bib</summary><pre>@article{wang2025self,<br>  title={Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines},<br>  author={Wang, Yuhang and Zhu, Yanxu and Lu, Dongyuan and Sang, Jitao},<br>  journal={arXiv preprint arXiv:2511.21214},<br>  year={2025}<br>}</pre></details> |
